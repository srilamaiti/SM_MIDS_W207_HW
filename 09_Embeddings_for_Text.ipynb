{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/SM_MIDS_W207_HW/blob/main/09_Embeddings_for_Text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKsRDH5ZUdfasdv"
      },
      "source": [
        "# Lab 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43534tdfgs-v"
      },
      "source": [
        "In this lab, we'll train models for sentiment classification and experiment with learned embeddings for text features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "outputs": [],
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style=\"darkgrid\")  # default style\n",
        "import plotly.graph_objs as plotly  # for interactive plots\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from keras.utils.layer_utils import count_params\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqppUDpmdptk"
      },
      "source": [
        "## Data for Sentiment Classification\n",
        "\n",
        "In this lab, we'll train a *sentiment* classifier for movie reviews. That is, the input is the text of a movie review and the output is the probability the input was a positive review. The target labels are binary, 0 for negative and 1 for positive.\n",
        "\n",
        "Our data includes 50,000 movie reviews on IMDB. The data comes pre-segmented into train and test splits. The [data loading function](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data) below also splits each input text into tokens (words) and maps the words to integer values. Each input is a sequence of integers corresponding to the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "s6M-asvhQWV_",
        "outputId": "30712446-2ba6-4995-c408-c26c7fa32c87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train.shape: (25000,)\n",
            "Y_train.shape: (25000,)\n",
            "X_test.shape: (25000,)\n",
            "Y_test.shape: (25000,)\n",
            "First training example data: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "First training example label: 1\n"
          ]
        }
      ],
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=None,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=1,\n",
        "                                                      oov_char=2,\n",
        "                                                      index_from=3)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"Y_train.shape:\", Y_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"Y_test.shape:\", Y_test.shape)\n",
        "\n",
        "print('First training example data:', X_train[0])\n",
        "print('First training example label:', Y_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyIWiy-4gQK-"
      },
      "source": [
        "So our first training example is a positive review. But that sequence of integer IDs is hard to read. The data loader provides a dictionary mapping words to IDs. Let's create a reverse index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HQ-qATkhUj7c",
        "outputId": "231bb2a4-d631-4b86-cafb-63e1d38acb3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Largest ID: 88587\n"
          ]
        }
      ],
      "source": [
        "# The imdb dataset comes with an index mapping words to integers.\n",
        "# In the index the words are ordered by frequency they occur.\n",
        "index = imdb.get_word_index()\n",
        "\n",
        "# Because we used index_from=3 (above), setting aside ids below 3 for special\n",
        "# symbols, we need to add 3 to the index values.\n",
        "index = dict([(key, value+3) for (key, value) in index.items()])\n",
        "\n",
        "# Create a reverse index so we can lookup tokens assigned to each id.\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
        "reverse_index[1] = '<START>'  # start of input\n",
        "reverse_index[2] = '#'        # out-of-vocabulary (OOV)\n",
        "reverse_index[3] = '<UNUSED>'\n",
        "\n",
        "max_id = max(reverse_index.keys())\n",
        "print('Largest ID:', max_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index['good'], reverse_index[52], len(X_train[0]), len(X_train[1]), Y_train[0], len(index)"
      ],
      "metadata": {
        "id": "Pi54VXaPrHda",
        "outputId": "b3bc52ef-c3d7-4e90-a68d-f0bc6a8fb219",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(52, 'good', 218, 189, 1, 88584)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h76-b07ehWNQ"
      },
      "source": [
        "Note that our index (and reverse index) have over 88,000 tokens. That's quite a large vocabulary! Let's also write a decoding function for our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UjobmouHS5Dm",
        "outputId": "8775cd9b-6859-4ebd-b2e3-456ff6d487c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert redford's is an amazing actor and now the same being director norman's father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for retail and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also congratulations to the two little boy's that played the part's of norman and paul they were just brilliant children are often left out of the praising list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
          ]
        }
      ],
      "source": [
        "def decode(token_ids):\n",
        "  \"\"\"Return a string with the decoded text given a list of token ids.\"\"\"\n",
        "  # Try looking up each id in the index, but return '#' (for OOV) if not found.\n",
        "  tokens = [reverse_index.get(i, \"#\") for i in token_ids]\n",
        "\n",
        "  # Connect the string tokens with a space.\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "# Show the ids corresponding tokens in the first example.\n",
        "print(X_train[0])\n",
        "print(decode(X_train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g47w5CackGBA"
      },
      "source": [
        "### Text Lengths\n",
        "As usual, let's start with some data analysis. How long are the reviews? Is there a difference in length between positive and negative reviews? A histogram will help answer these questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEOgzo8Gk3r7"
      },
      "outputs": [],
      "source": [
        "# Create a list of lengths for training examples with a positive label.\n",
        "text_lengths_pos = [len(x) for (i, x) in enumerate(X_train) if Y_train[i]]\n",
        "\n",
        "# And a list of lengths for training examples with a negative label.\n",
        "text_lengths_neg = [len(x) for (i, x) in enumerate(X_train) if not Y_train[i]]\n",
        "\n",
        "# The histogram function can take a list of inputs and corresponding labels.\n",
        "plt.hist([text_lengths_pos, text_lengths_neg], bins=20, range=(0, 1000),\n",
        "         label=['positive', 'negative'])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Also check the longest reviews.\n",
        "print('Longest positive review:', max(text_lengths_pos))\n",
        "print('Longest negative review:', max(text_lengths_neg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ZE9gpkml3a"
      },
      "source": [
        "---\n",
        "### Exercise 1: Token Counts (8 points)\n",
        "For each of the given tokens, construct a table with the number of positive training examples that include that token and the number of negative training examples that include that token. For reference, here are the counts for the first two tokens:\n",
        "\n",
        "|Token|Pos Count|Neg Count|\n",
        "|-|-|-|\n",
        "|good|4767|4849|\n",
        "|bad|1491|4396|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YOYo6d01aWI"
      },
      "outputs": [],
      "source": [
        "tokens = ['good', 'bad', 'amazing', 'boring', 'laugh', 'cry']\n",
        "\n",
        "# YOUR CODE HERE\n",
        "token_ids = []\n",
        "token_ids = [index[token] for token in tokens]\n",
        "token_occurrence_dict = {token: {'positive_count' : 0, 'negative_count' : 0} for token in tokens}\n",
        "#print(token_ids)\n",
        "#print(token_occurrence_dict)\n",
        "for token_idx, encoded_word in enumerate(token_ids):\n",
        "    for x_idx, encoded_text in enumerate(X_train):\n",
        "        if encoded_word in encoded_text:\n",
        "            y_val = Y_train[x_idx]\n",
        "            token_val = tokens[token_idx]\n",
        "            count_val = encoded_text.count(encoded_word)\n",
        "            if y_val == 1:\n",
        "                #print(\"a\")\n",
        "                token_occurrence_dict[token_val]['positive_count'] += 1\n",
        "            elif y_val == 0:\n",
        "                #print(\"b\")\n",
        "                token_occurrence_dict[token_val]['negative_count'] += 1\n",
        "\n",
        "df = pd.DataFrame(token_occurrence_dict).T.reset_index()\n",
        "df.columns = ['Token'\t,'Pos Count',\t'Neg Count']\n",
        "df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhzt-LnQ1m8w"
      },
      "source": [
        "## Feature Representation\n",
        "Consider the difference between the pixel features we used for image classification and the text features we are now dealing with.\n",
        "\n",
        "An image had 784 pixel positions. At each position, there is a single value in [0,1] (after normalization).\n",
        "\n",
        "In contrast, a review has a variable number of ordered tokens (up to 2494 in the training examples). Each token occurs in a particular position. We can think of the token positions much like the 784 pixel positions, except that some of the trailing positions are empty, since review lengths vary.  At each token position, there is a single token, one of the 88587 entries in the vocabulary. So we can think of a review as a (2500, 90000) matrix: At each of ~2500 token positions, we have 1 of ~90000 token ids.\n",
        "\n",
        "This representation would have 2500 * 90000 = 225 million features -- quite a lot more complexity than the images, though as you'll see below, we will make some simplifying assumptions, reducing both the number of token positions and the number of vocabulary items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm_F5JmWyfko"
      },
      "source": [
        "### Padding and Reduced Length\n",
        "As is clear from the length histogram, the current representation of the review text is a variable-length array. Since fixed-length arrays are easier to work with in Tensorflow, let's add special padding tokens at the end of each review until they are all the same length.\n",
        "\n",
        "We'll also use this operation to limit the number of token positions by truncating all reviews to a specified length. In the code below, as an example, we pad all training inputs to length 300."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4ou8bSUCWOx"
      },
      "outputs": [],
      "source": [
        "def pad_data(sequences, max_length):\n",
        "  # Keras has a convenient utility for padding a sequence.\n",
        "  # Also make sure we get a numpy array rather than an array of lists.\n",
        "  return np.array(list(\n",
        "      tf.keras.preprocessing.sequence.pad_sequences(\n",
        "          sequences, maxlen=max_length, padding='post', value=0)))\n",
        "\n",
        "# Pad and truncate to 300 tokens.\n",
        "X_train_padded = pad_data(X_train, max_length=300)\n",
        "\n",
        "# Check the padded output.\n",
        "print('Length of X_train[0]:', len(X_train[0]))\n",
        "print('Length of X_train_padded[0]:', len(X_train_padded[0]))\n",
        "print(X_train_padded[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFEmcwBjL4e_"
      },
      "source": [
        "### Reduced Vocabulary\n",
        "We also want to be able to limit the vocabulary size. Since our padding function produces fixed-length sequences in a numpy matrix, we can use clever numpy indexing to efficiently replace all token ids larger than some value with the designated out-of-vocabulary (OOV) id.\n",
        "\n",
        "In the code below, as an example, we'll keep just token ids less than 1000, replacing all others with OOV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "21qpyEgGNQeB"
      },
      "outputs": [],
      "source": [
        "def limit_vocab(sequences, max_token_id, oov_id=2):\n",
        "  \"\"\"Replace token ids greater than or equal to max_token_id with the oov_id.\"\"\"\n",
        "  reduced_sequences = np.copy(sequences)\n",
        "  reduced_sequences[reduced_sequences >= max_token_id] = oov_id\n",
        "  return reduced_sequences\n",
        "\n",
        "# Reduce vocabulary to 1000 tokens.\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
        "print(X_train_reduced[0])\n",
        "\n",
        "# Decode to see what this looks like in tokens. Note the '#' for OOVs.\n",
        "print(decode(X_train_reduced[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d24mOPC6ybC4"
      },
      "source": [
        "### One-hot Encoding\n",
        "Our current feature representations are **sparse**. That is, we only keep track of the token ids that are present in the input. A **one-hot** encoding replaces a value like 22 (corresponding to 'film') with an array with a single 1 at position 22 and zeros everywhere else. This will be very memory-inefficient, but we'll do it anyway for clarity.\n",
        "\n",
        "As discussed above, let's dramatically reduce both the number of token positions (review length) and the number of token ids (vocabulary). We'll clip each review after 20 tokens (so 2500 -> 20) and keep only the most common 1000 tokens (so 90000 -> 1000)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EXzkqVL3Jufj"
      },
      "outputs": [],
      "source": [
        "# Keras has a util to create one-hot encodings.\n",
        "X_train_padded = pad_data(X_train, max_length=20)\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
        "X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n",
        "print('X_train_one_hot shape:', X_train_one_hot.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5RvIN4w66Ej"
      },
      "source": [
        "Note the shape of the one-hot encoded features. For each of our 25000 training examples, we have a 20 x 1000 matrix. That is, for each of 20 token positions, we have a vector of 1000 elements containing a single 1 and 999 zeros.\n",
        "\n",
        "We can think of these 1000-dimensional one-hot arrays as **embeddings**. Each token in the input has a 1000-dimensional representation. But because of the one-hot setup, the distance between each pair of tokens is the same ([1,0,0,...], [0,1,0,...], etc.). By contrast, learned embeddings result in meaningful distances between pairs of tokens. We'll get to that soon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "296Cnt647b5c"
      },
      "source": [
        "## Logistic Regression with One-Hot Encodings\n",
        "Let's start with something familiar -- logistic regression. Since our feature representation is in 2 dimensions (20 x 1000), we need to flatten it to pass it to Keras (remember we did this with the pixel data too). Let's try two strategies for flattening.\n",
        "\n",
        "1. Flatten by *concatenating* (as we did with pixels), turning (20 x 1000) data into (20000,) data. The result is a separate feature for each token at each position.\n",
        "2. Flatten by *averaging* over token positions, turning (20 x 1000) data into (1000,) data. The result is an array with average token counts, ignoring position.\n",
        "\n",
        "NOTE: Our prior assignments have used the standard Stochastic Gradient Descent (SGD) optimizer to compute the gradient from an estimate of the loss (based on the current mini-batch). There are many alternative optimizers. Here we'll use the **Adam** optimizer, which sometimes gives better results. One key characteristic of Adam is that it effectively uses a different learning rate for each parameter rather than a fixed learning rate as in SGD."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6m6eebM-0dUW"
      },
      "outputs": [],
      "source": [
        "def build_onehot_model(average_over_positions=False):\n",
        "  \"\"\"Build a tf.keras model for one-hot data.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  if average_over_positions:\n",
        "    # This layer averages over the first dimension of the input by default.\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  else:\n",
        "    # Concatenate.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation=\"sigmoid\"         # sigmoid activation for classification\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',   # this is a classification task\n",
        "                optimizer='adam',             # fancy optimizer\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY3W_1-OSZ2X"
      },
      "source": [
        "Now let's try fitting the model to our training data and check performance metrics on the validation (held-out) data. But first, here's a function for plotting the learning curves given the training history object we get from Keras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOVmajSuMjN6"
      },
      "outputs": [],
      "source": [
        "def plot_history(history):\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
        "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
        "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyE4PgX70_op"
      },
      "outputs": [],
      "source": [
        "model = build_onehot_model()\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train_one_hot,  # one-hot training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuCh9aQPv7F_"
      },
      "source": [
        "---\n",
        "### Exercise 2: Comparing logistic regrerssion models (8 points)\n",
        "Train the one-hot model using both the concatenating and the averaging strategies and compare the results. Let's call these *LR-C* (Logistic Regression Concatenating) and *LR-A* (Logistic Regression Averaging). Then answer the following questions:\n",
        "\n",
        "1. What are the final training and validation accuracies for LR-C and LR-A?\n",
        "2. How many parameters are there in each model?\n",
        "3. Would you say that either model is overfitting? Why or why not?\n",
        "4. Briefly describe how LR-C differs from LR-A. How do you explain the relationship between their respective validation accuracy results? "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LR-C\n",
        "model1 = build_onehot_model(average_over_positions=False)\n",
        "\n",
        "# Fit the model.\n",
        "history1 = model1.fit(\n",
        "  x = X_train_one_hot,  # one-hot training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history1 = pd.DataFrame(history1.history)\n",
        "plot_history(history1)\n",
        "\n",
        "# LR-A\n",
        "model2 = build_onehot_model(average_over_positions=True)\n",
        "\n",
        "# Fit the model.\n",
        "history2 = model2.fit(\n",
        "  x = X_train_one_hot,  # one-hot training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history2 = pd.DataFrame(history2.history)\n",
        "plot_history(history2)"
      ],
      "metadata": {
        "id": "ID_zoOdiPajl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.summary()"
      ],
      "metadata": {
        "id": "FtpZwlyfQufR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "id": "tWsUke8iQ45J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history_accuracy(history):\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.xticks(range(0, len(history['accuracy'] + 1)))\n",
        "  plt.plot(history['accuracy'], label=\"training\", marker='o')\n",
        "  plt.plot(history['val_accuracy'], label=\"validation\", marker='o')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "52alBJCRRdnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history_accuracy(history1)"
      ],
      "metadata": {
        "id": "AomE9bVvRk7o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_history_accuracy(history2)"
      ],
      "metadata": {
        "id": "s26cTJe-Scys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEAN5BejHc__"
      },
      "source": [
        "*Written answers:*\n",
        "\n",
        "1. Final training and validation accuracies for LR-C and LR-A:-\n",
        "\n",
        "*   For LR-C model, final training and validation accuracies are 0.8100 and 0.6816 respectively.\n",
        "*   For LR-A model, final training and validation accuracies are 0.6715 and 0.6884 respectively.\n",
        "\n",
        "\n",
        "2. LR-C model has 20,001 parameters and model2 has 1,001 parameters.\n",
        "3. From the plot, it looks that LR-C model is overfitting as compared to LR-A model as the training accuracy is consistently more than validation accuracy.\n",
        "4. LR-C model flattens out input layer without any attention to the importances of any input whereas LR-A tries to average out the effects of the input, thus it generalizes well which is present in their respective validation accuracies(0.6816 vs 0.6884).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaSCkLk73Y3V"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJIBRqK7lsjG"
      },
      "source": [
        "## Logistic Regression with Embeddings\n",
        "Next, let's train model that replaces one-hot representations of each token with learned embeddings.\n",
        "\n",
        "The code below uses a Keras Embedding layer, which expects to receive a sparse (rather than one-hot) representation. That is, it expects a (padded) sequence of token ids; for each id, it looks up the corresponding embedding vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ho6uOeCaBs2e"
      },
      "outputs": [],
      "source": [
        "def build_embeddings_model(average_over_positions=False,\n",
        "                           vocab_size=1000,\n",
        "                           sequence_length=20,\n",
        "                           embedding_dim=2):\n",
        "  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size,\n",
        "      output_dim=embedding_dim,\n",
        "      input_length=sequence_length)\n",
        "  )\n",
        "\n",
        "  if average_over_positions:\n",
        "    # This layer averages over the first dimension of the input by default.\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  else:\n",
        "    # Concatenate.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation='sigmoid'         # apply the sigmoid function!\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', \n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyhoEjAiFSNB"
      },
      "source": [
        "Try training the model as before. We'll use the averaging strategy rather than the concatenating strategy for dealing with the token sequence. That is, we'll look up embedding vectors for each token. Then we'll average them to produce a single vector. Then we'll traing a logistic regression with that vector as input to predict the binary label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYUE5UwkxoU8"
      },
      "outputs": [],
      "source": [
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1000,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=2)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3k__61hFnag"
      },
      "source": [
        "---\n",
        "### Exercise 3: Experiments with embeddings (8 points)\n",
        "Train 6 models with embedding sizes in [2,4,8,16,32,64], keeping other settings fixed. Use the averaging strategy rather than the concatenating strategy.\n",
        "\n",
        "1. Construct a table with the training and validation accuracies of each model (after 5 training epochs).\n",
        "2. Compute the number of parameters in each model.\n",
        "3. Do learned embeddings appear to provide improved performance over the one-hot encoding? Why?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size_list = [2,4,8,16,32,64]\n",
        "execution_deatils_list = []\n",
        "for dim_sz in embedding_size_list:\n",
        "    model = build_embeddings_model(average_over_positions=True,\n",
        "                                   vocab_size=1000,\n",
        "                                   sequence_length=20,\n",
        "                                   embedding_dim=dim_sz)\n",
        "    history = model.fit(\n",
        "    x = X_train_reduced,  # our sparse padded training data\n",
        "    y = Y_train,          # corresponding binary labels\n",
        "    epochs=5,             # number of passes through the training data\n",
        "    batch_size=64,        # mini-batch size\n",
        "    validation_split=0.1, # use a fraction of the examples for validation\n",
        "    verbose=1             # display some progress output during training\n",
        "    )\n",
        "\n",
        "    history = pd.DataFrame(history.history)\n",
        "    plot_history(history)\n",
        "    param_tuple = (history.tail(1)[['accuracy', 'val_accuracy']].iloc[0,0], history.tail(1)[['accuracy', 'val_accuracy']].iloc[0,1], count_params(model.trainable_weights))\n",
        "    execution_deatils_list.append(param_tuple)"
      ],
      "metadata": {
        "id": "jxD8_MMNTF4r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(execution_deatils_list, columns = ['train_accurac', 'val_accuracy', 'model_parameters']).iloc[:,2]"
      ],
      "metadata": {
        "id": "Rr41p1zRc6RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(execution_deatils_list, columns = ['train_accurac', 'val_accuracy', 'model_parameters'])"
      ],
      "metadata": {
        "id": "Iz-k2gWK6kpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7t46ZdX2ofd"
      },
      "source": [
        "*Written answers:*\n",
        "\n",
        "1. Last training and validation accuracies of each model, executed with 5 epochs are as follows:-\n",
        "\n",
        "train_accurac|val_accuracy\n",
        "-|-\n",
        "0.718978|0.7100\n",
        "0.730222|0.7164\n",
        "0.741333|0.7208\n",
        "0.748400|0.7268\n",
        "0.752000|0.7300\n",
        "0.755111|0.7284\n",
        "\n",
        "2. Number of paramters of each model are as follows:-\n",
        "\n",
        " a. model 1 parameters 2003\n",
        " --\n",
        " b. model 2 parameters 4005\n",
        " --\n",
        " c. model 3 parameters 8009\n",
        " --\n",
        " d. model 4 parameters 16017\n",
        " --\n",
        " e. model 5 parameters 32033\n",
        " --\n",
        " f. model 6 parameters 64065\n",
        " --\n",
        "\n",
        "3. Learned encoding holds the contextual meaning whereas one hot encoding is just a static lookup, does not preserve the contextual meaning, thus two sentences containing same words in different order provide same value at the end of the chain. As learned encoding contains context information, it generalizes well for the test data which one hot encoding lacks. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qokei4Hm3Y3W"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2dWOuxqKHA6"
      },
      "source": [
        "## Inspecting Learned Embeddings\n",
        "Let's retrieve the learned embedding parameters from the trained model and plot the token embeddings.\n",
        "\n",
        "The model layers in a Keras Sequential model are stored as a list and the embeddings are the first layer. We can use the get_weights() function to get a numpy array with the parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfsbGSwkaFjo"
      },
      "outputs": [],
      "source": [
        "# Display the model layers.\n",
        "display(model.layers)\n",
        "\n",
        "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
        "embeddings = model.layers[0].get_weights()[0]\n",
        "display(embeddings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings"
      ],
      "metadata": {
        "id": "PXU5Fs1ZpEhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apPWscNwcXTE"
      },
      "source": [
        "Now we'll use a fancy plotting tool called *plotly* to show the embeddings with hovertext so you can move your mouse over the points to see the corresponding tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RZMTrA0KttL"
      },
      "outputs": [],
      "source": [
        "def plot_2d_embeddings(embeddings, id_start=1, count=100):\n",
        "  # Get 1st and 2nd embedding dims for the desired tokens.\n",
        "  x1 = embeddings[id_start:id_start+count, 0]\n",
        "  x2 = embeddings[id_start:id_start+count, 1]\n",
        "  \n",
        "  # Get the corresponding words from the reverse index (for labeling).\n",
        "  tokens = [reverse_index[i] for i in range(id_start, id_start+count)]\n",
        "\n",
        "  # Plot with the plotly library.\n",
        "  data = plotly.Scatter(x=x1, y=x2, text=tokens,\n",
        "                        mode='markers', textposition='bottom left',\n",
        "                        hoverinfo='text')\n",
        "  fig = plotly.Figure(data=[data],\n",
        "                      layout=plotly.Layout(title=\"Word Embeddings\",\n",
        "                                           hovermode='closest'))\n",
        "  fig.show()\n",
        "\n",
        "# Very frequent tokens tend to be more syntactic than semantic, so let's plot\n",
        "# some rarer words.    \n",
        "plot_2d_embeddings(embeddings, id_start=500, count=500)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Mm8MjRcZ20"
      },
      "source": [
        "---\n",
        "### Exercise 4: Interpretting Embeddings (8 points)\n",
        "Notice that the 2-D embeddings fall in a narrow diagonal band.\n",
        "\n",
        "1. Have the learned embeddings separated positive and negative words? What is the most negative word? Does this make sense?\n",
        "2. Give 2 examples of words that seem to have surprising embedding values and try to explain their positions. For example, what's going on with the tokens '7', '8', and '9'?\n",
        "3. The embedding for 'crazy' is very close to (0,0). Explain what this means in terms of the model's output.\n",
        "4. Can you explain what you think the 2 learned embedding dimensions mean, if anything?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reverse_index[7], reverse_index[8], reverse_index[9]"
      ],
      "metadata": {
        "id": "DtTl8P6lDMVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_qAAvvo2y3t"
      },
      "source": [
        "*Written answers:*\n",
        "\n",
        "1. Mosr negative word according to the above plot is \"avoid\". It makes sense as the word implies to stay away from something/someone and we do that when the person / things is not comfortable, thus a negativity is associated with it which causes the aversion. \n",
        "I do not see that the learned embeddings seperate well positive and negative words. Ex. \"able\" is identified as a negative word along with other words like \"editing\", \"development\", \"writer\" etc. Similarly, \"won't\", \"doubt\", \"haven't\", \"weird\", \"fighting\" are characterized as postive words.\n",
        "2. The tokens \"7\", \"8\" and \"9\" are identified as positive tokens. The reverse index shows \"7\", \"8\" and \"9\" correspond to 'of', 'to', 'is' respectively. These words are not necessarily postive words, rather are stop words. These numeric codes appear in the far end of the postive spectrum which denotes they are not so common positive words.\n",
        "3. The embedding for 'crazy' is very close to (0,0). We use the term \"crazy\" in either way, postive or negative. Ex. The person is crazy can convey either good or bad. Thus it's very close to neutral value (0,0).\n",
        "4. 2 learned embedding dimensions mean how frequently the two words appear in the same sentence. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyJfurXW3Y3i"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXCitmUvxfwb"
      },
      "source": [
        "## Scaling Up!\n",
        "Remember how we limited our input sequences to 20 tokens and 1000 vocabulary entries? Let's see how well we can do using more data and bigger models (more parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKZDEGS7xzr6"
      },
      "source": [
        "### Exercise 5: Improve Results (8 points)\n",
        "Using pieces of code from above, set up and train a model that improves the validation accuracy to at least 80%. You should include the following elements:\n",
        "\n",
        "1. Truncate and pad input to the desired length.\n",
        "2. Limit vocabulary to the desired size.\n",
        "3. Set up a model using embeddings.\n",
        "4. Add an additional layer or layers (after the embeddings layer and before the output layer).\n",
        "5. Evaluate on the test data. Remember to apply the same pre-processing to the test data. You can use model.evaluate()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekbJ4sIq2hID"
      },
      "outputs": [],
      "source": [
        "# Keras has a util to create one-hot encodings.\n",
        "max_length=256\n",
        "vocab_size=2000 #len(index)#2000\n",
        "X_train_padded_1 = pad_data(X_train, max_length=max_length)\n",
        "X_train_reduced_1 = limit_vocab(X_train_padded_1, max_token_id=vocab_size)\n",
        "X_train_one_hot_1 = tf.keras.utils.to_categorical(X_train_reduced_1)\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "tf.random.set_seed(0)\n",
        "\n",
        "model_scale = tf.keras.Sequential()\n",
        "model_scale.add(tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size,\n",
        "      output_dim=2,\n",
        "      input_length=max_length)\n",
        "  )\n",
        "\n",
        "model_scale.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model_scale.add(Dense(16, activation='relu'))\n",
        "\n",
        "# add Dropout for regularization\n",
        "#model_scale.add(Dropout(0.1))\n",
        "\n",
        "model_scale.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation='sigmoid'         # apply the sigmoid function!\n",
        "  ))\n",
        "\n",
        "model_scale.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "history_scale = model_scale.fit(\n",
        "  x = X_train_reduced_1,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history_scale = pd.DataFrame(history_scale.history)\n",
        "plot_history(history_scale)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_padded_1 = pad_data(X_test, max_length=max_length)\n",
        "#X_test_reduced_1 = limit_vocab(X_test_padded_1, max_token_id=vocab_size)"
      ],
      "metadata": {
        "id": "9Gb7Zvx4BR4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_scale.predict(X_test_padded_1)"
      ],
      "metadata": {
        "id": "buiuZfaHBa_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_test"
      ],
      "metadata": {
        "id": "yjNOtLAcOKbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = model_scale.evaluate(X_test_padded_1)\n",
        "for name, value in zip(model_scale.metrics_names, test_results):\n",
        "  print(\"%s: %.3f\" % (name, value))"
      ],
      "metadata": {
        "id": "NIkKK0mPBvrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EEJI9yC2jOJ"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}