{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/SM_MIDS_W207_HW/blob/main/02_Linear_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "# Lab 2\n",
        "Let's start with the same artificial data we used in Lab 1. Remember that we considered 2 models:\n",
        "1. $M_1(x) = x+5$ \n",
        "2. $M_2(x) = 2x+1$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "outputs": [],
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ulmn_bFdU87t"
      },
      "outputs": [],
      "source": [
        "def create_1d_data(num_examples=10, w=2, b=1, random_scale=1):\n",
        "  \"\"\"Create X, Y data with a linear relationship with added noise.\n",
        "\n",
        "  Args:\n",
        "    num_examples: number of examples to generate\n",
        "    w: desired slope\n",
        "    b: desired intercept\n",
        "    random_scale: add uniform noise between -random_scale and +random_scale\n",
        "\n",
        "  Returns:\n",
        "    X and Y with shape (num_examples)\n",
        "  \"\"\"\n",
        "  X = np.arange(num_examples)\n",
        "  np.random.seed(4)  # consistent random number generation\n",
        "  deltas = np.random.uniform(low=-random_scale, high=random_scale, size=X.shape)\n",
        "  Y = b + deltas + w * X\n",
        "  return X, Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6qJg0IiYVJ8U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "eb6f1dee-8ff9-4b8f-cde7-484525301c57"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Create some artificial data using create_1d_data.\n",
        "X, Y = create_1d_data()\n",
        "plt.scatter(X, Y)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCWuhbfLNmna"
      },
      "source": [
        "## Notation\n",
        "In our artificial data, things are pretty simple: each input example is just a single value. But soon, each input example will include multiple values or *features*, so we need some conventions to avoid confusion.\n",
        "\n",
        "Let's start with the inputs:\n",
        "\n",
        "\\begin{align}\n",
        "X =\n",
        "\\begin{pmatrix}\n",
        "x^{(0)} \\\\\n",
        "x^{(1)} \\\\\n",
        "\\vdots \\\\\n",
        "x^{(m-1)}\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "* Capital $X$ refers to all input examples together.\n",
        "* Lowercase $x$ refers to an individual input example; we use $x^{(i)}$ to refer to input example $i$; there are $m$ total examples.\n",
        "\n",
        "Further, each input example $x$ could itself be a vector of feature values:\n",
        "\n",
        "\\begin{align}\n",
        "x = [x_0, x_1, \\dots x_{n-1}]\n",
        "\\end{align}\n",
        "\n",
        "* Lowercase $x$ refers to all input features together for an individual input example.\n",
        "* $x_i$ refers to feature $i$ for an input example $x$; there are $n$ total features.\n",
        "\n",
        "Similarly, we can index labels $y^{(i)}$ in $Y$, which we can think of as a column vector where $y^{(i)}$ is the label for $x^{(i)}$.\n",
        "\n",
        "\\begin{align}\n",
        "Y =\n",
        "\\begin{pmatrix}\n",
        "y^{(0)} \\\\\n",
        "y^{(1)} \\\\\n",
        "\\vdots \\\\\n",
        "y^{(m-1)}\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "In general, we're using matrix notation. Rows refer to examples and columns refer to features. If we want to be very specific and refer to a particular feature of a particular input example, we can use $x_{i,j}$ for input $i$, feature $j$. Using matrices will be useful for coding ML algorithms since most of the operations we will do can be expressed as operations on matrices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2szkkNDvsCfn"
      },
      "source": [
        "##Parameter Vectors\n",
        "Let's prepare to learn a linear model $h(x)$ that approximates values of $Y$ from corresponding values of $X$. Since our input data has only one feature, our model will have two parameters (also called weights), which we'll refer to collectively as $W$:\n",
        "\n",
        "\\begin{align}\n",
        "h(x) = w_0 + w_1x\n",
        "\\end{align}\n",
        "\n",
        "Notice that if we prepend an extra feature (column) to $X$ that is always $1$, we can rewrite our model using a matrix multiplication:\n",
        "\n",
        "\\begin{align}\n",
        "h(x) = w_0x_0 + w_1x_1 = xW^T\n",
        "\\end{align}\n",
        "\n",
        "To make this matrix formulation as clear as possible, this is:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y} = xW^T =\n",
        "\\begin{pmatrix}\n",
        "x_0 & x_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "In addition, if we wanted to apply our model to *all* inputs $X$, we could simply use $XW^T$:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{Y} = XW^T =\n",
        "\\begin{pmatrix}\n",
        "x_{0,0} & x_{0,1} \\\\\n",
        "x_{1,0} & x_{1,1} \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "x_{m-1,0} & x_{m-1,1} \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Remember that [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) requires the inner dimensions to line up: \n",
        "\n",
        "\\begin{align}\n",
        "X_{\\{m \\times n\\}} W^T_{\\{n \\times 1 \\}}  = \\hat{Y}_{\\{m \\times 1 \\}}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NXo1n9j1LMT"
      },
      "source": [
        "### Exercise 1: Practice with Parameters\n",
        "Add a column of 1s to $X$. Then, use matrix multiplication (np.dot) to apply $M_1$ and $M_2$ (from above) to produce vectors of predictions. Print the shapes of the predictions to validate that they have the same shape as $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0FIiQ-j6tgn"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aBEZ_QOX6qOi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9109b86c-f175-4ab7-994a-b82bef2009be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "M1: \n",
            " [ 5.  6.  7.  8.  9. 10. 11. 12. 13. 14.]\n",
            "M2: \n",
            " [ 1.  3.  5.  7.  9. 11. 13. 15. 17. 19.]\n",
            "Y shape:  (10,)\n",
            "np.dot(X_with_1s, w1).shape:  (10,)\n",
            "np.dot(X_with_1s, w2).shape:  (10,)\n"
          ]
        }
      ],
      "source": [
        "# Add a column of 1s to X by using np.c_ to concatenate with the current values.\n",
        "X_with_1s = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "# YOUR CODE HERE\n",
        "M1 = X + 5\n",
        "M2 = 2*X + 1\n",
        "#w1 = np.array([5, 1]).reshape(2, 1)\n",
        "#w2 = np.array([1, 2]).reshape(2, 1)\n",
        "w1 = np.array([5, 1])\n",
        "w2 = np.array([1, 2])\n",
        "print(\"M1: \\n\", np.dot(X_with_1s, w1)) \n",
        "print(\"M2: \\n\", np.dot(X_with_1s, w2))\n",
        "print(\"Y shape: \", Y.shape)\n",
        "print(\"np.dot(X_with_1s, w1).shape: \", np.dot(X_with_1s, w1).shape)\n",
        "print(\"np.dot(X_with_1s, w2).shape: \", np.dot(X_with_1s, w2).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUNVK2acFMQ0"
      },
      "source": [
        "## Gradient Descent\n",
        "Here we'll demonstrate gradient descent for linear regression to learn the weight vector $W$. We'll use the more specific notation $h_W(x)$ since we want to specify that $h$ is parameterized by $W$. As above, we'll assume that $x_0=1$ so we can write $h$ as a sum or a matrix product:\n",
        "\n",
        "\\begin{align}\n",
        "h_W(x) = \\sum_{i=0}^{n-1} w_i x_i = x W^T\n",
        "\\end{align}\n",
        "\n",
        "In the derivation that follows, we'll use summations, but in the code below, we'll use matrix computations.\n",
        "\n",
        "In linear regression, we compute the loss, $J(W)$ from the mean squared difference between predictions $h_W(x)$ and targets $y$. In the following equation, we average the loss over each of the $m$ training examples.\n",
        "\n",
        "\\begin{align}\n",
        "J(W) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (h_W(x^{(i)}) - y^{(i)})^2\n",
        "\\end{align}\n",
        "\n",
        "Dividing by $2$ simplifies the formula of the gradient, since it cancels out the constant $2$ from by the derivative of the squared term (see below). Remember that the gradient is a vector of partial derivatives for each $w_j$ (holding the other elements of $w$ constant). The gradient points in direction of steepest ascent for the loss function $J$.\n",
        "\n",
        "Here we derive the parameter update rule by computing the gradient of the loss function. We need a derivative for each feature in $x$, so we'll show how to compute the derivative with respect to $w_j$. For simplicity, let's assume we have only one training example ($m = 1$):\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial w_j} J(W) &= \\frac{\\partial}{\\partial w_j} \\frac{1}{2} (h_W(x) - y)^2 \\tag{1}\\\\\n",
        "&= 2 \\cdot \\frac{1}{2} (h_W(x) - y) \\cdot \\frac{\\partial}{\\partial w_j} (h_W(x) - y) \\tag{2}\\\\\n",
        "&= (h_W(x) - y) \\frac{\\partial}{\\partial w_j} \\left(\\sum_{i=0}^{n-1} w_i x_i - y \\right) \\tag{3}\\\\\n",
        "&= (h_W(x) - y)x_j \\tag{4}\n",
        "\\end{align}\n",
        "\n",
        "The derivation has 2 key steps:\n",
        "\n",
        "(1) Apply the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) (step 1 -> 2).\n",
        "\n",
        "(2) The derivative with respect to $w_j$ of $h_W(x)$ is only non-zero for $w_j x_j$. For this component, the derivative is $x_j$ since the feature value is treated as a constant (step 3 -> 4).\n",
        "\n",
        "Ok, that's it. We can now implement gradient descent for linear regression. The only difference in the code below is that it computes the loss as an average over all training examples (rather than just a single example)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaXYiTm9ftRf"
      },
      "source": [
        "### Exercise 2: Implementing Gradient Descent for Linear Regression\n",
        "Fill in the `NotImplemented` parts of the gradient descent function below. There are detailed comments to help guide you. Note that this function uses vectors and matrices so you'll want to use numpy functions like `np.dot` to multiply them, for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTlTUJkS4DzQ"
      },
      "source": [
        "##### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_hP9rzDyFXTg"
      },
      "outputs": [],
      "source": [
        "from numpy.lib.function_base import gradient\n",
        "def gradient_descent(inputs, outputs, learning_rate, num_epochs):\n",
        "  \"\"\"Apply the gradient descent algorithm to learn learn linear regression.\n",
        "\n",
        "  Args:\n",
        "    inputs: A 2-D array where each column is an input feature and each\n",
        "            row is a training example.\n",
        "    outputs: A 1-D array containing the real-valued\n",
        "             label corresponding to the input data in the same row.\n",
        "    learning_rate: The learning rate to use for updates.\n",
        "    num_epochs: The number of passes through the full training data.\n",
        "\n",
        "  Returns:\n",
        "    weights: A 2-D array with the learned weights after each training epoch.\n",
        "    losses: A 1-D array with the loss after each epoch.\n",
        "  \"\"\"\n",
        "  # m = number of examples, n = number of features\n",
        "  m, n = inputs.shape\n",
        "  #print(f\"inputs.shape : {inputs.shape}\")\n",
        "  \n",
        "  # We'll use a vector of size n to store the learned weights and initialize\n",
        "  # all weights to 1. \n",
        "  W = np.ones(n) # n is a 1-dim array with length = number of features + 1\n",
        "  #print(W)\n",
        "  \n",
        "  # Keep track of the training loss and weights after each step.\n",
        "  losses = []\n",
        "  weights = []\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    # Append the old weights to the weights list to keep track of them.\n",
        "    weights.append(W)\n",
        "\n",
        "    # Evaluate the current predictions for the training examples given\n",
        "    # the current estimate of W (you did this in exercise 1). \n",
        "    predictions = np.dot(inputs, W.T)\n",
        "    #print(\"predictions.shape\", predictions.shape)\n",
        "\n",
        "    # Find the difference between the predictions and the actual target\n",
        "    # values.\n",
        "    diff =  predictions - outputs\n",
        "    #print(\"\\noutputs:\\n\",outputs, \"\\npredictions:\\n\",predictions, \"\\ndiff:\\n\",diff)\n",
        "    #print(\"diff.shape\",diff.shape)\n",
        "    \n",
        "    # In standard linear regression, we want to minimize the sum of squared\n",
        "    # differences. Compute the mean squared error loss. Don't bother with the\n",
        "    # 1/2 scaling factor here.\n",
        "    loss = np.sum(np.square(diff)) / m\n",
        "    #print(\"loss\",loss)\n",
        "\n",
        "    # Append the loss to the losses list to keep a track of it.\n",
        "    losses.append(loss)\n",
        "    \n",
        "    # Compute the gradient with respect to the loss.\n",
        "    # [Formula (4) in the Gradient Descent Implementation]\n",
        "    # Dim of diff = 1 X m, dim of weight = 1 X n\n",
        "    #print(diff.shape, inputs.shape) \n",
        "    \"\"\"\n",
        "    gradient = np.ones(n)\n",
        "    #print(f\"gradient.shape: {gradient.shape}\")\n",
        "    if n > 1:\n",
        "        gradient[0] = np.sum(diff) / len(inputs)\n",
        "        gradient[1] = np.sum(np.dot(diff, inputs)) / len(inputs)\n",
        "    else:\n",
        "        gradient[0] = np.sum(np.dot(diff, inputs)) / len(inputs)\n",
        "    #print(\"gradient:\\n\",gradient)\n",
        "    \"\"\"\n",
        "    gradient = np.dot(diff, inputs) / m\n",
        "    # Update weights, scaling the gradient by the learning rate.\n",
        "    W = W - learning_rate * gradient\n",
        "    #print(\"\\nW:\\n\",W)\n",
        "    #print(\"***\")\n",
        "    \n",
        "  return np.array(weights), np.array(losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXN_YY-daSPK"
      },
      "source": [
        "Let's try running gradient descent with our artificial data and print out the results. Note that we're passing the version of the input data with a column of $1s$ so that we learn an *intercept* (also called a *bias*). We can also try learning without the intercept.\n",
        "\n",
        "Note: if your implementation of gradient descent is correct, you should get a loss of ~0.409 after 5 epochs (with a bias parameter)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "B4z23bKHayGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86d21152-2e7c-49b4-cde8-13661c8b1f6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running gradient descent...\n",
            "28.096877896274293 [1. 1.]\n",
            "5.175011571046788 [1.09314344 1.56138661]\n",
            "1.2139312663315942 [1.13389921 1.79439995]\n",
            "0.5285784110759881 [1.15286867 1.89092766]\n",
            "0.40916048838830177 [1.16277124 1.93072732]\n",
            "\n",
            "Running gradient descent without biases...\n",
            "38.4112216429843 [1.]\n",
            "8.073448610301934 [1.65138661]\n",
            "2.4639943765589676 [1.93148285]\n",
            "1.426806288739892 [2.05192424]\n",
            "1.235030211302147 [2.10371403]\n"
          ]
        }
      ],
      "source": [
        "print('Running gradient descent...')\n",
        "weights, losses = gradient_descent(X_with_1s, Y, learning_rate = .02, num_epochs = 5)\n",
        "for W, loss in zip(weights, losses):\n",
        "  print(loss, W)\n",
        "\n",
        "print('\\nRunning gradient descent without biases...')\n",
        "# Make sure we're providing an input with the right 2-D shape.\n",
        "X_without_1s = np.expand_dims(X, axis=0).T\n",
        "#print(X_without_1s)\n",
        "weights_without_bias, losses_without_bias = gradient_descent(X_without_1s, Y,learning_rate = .02, num_epochs = 5)\n",
        "for W, loss in zip(weights_without_bias, losses_without_bias):\n",
        "  print(loss, W)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7vGx68Ec2Si"
      },
      "source": [
        "### Exercise 3: Interpreting the Model\n",
        "Write down the learned model with and without an intercept term. Which model fits the data better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlVWFCRtqZIZ"
      },
      "source": [
        "#### Student Solution\n",
        "\n",
        "### Model with bias    : y = 1.16277124 + 1.93072732 * x\n",
        "### Model without bias : y = 2.10371403 * x\n",
        "\n",
        "### Loss for model with bias is 0.40916048838830177\n",
        "### Loss for model without bias is 1.235030211302147\n",
        "\n",
        "### We consider a model better when it provides lower error, in our case mean square error(MSE). Between the above two models, model with bias provides lower MSE(0.409 vs 1.23), thus a better model between these 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-DdmWmPrDvk"
      },
      "source": [
        "## Gradient Descent Progress\n",
        "Let's write a function that lets us visualize the progress of gradient descent during training. Our gradient descent function already provides intermediate weight vectors and losses after each epoch, so we just need to plot these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lWcTeR_gNz9d"
      },
      "outputs": [],
      "source": [
        "def plot_learning(inputs, outputs, weights, losses):\n",
        "  \"\"\"Plot predictions and losses after each training epoch.\n",
        "\n",
        "  Args:\n",
        "    inputs: A 2-D array where each column is an input feature and each\n",
        "            row is a training example.\n",
        "    outputs: A 1-D array containing the real-valued\n",
        "             label corresponding to the input data in the same row.\n",
        "    weights: A 2-D array with the learned weights after each training epoch.\n",
        "    losses: A 1-D array with the loss after each epoch.\n",
        "  \"\"\"\n",
        "  # Create a figure.\n",
        "  plt.figure(1, figsize=[10,4])\n",
        "\n",
        "  # The first subplot will contain the predictions. Start by plotting the\n",
        "  # outputs (Y).\n",
        "  plt.subplot(121)\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.xticks(inputs[:,1])\n",
        "  plt.scatter(inputs[:,1], outputs, color='black', label='Y')\n",
        "  \n",
        "  # For each epoch, retrieve the estimated weights W, compute predictions, and\n",
        "  # plot the resulting line.\n",
        "  num_epochs = len(weights)\n",
        "  for i in range(num_epochs):\n",
        "    W = weights[i]\n",
        "    predictions = np.dot(inputs, W.T)\n",
        "    plt.plot(inputs[:,1], predictions, label='Epoch %d' %i)\n",
        "  plt.legend()\n",
        "\n",
        "  # The second subplot will contain the losses.\n",
        "  plt.subplot(122)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xticks(range(num_epochs))\n",
        "  plt.plot(range(num_epochs), losses, marker='o', color='black',\n",
        "           linestyle='dashed')\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WECpzvgkrkFk"
      },
      "source": [
        "### Exercise 4: Plotting Progress\n",
        "\n",
        "Re-run gradient descent using X_with_1s, but this time with learning_rate=0.01 and num_epochs=7.\n",
        "\n",
        "Run the plot_learning function using the weights and losses returned by gradient_descent (from above) and answer the following questions:\n",
        "\n",
        "1. Is learning converging faster or slower than when we used learning_rate=0.02?\n",
        "2. If you continue training, will the loss eventually reach 0?\n",
        "3. If you continue training, will the model eventually converge to $h(x)=2x+1$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjfwzNhzrB1D"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "v7CK6106tWgi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "outputId": "9d6fbbc9-99f9-4c77-e4af-34181c542b79"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running gradient descent...\n",
            "28.096877896274293 [1. 1.]\n",
            "14.270645620677294 [1.04657172 1.28069331]\n",
            "7.343191347047517 [1.08004652 1.47929329]\n",
            "3.8721252845709295 [1.10424958 1.61978592]\n",
            "2.1327609079282 [1.12188843 1.71914901]\n",
            "1.2610055930000243 [1.13487956 1.78939986]\n",
            "0.8239365677061619 [1.14457949 1.83904463]\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x288 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "print('Running gradient descent...')\n",
        "weights, losses = gradient_descent(X_with_1s, Y, learning_rate = .01, num_epochs = 7)\n",
        "for W, loss in zip(weights, losses):\n",
        "  print(loss, W)\n",
        "plot_learning(X_with_1s, Y, weights, losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-mgY_e5upTL"
      },
      "source": [
        "WRITE YOUR ANSWERS HERE\n",
        "\n",
        "1. Learning converges faster with learning rate .02 compared to .01. As the learning rate is larger, it will update the parameters at faster scale, hence, convergence will be faster.\n",
        "2. No, even if I continue training with the same learning rate and increase the epochs, the loss does not reach 0 actually.\n",
        "3. If you continue training, the model ()=2+1 will converge to 0.27655 with the intercept 1.77514503 and slope 1.86250177."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib.function_base import gradient\n",
        "def gradient_descent_1(inputs, outputs, learning_rate, num_epochs):\n",
        "  \"\"\"Apply the gradient descent algorithm to learn learn linear regression.\n",
        "\n",
        "  Args:\n",
        "    inputs: A 2-D array where each column is an input feature and each\n",
        "            row is a training example.\n",
        "    outputs: A 1-D array containing the real-valued\n",
        "             label corresponding to the input data in the same row.\n",
        "    learning_rate: The learning rate to use for updates.\n",
        "    num_epochs: The number of passes through the full training data.\n",
        "\n",
        "  Returns:\n",
        "    weights: A 2-D array with the learned weights after each training epoch.\n",
        "    losses: A 1-D array with the loss after each epoch.\n",
        "  \"\"\"\n",
        "  # m = number of examples, n = number of features\n",
        "  m, n = inputs.shape\n",
        "  #print(f\"inputs.shape : {inputs.shape}\")\n",
        "  \n",
        "  # We'll use a vector of size n to store the learned weights and initialize\n",
        "  # all weights to 1. \n",
        "  W = np.array([1, 2]) #np.ones(n) # n is a 1-dim array with length = number of features + 1\n",
        "  #print(W)\n",
        "  \n",
        "  # Keep track of the training loss and weights after each step.\n",
        "  losses = []\n",
        "  weights = []\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "    # Append the old weights to the weights list to keep track of them.\n",
        "    weights.append(W)\n",
        "\n",
        "    # Evaluate the current predictions for the training examples given\n",
        "    # the current estimate of W (you did this in exercise 1). \n",
        "    predictions = np.dot(inputs, W.T)\n",
        "    #print(\"predictions.shape\", predictions.shape)\n",
        "\n",
        "    # Find the difference between the predictions and the actual target\n",
        "    # values.\n",
        "    diff =  predictions - outputs\n",
        "    #print(\"\\noutputs:\\n\",outputs, \"\\npredictions:\\n\",predictions, \"\\ndiff:\\n\",diff)\n",
        "    #print(\"diff.shape\",diff.shape)\n",
        "    \n",
        "    # In standard linear regression, we want to minimize the sum of squared\n",
        "    # differences. Compute the mean squared error loss. Don't bother with the\n",
        "    # 1/2 scaling factor here.\n",
        "    loss = np.sum(np.square(diff)) / m\n",
        "    #print(\"loss\",loss)\n",
        "\n",
        "    # Append the loss to the losses list to keep a track of it.\n",
        "    losses.append(loss)\n",
        "    \n",
        "    # Compute the gradient with respect to the loss.\n",
        "    # [Formula (4) in the Gradient Descent Implementation]\n",
        "    # Dim of diff = 1 X m, dim of weight = 1 X n\n",
        "    #print(diff.shape, inputs.shape) \n",
        "    \"\"\"\n",
        "    gradient = np.ones(n)\n",
        "    #print(f\"gradient.shape: {gradient.shape}\")\n",
        "    if n > 1:\n",
        "        gradient[0] = np.sum(diff) / len(inputs)\n",
        "        gradient[1] = np.sum(np.dot(diff, inputs)) / len(inputs)\n",
        "    else:\n",
        "        gradient[0] = np.sum(np.dot(diff, inputs)) / len(inputs)\n",
        "    #print(\"gradient:\\n\",gradient)\n",
        "    \"\"\"\n",
        "    gradient = np.dot(diff, inputs) / m\n",
        "    # Update weights, scaling the gradient by the learning rate.\n",
        "    W = W - learning_rate * gradient\n",
        "    #print(\"\\nW:\\n\",W)\n",
        "    #print(\"***\")\n",
        "    \n",
        "  return np.array(weights), np.array(losses)\n",
        "\n",
        "X, Y = create_1d_data()\n",
        "# Add a column of 1s to X by using np.c_ to concatenate with the current values.\n",
        "X_with_1s = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "print('Running gradient descent...')\n",
        "weights, losses = gradient_descent_1(X_with_1s, Y, learning_rate = .02, num_epochs = 1000)\n",
        "for W, loss in zip(weights, losses):\n",
        "  print(loss, W)"
      ],
      "metadata": {
        "id": "aQeBiSuYwGvn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8566bad8-61d4-4306-9474-4c9483ce66fd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running gradient descent...\n",
            "0.45821672338761366 [1. 2.]\n",
            "0.4516901861371604 [1.00314344 1.99138661]\n",
            "0.4489436230753161 [1.00699921 1.98739995]\n",
            "0.446868332874575 [1.01113667 1.98533866]\n",
            "0.4449270426668278 [1.01537689 1.98407993]\n",
            "0.4430267287084674 [1.0196456  1.98315706]\n",
            "0.4411511192017067 [1.02391199 1.98237604]\n",
            "0.43929720447999515 [1.02816334 1.98165623]\n",
            "0.4374642683135258 [1.03239445 1.98096409]\n",
            "0.43565199290326023 [1.03660323 1.98028567]\n",
            "0.43386013146976155 [1.0407889  1.97961516]\n",
            "0.4320884516641569 [1.04495119 1.97895013]\n",
            "0.4303367257673295 [1.04909009 1.97828956]\n",
            "0.42860472897263363 [1.05320567 1.97763301]\n",
            "0.4268922390654219 [1.05729802 1.9769803 ]\n",
            "0.42519903634415623 [1.06136727 1.97633132]\n",
            "0.4235249035835123 [1.06541354 1.97568602]\n",
            "0.42186962600501 [1.06943697 1.97504438]\n",
            "0.4202329912492006 [1.07343767 1.97440637]\n",
            "0.4186147893483742 [1.07741578 1.97377196]\n",
            "0.4170148126996174 [1.08137143 1.97314113]\n",
            "0.4154328560381777 [1.08530474 1.97251387]\n",
            "0.41386871641112866 [1.08921583 1.97189015]\n",
            "0.4123221931513335 [1.09310484 1.97126995]\n",
            "0.41079308785169977 [1.09697188 1.97065326]\n",
            "0.40928120433972576 [1.10081709 1.97004004]\n",
            "0.40778634865233404 [1.10464058 1.96943029]\n",
            "0.4063083290109857 [1.10844248 1.96882398]\n",
            "0.4048469557970793 [1.11222291 1.9682211 ]\n",
            "0.4034020415276209 [1.11598199 1.96762162]\n",
            "0.40197340083117367 [1.11971984 1.96702553]\n",
            "0.40056085042407574 [1.12343658 1.9664328 ]\n",
            "0.39916420908692635 [1.12713234 1.96584342]\n",
            "0.3977832976413355 [1.13080722 1.96525737]\n",
            "0.3964179389269391 [1.13446135 1.96467463]\n",
            "0.3950679577786693 [1.13809484 1.96409518]\n",
            "0.3937331810042811 [1.14170782 1.963519  ]\n",
            "0.39241343736213596 [1.14530039 1.96294608]\n",
            "0.3911085575392289 [1.14887267 1.96237639]\n",
            "0.38981837412947107 [1.15242478 1.96180992]\n",
            "0.3885427216122101 [1.15595683 1.96124665]\n",
            "0.38728143633099615 [1.15946893 1.96068656]\n",
            "0.3860343564725848 [1.1629612  1.96012963]\n",
            "0.38480132204618017 [1.16643375 1.95957584]\n",
            "0.38358217486290636 [1.16988668 1.95902519]\n",
            "0.38237675851551567 [1.17332012 1.95847764]\n",
            "0.38118491835832025 [1.17673417 1.95793319]\n",
            "0.380006501487354 [1.18012894 1.95739181]\n",
            "0.37884135672075614 [1.18350453 1.95685348]\n",
            "0.37768933457937476 [1.18686106 1.9563182 ]\n",
            "0.37655028726759054 [1.19019864 1.95578594]\n",
            "0.3754240686543543 [1.19351737 1.95525669]\n",
            "0.37431053425444266 [1.19681736 1.95473042]\n",
            "0.3732095412099187 [1.20009871 1.95420713]\n",
            "0.3721209482718044 [1.20336153 1.95368679]\n",
            "0.3710446157819626 [1.20660593 1.9531694 ]\n",
            "0.36998040565517587 [1.209832   1.95265492]\n",
            "0.3689281813614337 [1.21303986 1.95214335]\n",
            "0.36788780790841524 [1.2162296  1.95163466]\n",
            "0.36685915182417583 [1.21940132 1.95112885]\n",
            "0.3658420811400139 [1.22255514 1.9506259 ]\n",
            "0.3648364653735491 [1.22569114 1.95012579]\n",
            "0.36384217551197895 [1.22880943 1.9496285 ]\n",
            "0.3628590839955277 [1.23191012 1.94913402]\n",
            "0.36188706470108156 [1.23499329 1.94864233]\n",
            "0.36092599292600813 [1.23805905 1.94815342]\n",
            "0.3599757453721575 [1.2411075  1.94766727]\n",
            "0.3590362001300448 [1.24413874 1.94718386]\n",
            "0.35810723666320976 [1.24715285 1.94670319]\n",
            "0.3571887357927531 [1.25014995 1.94622522]\n",
            "0.35628057968204613 [1.25313011 1.94574996]\n",
            "0.35538265182161305 [1.25609345 1.94527739]\n",
            "0.3544948370141859 [1.25904006 1.94480748]\n",
            "0.35361702135992285 [1.26197002 1.94434022]\n",
            "0.3527490922417965 [1.26488344 1.94387561]\n",
            "0.351890938311148 [1.2677804  1.94341361]\n",
            "0.3510424494733984 [1.270661   1.94295423]\n",
            "0.3502035168739265 [1.27352534 1.94249744]\n",
            "0.3493740328841036 [1.2763735  1.94204323]\n",
            "0.34855389108748713 [1.27920558 1.94159159]\n",
            "0.3477429862661624 [1.28202166 1.94114249]\n",
            "0.34694121438725073 [1.28482184 1.94069593]\n",
            "0.34614847258956 [1.28760621 1.9402519 ]\n",
            "0.3453646591703865 [1.29037485 1.93981037]\n",
            "0.3445896735724724 [1.29312786 1.93937133]\n",
            "0.34382341637109787 [1.29586532 1.93893478]\n",
            "0.3430657892613317 [1.29858732 1.93850069]\n",
            "0.3423166950454171 [1.30129395 1.93806905]\n",
            "0.34157603762030303 [1.30398529 1.93763985]\n",
            "0.34084372196531143 [1.30666144 1.93721307]\n",
            "0.3401196541299531 [1.30932247 1.9367887 ]\n",
            "0.33940374122186745 [1.31196848 1.93636673]\n",
            "0.33869589139491185 [1.31459954 1.93594714]\n",
            "0.33799601383737254 [1.31721574 1.93552992]\n",
            "0.33730401876031807 [1.31981717 1.93511506]\n",
            "0.3366198173860791 [1.32240391 1.93470254]\n",
            "0.3359433219368587 [1.32497604 1.93429235]\n",
            "0.3352744456234699 [1.32753365 1.93388448]\n",
            "0.33461310263420363 [1.33007681 1.93347891]\n",
            "0.3339592081238188 [1.33260561 1.93307563]\n",
            "0.3333126782026568 [1.33512012 1.93267463]\n",
            "0.3326734299258789 [1.33762044 1.93227589]\n",
            "0.3320413812828261 [1.34010664 1.9318794 ]\n",
            "0.3314164511864976 [1.3425788  1.93148516]\n",
            "0.33079855946314707 [1.345037   1.93109314]\n",
            "0.33018762684199754 [1.34748131 1.93070333]\n",
            "0.32958357494507207 [1.34991182 1.93031573]\n",
            "0.3289863262771374 [1.35232861 1.92993031]\n",
            "0.32839580421576264 [1.35473175 1.92954707]\n",
            "0.32781193300148864 [1.35712131 1.92916599]\n",
            "0.32723463772811046 [1.35949738 1.92878707]\n",
            "0.32666384433306395 [1.36186004 1.92841029]\n",
            "0.3260994795879282 [1.36420935 1.92803563]\n",
            "0.3255414710890271 [1.36654539 1.92766309]\n",
            "0.3249897472481428 [1.36886824 1.92729266]\n",
            "0.32444423728333255 [1.37117798 1.92692431]\n",
            "0.3239048712098428 [1.37347467 1.92655805]\n",
            "0.32337157983113857 [1.37575839 1.92619385]\n",
            "0.32284429473001836 [1.37802921 1.92583171]\n",
            "0.3223229482598409 [1.38028721 1.92547162]\n",
            "0.3218074735358469 [1.38253246 1.92511356]\n",
            "0.3212978044265756 [1.38476502 1.92475752]\n",
            "0.32079387554538363 [1.38698498 1.92440349]\n",
            "0.3202956222420533 [1.38919241 1.92405147]\n",
            "0.31980298059450235 [1.39138737 1.92370143]\n",
            "0.3193158874005787 [1.39356993 1.92335336]\n",
            "0.3188342801699559 [1.39574016 1.92300726]\n",
            "0.318358097116113 [1.39789814 1.92266312]\n",
            "0.31788727714841153 [1.40004394 1.92232092]\n",
            "0.317421759864254 [1.40217761 1.92198065]\n",
            "0.31696148554133796 [1.40429924 1.92164231]\n",
            "0.3165063951299918 [1.40640888 1.92130587]\n",
            "0.3160564302456009 [1.40850662 1.92097134]\n",
            "0.31561153316111545 [1.4105925  1.92063869]\n",
            "0.31517164679964715 [1.41266661 1.92030792]\n",
            "0.3147367147271428 [1.414729   1.91997902]\n",
            "0.3143066811451492 [1.41677974 1.91965198]\n",
            "0.31388149088364936 [1.41881891 1.91932679]\n",
            "0.31346108939398937 [1.42084656 1.91900343]\n",
            "0.3130454227418769 [1.42286275 1.9186819 ]\n",
            "0.31263443760046344 [1.42486757 1.91836218]\n",
            "0.31222808124350243 [1.42686106 1.91804427]\n",
            "0.31182630153858415 [1.42884329 1.91772815]\n",
            "0.31142904694044965 [1.43081433 1.91741382]\n",
            "0.3110362664843759 [1.43277423 1.91710127]\n",
            "0.31064790977963747 [1.43472307 1.91679047]\n",
            "0.31026392700304317 [1.43666091 1.91648144]\n",
            "0.30988426889254317 [1.4385878  1.91617415]\n",
            "0.3095088867409086 [1.4405038  1.91586859]\n",
            "0.3091377323894837 [1.44240899 1.91556476]\n",
            "0.3087707582220078 [1.44430342 1.91526265]\n",
            "0.3084079171585051 [1.44618715 1.91496224]\n",
            "0.3080491626492442 [1.44806024 1.91466353]\n",
            "0.3076944486687708 [1.44992276 1.91436651]\n",
            "0.3073437297099952 [1.45177475 1.91407116]\n",
            "0.3069969607783624 [1.45361629 1.91377748]\n",
            "0.3066540973860729 [1.45544743 1.91348546]\n",
            "0.30631509554637826 [1.45726823 1.91319509]\n",
            "0.3059799117679387 [1.45907874 1.91290636]\n",
            "0.30564850304924224 [1.46087903 1.91261926]\n",
            "0.3053208268730876 [1.46266916 1.91233378]\n",
            "0.3049968412011311 [1.46444917 1.91204991]\n",
            "0.30467650446849076 [1.46621913 1.91176765]\n",
            "0.30435977557841826 [1.4679791  1.91148698]\n",
            "0.3040466138970204 [1.46972913 1.91120789]\n",
            "0.3037369792480525 [1.47146927 1.91093038]\n",
            "0.3034308319077599 [1.47319959 1.91065444]\n",
            "0.30312813259978244 [1.47492013 1.91038006]\n",
            "0.30282884249011754 [1.47663096 1.91010723]\n",
            "0.30253292318213576 [1.47833213 1.90983593]\n",
            "0.30224033671165584 [1.48002369 1.90956617]\n",
            "0.3019510455420732 [1.4817057  1.90929793]\n",
            "0.3016650125595469 [1.48337821 1.90903121]\n",
            "0.30138220106823477 [1.48504127 1.90876599]\n",
            "0.30110257478558855 [1.48669495 1.90850227]\n",
            "0.30082609783769754 [1.48833928 1.90824004]\n",
            "0.3005527347546854 [1.48997433 1.9079793 ]\n",
            "0.3002824504661623 [1.49160014 1.90772002]\n",
            "0.30001521029672423 [1.49321677 1.90746221]\n",
            "0.2997509799615026 [1.49482428 1.90720585]\n",
            "0.29948972556176934 [1.4964227  1.90695094]\n",
            "0.2992314135805858 [1.4980121  1.90669747]\n",
            "0.2989760108785021 [1.49959252 1.90644544]\n",
            "0.2987234846893075 [1.50116402 1.90619482]\n",
            "0.29847380261582496 [1.50272665 1.90594562]\n",
            "0.29822693262575745 [1.50428044 1.90569783]\n",
            "0.297982843047575 [1.50582547 1.90545144]\n",
            "0.2977415025664531 [1.50736177 1.90520644]\n",
            "0.2975028802202574 [1.50888939 1.90496282]\n",
            "0.29726694539556686 [1.51040838 1.90472058]\n",
            "0.2970336678237503 [1.5119188  1.90447971]\n",
            "0.2968030175770823 [1.51342069 1.90424019]\n",
            "0.29657496506490155 [1.5149141  1.90400203]\n",
            "0.2963494810298174 [1.51639907 1.90376522]\n",
            "0.296126536543954 [1.51787566 1.90352974]\n",
            "0.29590610300524184 [1.5193439  1.90329559]\n",
            "0.29568815213374566 [1.52080386 1.90306276]\n",
            "0.29547265596803746 [1.52225557 1.90283125]\n",
            "0.29525958686160847 [1.52369908 1.90260105]\n",
            "0.29504891747932455 [1.52513445 1.90237215]\n",
            "0.29484062079391743 [1.5265617  1.90214453]\n",
            "0.29463467008251654 [1.5279809  1.90191821]\n",
            "0.29443103892322353 [1.52939208 1.90169316]\n",
            "0.2942297011917188 [1.53079529 1.90146938]\n",
            "0.2940306310579145 [1.53219058 1.90124687]\n",
            "0.29383380298263573 [1.53357798 1.90102561]\n",
            "0.29363919171434694 [1.53495756 1.90080561]\n",
            "0.2934467722859143 [1.53632934 1.90058684]\n",
            "0.29325652001139746 [1.53769337 1.90036931]\n",
            "0.29306841048288695 [1.5390497  1.90015301]\n",
            "0.2928824195673704 [1.54039838 1.89993793]\n",
            "0.29269852340363667 [1.54173943 1.89972407]\n",
            "0.29251669839921757 [1.54307292 1.89951141]\n",
            "0.29233692122735644 [1.54439887 1.89929996]\n",
            "0.2921591688240187 [1.54571733 1.8990897 ]\n",
            "0.29198341838493247 [1.54702835 1.89888062]\n",
            "0.2918096473626615 [1.54833196 1.89867273]\n",
            "0.291637833463715 [1.54962822 1.89846601]\n",
            "0.2914679546456851 [1.55091715 1.89826046]\n",
            "0.2912999891144202 [1.5521988  1.89805606]\n",
            "0.2911339153212283 [1.55347322 1.89785283]\n",
            "0.2909697119601144 [1.55474044 1.89765074]\n",
            "0.2908073579650441 [1.5560005  1.89744979]\n",
            "0.29064683250724277 [1.55725345 1.89724998]\n",
            "0.2904881149925237 [1.55849932 1.89705129]\n",
            "0.2903311850586442 [1.55973815 1.89685373]\n",
            "0.29017602257269676 [1.56096999 1.89665728]\n",
            "0.29002260762852156 [1.56219487 1.89646194]\n",
            "0.2898709205441567 [1.56341284 1.89626771]\n",
            "0.2897209418593104 [1.56462392 1.89607457]\n",
            "0.289572652332866 [1.56582817 1.89588252]\n",
            "0.28942603294041197 [1.56702562 1.89569156]\n",
            "0.28928106487180383 [1.5682163  1.89550168]\n",
            "0.2891377295287478 [1.56940026 1.89531287]\n",
            "0.2889960085224176 [1.57057754 1.89512512]\n",
            "0.28885588367109255 [1.57174816 1.89493843]\n",
            "0.2887173369978279 [1.57291218 1.8947528 ]\n",
            "0.2885803507281457 [1.57406962 1.89456822]\n",
            "0.2884449072877572 [1.57522053 1.89438468]\n",
            "0.28831098930030513 [1.57636493 1.89420218]\n",
            "0.28817857958513843 [1.57750287 1.8940207 ]\n",
            "0.2880476611551031 [1.57863439 1.89384026]\n",
            "0.2879182172143683 [1.57975952 1.89366083]\n",
            "0.28779023115626695 [1.58087829 1.89348241]\n",
            "0.28766368656116753 [1.58199075 1.893305  ]\n",
            "0.28753856719436777 [1.58309692 1.8931286 ]\n",
            "0.2874148570040117 [1.58419684 1.89295319]\n",
            "0.28729254011902816 [1.58529056 1.89277877]\n",
            "0.28717160084709803 [1.58637809 1.89260533]\n",
            "0.2870520236726383 [1.58745949 1.89243288]\n",
            "0.28693379325481344 [1.58853478 1.89226139]\n",
            "0.2868168944255659 [1.589604   1.89209088]\n",
            "0.28670131218767125 [1.59066717 1.89192133]\n",
            "0.28658703171281324 [1.59172435 1.89175274]\n",
            "0.2864740383396819 [1.59277555 1.8915851 ]\n",
            "0.286362317572093 [1.59382082 1.8914184 ]\n",
            "0.28625185507712747 [1.59486018 1.89125265]\n",
            "0.2861426366832927 [1.59589368 1.89108784]\n",
            "0.28603464837870557 [1.59692134 1.89092395]\n",
            "0.2859278763092931 [1.59794319 1.89076099]\n",
            "0.2858223067770167 [1.59895928 1.89059895]\n",
            "0.2857179262381139 [1.59996962 1.89043783]\n",
            "0.28561472130135923 [1.60097426 1.89027761]\n",
            "0.2855126787263515 [1.60197323 1.8901183 ]\n",
            "0.28541178542180895 [1.60296656 1.88995989]\n",
            "0.285312028443893 [1.60395427 1.88980237]\n",
            "0.285213394994548 [1.60493641 1.88964575]\n",
            "0.2851158724198563 [1.605913   1.88949001]\n",
            "0.2850194482084197 [1.60688408 1.88933514]\n",
            "0.28492410998974915 [1.60784967 1.88918116]\n",
            "0.2848298455326825 [1.60880981 1.88902804]\n",
            "0.28473664274381016 [1.60976453 1.88887578]\n",
            "0.2846444896659285 [1.61071386 1.88872439]\n",
            "0.2845533744765047 [1.61165782 1.88857385]\n",
            "0.2844632854861552 [1.61259646 1.88842416]\n",
            "0.2843742111371538 [1.61352979 1.88827532]\n",
            "0.28428614000194374 [1.61445785 1.88812732]\n",
            "0.2841990607816724 [1.61538068 1.88798015]\n",
            "0.28411296230474237 [1.61629829 1.88783382]\n",
            "0.28402783352537864 [1.61721071 1.88768831]\n",
            "0.28394366352221023 [1.61811799 1.88754362]\n",
            "0.2838604414968712 [1.61902014 1.88739975]\n",
            "0.28377815677261203 [1.6199172  1.88725669]\n",
            "0.28369679879293297 [1.62080919 1.88711444]\n",
            "0.2836163571202281 [1.62169614 1.88697299]\n",
            "0.28353682143444714 [1.62257809 1.88683235]\n",
            "0.28345818153177016 [1.62345505 1.88669249]\n",
            "0.2833804273232999 [1.62432707 1.88655343]\n",
            "0.28330354883376707 [1.62519415 1.88641515]\n",
            "0.2832275362002502 [1.62605634 1.88627765]\n",
            "0.28315237967091 [1.62691367 1.88614093]\n",
            "0.28307806960373955 [1.62776615 1.88600498]\n",
            "0.2830045964653262 [1.62861381 1.8858698 ]\n",
            "0.28293195082962813 [1.62945669 1.88573538]\n",
            "0.28286012337676775 [1.63029481 1.88560172]\n",
            "0.28278910489183245 [1.6311282  1.88546882]\n",
            "0.2827188862636939 [1.63195688 1.88533667]\n",
            "0.2826494584838408 [1.63278088 1.88520526]\n",
            "0.2825808126452198 [1.63360022 1.88507459]\n",
            "0.2825129399410952 [1.63441494 1.88494467]\n",
            "0.2824458316639195 [1.63522506 1.88481547]\n",
            "0.2823794792042149 [1.6360306  1.88468701]\n",
            "0.2823138740494676 [1.6368316  1.88455927]\n",
            "0.28224900778303935 [1.63762807 1.88443225]\n",
            "0.2821848720830858 [1.63842004 1.88430595]\n",
            "0.28212145872148575 [1.63920754 1.88418037]\n",
            "0.28205875956279186 [1.6399906  1.88405549]\n",
            "0.2819967665631822 [1.64076923 1.88393132]\n",
            "0.2819354717694281 [1.64154346 1.88380785]\n",
            "0.2818748673178791 [1.64231332 1.88368507]\n",
            "0.2818149454334463 [1.64307884 1.88356299]\n",
            "0.2817556984286121 [1.64384003 1.8834416 ]\n",
            "0.28169711870243846 [1.64459692 1.8833209 ]\n",
            "0.28163919873959486 [1.64534954 1.88320088]\n",
            "0.2815819311093955 [1.64609791 1.88308153]\n",
            "0.28152530846483953 [1.64684205 1.88296286]\n",
            "0.28146932354167614 [1.64758199 1.88284486]\n",
            "0.28141396915746925 [1.64831775 1.88272752]\n",
            "0.28135923821067454 [1.64904936 1.88261085]\n",
            "0.28130512367973015 [1.64977683 1.88249483]\n",
            "0.2812516186221563 [1.6505002  1.88237948]\n",
            "0.28119871617366454 [1.65121948 1.88226477]\n",
            "0.2811464095472749 [1.6519347  1.88215071]\n",
            "0.28109469203244847 [1.65264587 1.88203729]\n",
            "0.28104355699422356 [1.65335304 1.88192452]\n",
            "0.28099299787236687 [1.65405621 1.88181238]\n",
            "0.28094300818052986 [1.65475541 1.88170088]\n",
            "0.2808935815054173 [1.65545066 1.88159   ]\n",
            "0.2808447115059664 [1.65614198 1.88147975]\n",
            "0.28079639191252953 [1.6568294  1.88137013]\n",
            "0.2807486165260734 [1.65751294 1.88126112]\n",
            "0.28070137921738136 [1.65819262 1.88115273]\n",
            "0.2806546739262684 [1.65886846 1.88104495]\n",
            "0.2806084946608037 [1.65954048 1.88093778]\n",
            "0.2805628354965412 [1.66020871 1.88083121]\n",
            "0.28051769057575876 [1.66087316 1.88072525]\n",
            "0.2804730541067099 [1.66153386 1.88061988]\n",
            "0.2804289203628764 [1.66219083 1.88051511]\n",
            "0.28038528368223753 [1.66284409 1.88041094]\n",
            "0.2803421384665417 [1.66349367 1.88030735]\n",
            "0.2802994791805869 [1.66413957 1.88020434]\n",
            "0.2802573003515144 [1.66478182 1.88010192]\n",
            "0.2802155965681032 [1.66542045 1.88000007]\n",
            "0.28017436248007666 [1.66605547 1.8798988 ]\n",
            "0.2801335927974179 [1.66668691 1.8797981 ]\n",
            "0.28009328228968755 [1.66731478 1.87969797]\n",
            "0.28005342578535763 [1.6679391  1.87959841]\n",
            "0.28001401817114313 [1.6685599  1.87949941]\n",
            "0.2799750543913485 [1.6691772  1.87940097]\n",
            "0.27993652944721986 [1.669791   1.87930308]\n",
            "0.27989843839630135 [1.67040134 1.87920575]\n",
            "0.2798607763518033 [1.67100824 1.87910896]\n",
            "0.27982353848197317 [1.6716117  1.87901272]\n",
            "0.279786720009478 [1.67221176 1.87891703]\n",
            "0.27975031621078966 [1.67280843 1.87882188]\n",
            "0.2797143224155785 [1.67340173 1.87872726]\n",
            "0.27967873400611776 [1.67399168 1.87863318]\n",
            "0.27964354641668526 [1.6745783  1.87853963]\n",
            "0.2796087551329847 [1.6751616 1.8784466]\n",
            "0.2795743556915595 [1.67574161 1.87835411]\n",
            "0.279540343679225 [1.67631835 1.87826213]\n",
            "0.2795067147325004 [1.67689183 1.87817068]\n",
            "0.2794734645370493 [1.67746207 1.87807974]\n",
            "0.27944058882712663 [1.67802909 1.87798931]\n",
            "0.27940808338503037 [1.6785929 1.8778994]\n",
            "0.2793759440405605 [1.67915354 1.87780999]\n",
            "0.27934416667048584 [1.67971101 1.87772109]\n",
            "0.27931274719801413 [1.68026533 1.87763269]\n",
            "0.2792816815922653 [1.68081651 1.87754479]\n",
            "0.279250965867762 [1.68136459 1.87745738]\n",
            "0.2792205960839105 [1.68190957 1.87737047]\n",
            "0.27919056834450195 [1.68245147 1.87728405]\n",
            "0.2791608787972061 [1.68299032 1.87719812]\n",
            "0.279131523633081 [1.68352612 1.87711268]\n",
            "0.2791024990860844 [1.68405889 1.87702771]\n",
            "0.2790738014325894 [1.68458866 1.87694323]\n",
            "0.27904542699090573 [1.68511543 1.87685922]\n",
            "0.27901737212081185 [1.68563923 1.87677569]\n",
            "0.2789896332230808 [1.68616007 1.87669263]\n",
            "0.2789622067390256 [1.68667797 1.87661003]\n",
            "0.27893508915003734 [1.68719295 1.87652791]\n",
            "0.27890827697713716 [1.68770501 1.87644625]\n",
            "0.2788817667805296 [1.68821419 1.87636505]\n",
            "0.27885555515915833 [1.68872049 1.87628431]\n",
            "0.278829638750274 [1.68922393 1.87620402]\n",
            "0.2788040142289989 [1.68972452 1.87612419]\n",
            "0.2787786783079033 [1.69022229 1.8760448 ]\n",
            "0.2787536277365834 [1.69071725 1.87596587]\n",
            "0.2787288593012413 [1.69120942 1.87588738]\n",
            "0.2787043698242774 [1.6916988  1.87580934]\n",
            "0.2786801561638793 [1.69218542 1.87573174]\n",
            "0.2786562152136196 [1.6926693  1.87565457]\n",
            "0.2786325439020575 [1.69315044 1.87557784]\n",
            "0.27860913919234476 [1.69362886 1.87550154]\n",
            "0.27858599808183754 [1.69410458 1.87542568]\n",
            "0.27856311760170743 [1.69457762 1.87535024]\n",
            "0.2785404948165639 [1.69504798 1.87527523]\n",
            "0.2785181268240785 [1.69551569 1.87520064]\n",
            "0.2784960107546066 [1.69598075 1.87512648]\n",
            "0.2784741437708277 [1.69644319 1.87505273]\n",
            "0.2784525230673745 [1.69690302 1.8749794 ]\n",
            "0.2784311458704761 [1.69736025 1.87490648]\n",
            "0.27841000943760325 [1.6978149  1.87483398]\n",
            "0.2783891110571113 [1.69826698 1.87476188]\n",
            "0.27836844804789945 [1.69871651 1.87469019]\n",
            "0.2783480177590611 [1.6991635  1.87461891]\n",
            "0.2783278175695477 [1.69960797 1.87454803]\n",
            "0.27830784488782834 [1.70004992 1.87447755]\n",
            "0.2782880971515618 [1.70048938 1.87440746]\n",
            "0.2782685718272641 [1.70092636 1.87433778]\n",
            "0.2782492664099866 [1.70136087 1.87426848]\n",
            "0.27823017842299125 [1.70179293 1.87419958]\n",
            "0.27821130541743566 [1.70222254 1.87413107]\n",
            "0.2781926449720572 [1.70264973 1.87406294]\n",
            "0.2781741946928648 [1.70307451 1.8739952 ]\n",
            "0.27815595221282774 [1.70349689 1.87392784]\n",
            "0.2781379151915767 [1.70391688 1.87386086]\n",
            "0.2781200813150993 [1.70433451 1.87379426]\n",
            "0.2781024482954455 [1.70474977 1.87372804]\n",
            "0.27808501387043416 [1.70516269 1.87366219]\n",
            "0.27806777580336217 [1.70557328 1.87359671]\n",
            "0.2780507318827169 [1.70598154 1.8735316 ]\n",
            "0.27803387992189516 [1.70638751 1.87346686]\n",
            "0.27801721775891813 [1.70679118 1.87340249]\n",
            "0.2780007432561585 [1.70719257 1.87333848]\n",
            "0.27798445430006324 [1.70759169 1.87327483]\n",
            "0.2779683488008849 [1.70798856 1.87321153]\n",
            "0.27795242469241044 [1.70838319 1.8731486 ]\n",
            "0.2779366799316986 [1.70877559 1.87308602]\n",
            "0.2779211124988182 [1.70916577 1.8730238 ]\n",
            "0.2779057203965853 [1.70955375 1.87296193]\n",
            "0.27789050165031226 [1.70993954 1.8729004 ]\n",
            "0.27787545430755023 [1.71032315 1.87283923]\n",
            "0.2778605764378396 [1.71070459 1.8727784 ]\n",
            "0.2778458661324648 [1.71108388 1.87271791]\n",
            "0.2778313215042059 [1.71146103 1.87265776]\n",
            "0.27781694068709745 [1.71183605 1.87259796]\n",
            "0.2778027218361915 [1.71220895 1.87253849]\n",
            "0.27778866312731865 [1.71257974 1.87247936]\n",
            "0.2777747627568542 [1.71294845 1.87242056]\n",
            "0.2777610189414865 [1.71331506 1.87236209]\n",
            "0.277747429917989 [1.71367961 1.87230396]\n",
            "0.2777339939429951 [1.7140421  1.87224615]\n",
            "0.27772070929277143 [1.71440254 1.87218867]\n",
            "0.2777075742629999 [1.71476095 1.87213151]\n",
            "0.27769458716855505 [1.71511733 1.87207468]\n",
            "0.2776817463432943 [1.7154717  1.87201816]\n",
            "0.2776690501398359 [1.71582407 1.87196197]\n",
            "0.27765649692935546 [1.71617445 1.87190609]\n",
            "0.27764408510137256 [1.71652285 1.87185053]\n",
            "0.2776318130635437 [1.71686928 1.87179528]\n",
            "0.277619679241461 [1.71721376 1.87174035]\n",
            "0.2776076820784471 [1.71755629 1.87168572]\n",
            "0.2775958200353596 [1.71789689 1.87163141]\n",
            "0.277584091590387 [1.71823556 1.8715774 ]\n",
            "0.2775724952388618 [1.71857232 1.87152369]\n",
            "0.2775610294930594 [1.71890718 1.87147029]\n",
            "0.27754969288201276 [1.71924015 1.87141719]\n",
            "0.2775384839513221 [1.71957123 1.87136439]\n",
            "0.2775274012629666 [1.71990045 1.87131189]\n",
            "0.27751644339512244 [1.72022781 1.87125968]\n",
            "0.2775056089419793 [1.72055332 1.87120777]\n",
            "0.27749489651355996 [1.72087699 1.87115616]\n",
            "0.2774843047355409 [1.72119884 1.87110483]\n",
            "0.2774738322490794 [1.72151886 1.87105379]\n",
            "0.2774634777106354 [1.72183708 1.87100305]\n",
            "0.2774532397917997 [1.7221535  1.87095258]\n",
            "0.27744311717912706 [1.72246814 1.87090241]\n",
            "0.27743310857396297 [1.722781   1.87085251]\n",
            "0.277423212692281 [1.72309209 1.8708029 ]\n",
            "0.2774134282645136 [1.72340142 1.87075357]\n",
            "0.27740375403539563 [1.72370901 1.87070452]\n",
            "0.27739418876379596 [1.72401486 1.87065574]\n",
            "0.2773847312225646 [1.72431898 1.87060724]\n",
            "0.27737538019837105 [1.72462139 1.87055902]\n",
            "0.27736613449154995 [1.72492209 1.87051106]\n",
            "0.2773569929159486 [1.72522109 1.87046338]\n",
            "0.27734795429877274 [1.7255184  1.87041597]\n",
            "0.27733901748043693 [1.72581403 1.87036882]\n",
            "0.27733018131441517 [1.72610799 1.87032194]\n",
            "0.2773214446670963 [1.7264003  1.87027533]\n",
            "0.2773128064176345 [1.72669095 1.87022898]\n",
            "0.2773042654578085 [1.72697996 1.87018289]\n",
            "0.27729582069187864 [1.72726734 1.87013706]\n",
            "0.2772874710364463 [1.72755309 1.87009149]\n",
            "0.27727921542031486 [1.72783723 1.87004617]\n",
            "0.27727105278435143 [1.72811977 1.87000111]\n",
            "0.27726298208135247 [1.72840071 1.86995631]\n",
            "0.2772550022759089 [1.72868007 1.86991176]\n",
            "0.277247112344272 [1.72895785 1.86986746]\n",
            "0.27723931127422524 [1.72923405 1.86982341]\n",
            "0.27723159806494985 [1.7295087  1.86977962]\n",
            "0.2772239717269008 [1.7297818  1.86973606]\n",
            "0.27721643128167694 [1.73005336 1.86969276]\n",
            "0.2772089757618972 [1.73032338 1.86964969]\n",
            "0.2772016042110758 [1.73059188 1.86960688]\n",
            "0.27719431568349845 [1.73085886 1.8695643 ]\n",
            "0.27718710924410317 [1.73112433 1.86952196]\n",
            "0.27717998396835886 [1.73138831 1.86947987]\n",
            "0.2771729389421477 [1.73165079 1.86943801]\n",
            "0.27716597326164594 [1.73191179 1.86939638]\n",
            "0.2771590860332112 [1.73217132 1.869355  ]\n",
            "0.27715227637326395 [1.73242938 1.86931384]\n",
            "0.27714554340817654 [1.73268598 1.86927292]\n",
            "0.27713888627416267 [1.73294114 1.86923223]\n",
            "0.2771323041171615 [1.73319485 1.86919177]\n",
            "0.2771257960927336 [1.73344713 1.86915154]\n",
            "0.27711936136594917 [1.73369799 1.86911153]\n",
            "0.2771129991112819 [1.73394743 1.86907175]\n",
            "0.2771067085125027 [1.73419546 1.8690322 ]\n",
            "0.27710048876257665 [1.73444209 1.86899286]\n",
            "0.27709433906355724 [1.73468733 1.86895376]\n",
            "0.27708825862648456 [1.73493118 1.86891487]\n",
            "0.27708224667128667 [1.73517366 1.8688762 ]\n",
            "0.277076302426674 [1.73541477 1.86883775]\n",
            "0.27707042513004676 [1.73565451 1.86879951]\n",
            "0.277064614027391 [1.7358929 1.8687615]\n",
            "0.2770588683731898 [1.73612995 1.86872369]\n",
            "0.27705318743031704 [1.73636565 1.86868611]\n",
            "0.27704757046995193 [1.73660003 1.86864873]\n",
            "0.27704201677148166 [1.73683308 1.86861156]\n",
            "0.27703652562240955 [1.73706481 1.86857461]\n",
            "0.2770310963182631 [1.73729524 1.86853786]\n",
            "0.2770257281625042 [1.73752437 1.86850132]\n",
            "0.27702042046643843 [1.7377522  1.86846499]\n",
            "0.277015172549131 [1.73797874 1.86842886]\n",
            "0.27700998373731317 [1.73820401 1.86839293]\n",
            "0.2770048533652997 [1.738428   1.86835721]\n",
            "0.27699978077490395 [1.73865073 1.86832169]\n",
            "0.27699476531535067 [1.7388722  1.86828637]\n",
            "0.27698980634319537 [1.73909242 1.86825125]\n",
            "0.2769849032222403 [1.7393114  1.86821633]\n",
            "0.27698005532345155 [1.73952913 1.86818161]\n",
            "0.27697526202488165 [1.73974564 1.86814708]\n",
            "0.27697052271158673 [1.73996093 1.86811275]\n",
            "0.27696583677554903 [1.740175   1.86807861]\n",
            "0.2769612036155992 [1.74038787 1.86804466]\n",
            "0.276956622637338 [1.74059953 1.86801091]\n",
            "0.27695209325306136 [1.74080999 1.86797735]\n",
            "0.27694761488168335 [1.74101927 1.86794397]\n",
            "0.27694318694866416 [1.74122736 1.86791079]\n",
            "0.2769388088859336 [1.74143428 1.86787779]\n",
            "0.27693448013181887 [1.74164003 1.86784497]\n",
            "0.2769302001309744 [1.74184462 1.86781235]\n",
            "0.27692596833430966 [1.74204806 1.86777991]\n",
            "0.27692178419891667 [1.74225034 1.86774765]\n",
            "0.2769176471880025 [1.74245148 1.86771557]\n",
            "0.27691355677082163 [1.74265149 1.86768367]\n",
            "0.2769095124226042 [1.74285037 1.86765196]\n",
            "0.2769055136244914 [1.74304812 1.86762042]\n",
            "0.27690155986346765 [1.74324476 1.86758906]\n",
            "0.2768976506322962 [1.74344029 1.86755788]\n",
            "0.2768937854294519 [1.74363471 1.86752687]\n",
            "0.2768899637590592 [1.74382803 1.86749604]\n",
            "0.27688618513082836 [1.74402027 1.86746539]\n",
            "0.2768824490599885 [1.74421141 1.8674349 ]\n",
            "0.27687875506723086 [1.74440148 1.86740459]\n",
            "0.27687510267864435 [1.74459047 1.86737445]\n",
            "0.27687149142565604 [1.7447784  1.86734448]\n",
            "0.2768679208449695 [1.74496527 1.86731468]\n",
            "0.27686439047850603 [1.74515108 1.86728505]\n",
            "0.27686089987334767 [1.74533584 1.86725559]\n",
            "0.27685744858167594 [1.74551956 1.86722629]\n",
            "0.27685403616071663 [1.74570224 1.86719716]\n",
            "0.2768506621726821 [1.74588389 1.86716819]\n",
            "0.2768473261847157 [1.74606451 1.86713938]\n",
            "0.2768440277688359 [1.74624411 1.86711074]\n",
            "0.2768407665018805 [1.7464227  1.86708226]\n",
            "0.2768375419654546 [1.74660028 1.86705394]\n",
            "0.27683435374587495 [1.74677686 1.86702578]\n",
            "0.2768312014341173 [1.74695244 1.86699778]\n",
            "0.2768280846257631 [1.74712703 1.86696994]\n",
            "0.276825002920951 [1.74730063 1.86694225]\n",
            "0.27682195592432174 [1.74747325 1.86691472]\n",
            "0.27681894324496825 [1.7476449  1.86688735]\n",
            "0.27681596449638796 [1.74781558 1.86686013]\n",
            "0.27681301929642893 [1.74798529 1.86683307]\n",
            "0.2768101072672455 [1.74815405 1.86680615]\n",
            "0.2768072280352462 [1.74832185 1.86677939]\n",
            "0.2768043812310478 [1.7484887  1.86675278]\n",
            "0.2768015664894269 [1.74865462 1.86672633]\n",
            "0.2767987834492743 [1.74881959 1.86670002]\n",
            "0.2767960317535466 [1.74898364 1.86667386]\n",
            "0.2767933110492222 [1.74914675 1.86664784]\n",
            "0.2767906209872564 [1.74930895 1.86662198]\n",
            "0.2767879612225336 [1.74947023 1.86659626]\n",
            "0.2767853314138279 [1.7496306  1.86657068]\n",
            "0.27678273122375396 [1.74979007 1.86654525]\n",
            "0.2767801603187273 [1.74994863 1.86651996]\n",
            "0.2767776183689208 [1.7501063  1.86649482]\n",
            "0.27677510504822134 [1.75026308 1.86646982]\n",
            "0.27677262003419034 [1.75041897 1.86644496]\n",
            "0.2767701630080182 [1.75057398 1.86642024]\n",
            "0.2767677336544876 [1.75072812 1.86639566]\n",
            "0.2767653316619311 [1.75088138 1.86637121]\n",
            "0.2767629567221919 [1.75103378 1.86634691]\n",
            "0.2767606085305826 [1.75118532 1.86632274]\n",
            "0.2767582867858486 [1.75133601 1.86629871]\n",
            "0.27675599119012817 [1.75148584 1.86627482]\n",
            "0.27675372144891425 [1.75163483 1.86625106]\n",
            "0.2767514772710152 [1.75178297 1.86622743]\n",
            "0.2767492583685208 [1.75193028 1.86620394]\n",
            "0.276747064456763 [1.75207676 1.86618058]\n",
            "0.27674489525428003 [1.75222241 1.86615735]\n",
            "0.27674275048277885 [1.75236724 1.86613426]\n",
            "0.27674062986710224 [1.75251125 1.86611129]\n",
            "0.2767385331351916 [1.75265444 1.86608845]\n",
            "0.27673646001805186 [1.75279683 1.86606575]\n",
            "0.27673441024971873 [1.75293841 1.86604317]\n",
            "0.2767323835672222 [1.7530792  1.86602072]\n",
            "0.27673037971055353 [1.75321919 1.86599839]\n",
            "0.27672839842263514 [1.75335839 1.86597619]\n",
            "0.27672643944928144 [1.7534968  1.86595412]\n",
            "0.2767245025391718 [1.75363443 1.86593217]\n",
            "0.27672258744381406 [1.75377128 1.86591035]\n",
            "0.2767206939175187 [1.75390736 1.86588865]\n",
            "0.2767188217173596 [1.75404268 1.86586707]\n",
            "0.2767169706031467 [1.75417722 1.86584561]\n",
            "0.2767151403373983 [1.75431101 1.86582427]\n",
            "0.2767133306853038 [1.75444404 1.86580306]\n",
            "0.27671154141469867 [1.75457633 1.86578196]\n",
            "0.2767097722960343 [1.75470786 1.86576099]\n",
            "0.276708023102345 [1.75483865 1.86574013]\n",
            "0.27670629360922366 [1.7549687  1.86571939]\n",
            "0.27670458359478917 [1.75509802 1.86569877]\n",
            "0.27670289283965954 [1.75522661 1.86567826]\n",
            "0.27670122112692447 [1.75535447 1.86565787]\n",
            "0.2766995682421152 [1.75548161 1.86563759]\n",
            "0.2766979339731802 [1.75560803 1.86561743]\n",
            "0.27669631811045436 [1.75573374 1.86559738]\n",
            "0.2766947204466355 [1.75585874 1.86557745]\n",
            "0.2766931407767554 [1.75598303 1.86555763]\n",
            "0.27669157889815427 [1.75610662 1.86553792]\n",
            "0.27669003461045516 [1.75622951 1.86551832]\n",
            "0.27668850771553793 [1.75635171 1.86549883]\n",
            "0.2766869980175136 [1.75647322 1.86547946]\n",
            "0.27668550532269964 [1.75659404 1.86546019]\n",
            "0.27668402943959397 [1.75671418 1.86544103]\n",
            "0.276682570178853 [1.75683364 1.86542198]\n",
            "0.27668112735326467 [1.75695243 1.86540303]\n",
            "0.27667970077772497 [1.75707055 1.8653842 ]\n",
            "0.27667829026921587 [1.757188   1.86536547]\n",
            "0.27667689564677905 [1.75730478 1.86534684]\n",
            "0.2766755167314959 [1.75742091 1.86532832]\n",
            "0.2766741533464614 [1.75753638 1.86530991]\n",
            "0.276672805316765 [1.7576512 1.8652916]\n",
            "0.27667147246946305 [1.75776536 1.86527339]\n",
            "0.2766701546335625 [1.75787889 1.86525529]\n",
            "0.27666885163999566 [1.75799177 1.86523729]\n",
            "0.2766675633215969 [1.75810402 1.86521938]\n",
            "0.27666628951308625 [1.75821563 1.86520159]\n",
            "0.2766650300510428 [1.75832661 1.86518389]\n",
            "0.2766637847738883 [1.75843697 1.86516629]\n",
            "0.27666255352186175 [1.7585467  1.86514879]\n",
            "0.2766613361370044 [1.75865581 1.86513139]\n",
            "0.2766601324631337 [1.75876431 1.86511409]\n",
            "0.27665894234582905 [1.75887219 1.86509688]\n",
            "0.27665776563240635 [1.75897947 1.86507977]\n",
            "0.2766566021719029 [1.75908614 1.86506276]\n",
            "0.276655451815055 [1.7591922  1.86504585]\n",
            "0.27665431441428134 [1.75929767 1.86502903]\n",
            "0.27665318982366205 [1.75940254 1.8650123 ]\n",
            "0.27665207789892043 [1.75950682 1.86499567]\n",
            "0.2766509784974049 [1.75961051 1.86497914]\n",
            "0.2766498914780712 [1.75971362 1.86496269]\n",
            "0.27664881670146285 [1.75981614 1.86494634]\n",
            "0.276647754029694 [1.75991808 1.86493009]\n",
            "0.2766467033264318 [1.76001945 1.86491392]\n",
            "0.2766456644568802 [1.76012025 1.86489785]\n",
            "0.2766446372877586 [1.76022047 1.86488186]\n",
            "0.27664362168729084 [1.76032013 1.86486597]\n",
            "0.27664261752518193 [1.76041923 1.86485017]\n",
            "0.27664162467260617 [1.76051777 1.86483445]\n",
            "0.2766406430021875 [1.76061575 1.86481883]\n",
            "0.27663967238798537 [1.76071318 1.86480329]\n",
            "0.27663871270547774 [1.76081006 1.86478784]\n",
            "0.27663776383154415 [1.76090639 1.86477248]\n",
            "0.2766368256444525 [1.76100217 1.8647572 ]\n",
            "0.27663589802383926 [1.76109742 1.86474201]\n",
            "0.2766349808506983 [1.76119213 1.86472691]\n",
            "0.27663407400736306 [1.7612863  1.86471189]\n",
            "0.2766331773774926 [1.76137994 1.86469696]\n",
            "0.27663229084605717 [1.76147305 1.86468211]\n",
            "0.2766314142993203 [1.76156564 1.86466734]\n",
            "0.2766305476248282 [1.7616577  1.86465266]\n",
            "0.2766296907113931 [1.76174925 1.86463806]\n",
            "0.27662884344907973 [1.76184027 1.86462355]\n",
            "0.27662800572919066 [1.76193079 1.86460911]\n",
            "0.2766271774442528 [1.76202079 1.86459476]\n",
            "0.2766263584880041 [1.76211028 1.86458049]\n",
            "0.2766255487553774 [1.76219927 1.8645663 ]\n",
            "0.2766247481424915 [1.76228776 1.86455218]\n",
            "0.27662395654663363 [1.76237574 1.86453815]\n",
            "0.2766231738662481 [1.76246323 1.8645242 ]\n",
            "0.2766224000009224 [1.76255022 1.86451033]\n",
            "0.27662163485137664 [1.76263673 1.86449653]\n",
            "0.2766208783194467 [1.76272274 1.86448282]\n",
            "0.27662013030807653 [1.76280827 1.86446918]\n",
            "0.2766193907213006 [1.76289332 1.86445561]\n",
            "0.27661865946423775 [1.76297788 1.86444213]\n",
            "0.27661793644307087 [1.76306197 1.86442872]\n",
            "0.2766172215650431 [1.76314559 1.86441538]\n",
            "0.2766165147384412 [1.76322873 1.86440212]\n",
            "0.2766158158725845 [1.7633114  1.86438894]\n",
            "0.27661512487781315 [1.7633936  1.86437583]\n",
            "0.2766144416654797 [1.76347535 1.86436279]\n",
            "0.2766137661479317 [1.76355662 1.86434983]\n",
            "0.27661309823850533 [1.76363744 1.86433694]\n",
            "0.27661243785151385 [1.76371781 1.86432413]\n",
            "0.2766117849022335 [1.76379772 1.86431138]\n",
            "0.2766111393068972 [1.76387718 1.86429871]\n",
            "0.2766105009826791 [1.76395619 1.86428611]\n",
            "0.27660986984768804 [1.76403475 1.86427358]\n",
            "0.27660924582095275 [1.76411287 1.86426112]\n",
            "0.276608628822416 [1.76419055 1.86424874]\n",
            "0.27660801877292224 [1.76426779 1.86423642]\n",
            "0.2766074155942063 [1.76434459 1.86422417]\n",
            "0.2766068192088855 [1.76442096 1.86421199]\n",
            "0.2766062295404469 [1.7644969  1.86419988]\n",
            "0.27660564651324165 [1.76457241 1.86418784]\n",
            "0.2766050700524715 [1.7646475  1.86417587]\n",
            "0.27660450008418114 [1.76472216 1.86416396]\n",
            "0.27660393653524745 [1.76479639 1.86415212]\n",
            "0.276603379333372 [1.76487021 1.86414035]\n",
            "0.27660282840706996 [1.76494361 1.86412864]\n",
            "0.2766022836856611 [1.7650166 1.864117 ]\n",
            "0.27660174509926294 [1.76508918 1.86410543]\n",
            "0.2766012125787782 [1.76516134 1.86409392]\n",
            "0.2766006860558888 [1.7652331  1.86408248]\n",
            "0.2766001654630451 [1.76530445 1.8640711 ]\n",
            "0.27659965073346043 [1.7653754  1.86405978]\n",
            "0.2765991418010981 [1.76544595 1.86404853]\n",
            "0.27659863860066636 [1.7655161  1.86403734]\n",
            "0.2765981410676081 [1.76558586 1.86402622]\n",
            "0.2765976491380934 [1.76565522 1.86401516]\n",
            "0.2765971627490119 [1.76572419 1.86400416]\n",
            "0.27659668183796354 [1.76579276 1.86399322]\n",
            "0.2765962063432513 [1.76586096 1.86398235]\n",
            "0.2765957362038726 [1.76592876 1.86397154]\n",
            "0.2765952713595119 [1.76599619 1.86396078]\n",
            "0.2765948117505338 [1.76606323 1.86395009]\n",
            "0.27659435731797394 [1.7661299  1.86393946]\n",
            "0.2765939080035317 [1.76619618 1.86392889]\n",
            "0.2765934637495637 [1.7662621  1.86391838]\n",
            "0.2765930244990751 [1.76632764 1.86390793]\n",
            "0.27659259019571464 [1.76639281 1.86389753]\n",
            "0.27659216078376303 [1.76645761 1.8638872 ]\n",
            "0.2765917362081314 [1.76652205 1.86387692]\n",
            "0.2765913164143482 [1.76658612 1.8638667 ]\n",
            "0.27659090134855885 [1.76664984 1.86385654]\n",
            "0.27659049095751287 [1.76671319 1.86384644]\n",
            "0.27659008518856 [1.76677618 1.86383639]\n",
            "0.2765896839896437 [1.76683882 1.8638264 ]\n",
            "0.27658928730929333 [1.76690111 1.86381647]\n",
            "0.27658889509661655 [1.76696304 1.8638066 ]\n",
            "0.2765885073012967 [1.76702462 1.86379677]\n",
            "0.2765881238735824 [1.76708586 1.86378701]\n",
            "0.27658774476428294 [1.76714675 1.8637773 ]\n",
            "0.2765873699247612 [1.76720729 1.86376764]\n",
            "0.27658699930692815 [1.7672675  1.86375804]\n",
            "0.2765866328632358 [1.76732736 1.8637485 ]\n",
            "0.276586270546674 [1.76738689 1.863739  ]\n",
            "0.27658591231075846 [1.76744607 1.86372956]\n",
            "0.27658555810953095 [1.76750493 1.86372018]\n",
            "0.2765852078975512 [1.76756345 1.86371084]\n",
            "0.27658486162988755 [1.76762165 1.86370156]\n",
            "0.2765845192621176 [1.76767951 1.86369234]\n",
            "0.2765841807503188 [1.76773705 1.86368316]\n",
            "0.2765838460510615 [1.76779426 1.86367404]\n",
            "0.2765835151214077 [1.76785115 1.86366496]\n",
            "0.27658318791890013 [1.76790771 1.86365594]\n",
            "0.27658286440156227 [1.76796396 1.86364697]\n",
            "0.2765825445278895 [1.76801989 1.86363805]\n",
            "0.2765822282568432 [1.76807551 1.86362918]\n",
            "0.27658191554784956 [1.76813081 1.86362037]\n",
            "0.27658160636078966 [1.7681858 1.8636116]\n",
            "0.27658130065599734 [1.76824048 1.86360288]\n",
            "0.27658099839425326 [1.76829484 1.86359421]\n",
            "0.2765806995367795 [1.76834891 1.86358558]\n",
            "0.27658040404523443 [1.76840266 1.86357701]\n",
            "0.27658011188170945 [1.76845612 1.86356849]\n",
            "0.2765798230087221 [1.76850927 1.86356001]\n",
            "0.2765795373892114 [1.76856212 1.86355158]\n",
            "0.2765792549865353 [1.76861467 1.8635432 ]\n",
            "0.2765789757644635 [1.76866693 1.86353487]\n",
            "0.27657869968717386 [1.76871889 1.86352658]\n",
            "0.27657842671924804 [1.76877056 1.86351834]\n",
            "0.27657815682566594 [1.76882193 1.86351015]\n",
            "0.27657788997180344 [1.76887302 1.863502  ]\n",
            "0.27657762612342407 [1.76892381 1.8634939 ]\n",
            "0.27657736524667703 [1.76897432 1.86348585]\n",
            "0.27657710730809626 [1.76902455 1.86347784]\n",
            "0.2765768522745893 [1.76907449 1.86346987]\n",
            "0.2765766001134366 [1.76912415 1.86346195]\n",
            "0.276576350792288 [1.76917353 1.86345408]\n",
            "0.2765761042791589 [1.76922263 1.86344625]\n",
            "0.27657586054242167 [1.76927145 1.86343846]\n",
            "0.27657561955080767 [1.76932    1.86343072]\n",
            "0.27657538127339937 [1.76936827 1.86342302]\n",
            "0.2765751456796276 [1.76941627 1.86341537]\n",
            "0.27657491273926815 [1.769464   1.86340776]\n",
            "0.27657468242243544 [1.76951146 1.86340019]\n",
            "0.2765744546995825 [1.76955865 1.86339266]\n",
            "0.2765742295414935 [1.76960558 1.86338518]\n",
            "0.27657400691928297 [1.76965224 1.86337774]\n",
            "0.2765737868043895 [1.76969863 1.86337034]\n",
            "0.2765735691685745 [1.76974477 1.86336298]\n",
            "0.2765733539839171 [1.76979064 1.86335566]\n",
            "0.2765731412228104 [1.76983626 1.86334839]\n",
            "0.2765729308579591 [1.76988161 1.86334116]\n",
            "0.2765727228623749 [1.76992671 1.86333396]\n",
            "0.2765725172093735 [1.76997156 1.86332681]\n",
            "0.27657231387257103 [1.77001615 1.8633197 ]\n",
            "0.27657211282588123 [1.77006049 1.86331263]\n",
            "0.27657191404351145 [1.77010459 1.8633056 ]\n",
            "0.2765717174999589 [1.77014843 1.86329861]\n",
            "0.27657152317000894 [1.77019202 1.86329165]\n",
            "0.27657133102873044 [1.77023537 1.86328474]\n",
            "0.27657114105147296 [1.77027847 1.86327787]\n",
            "0.27657095321386443 [1.77032133 1.86327103]\n",
            "0.2765707674918064 [1.77036395 1.86326424]\n",
            "0.27657058386147126 [1.77040633 1.86325748]\n",
            "0.2765704022993023 [1.77044847 1.86325076]\n",
            "0.27657022278200527 [1.77049037 1.86324408]\n",
            "0.27657004528655044 [1.77053203 1.86323743]\n",
            "0.2765698697901656 [1.77057346 1.86323082]\n",
            "0.27656969627033606 [1.77061465 1.86322425]\n",
            "0.27656952470480073 [1.77065561 1.86321772]\n",
            "0.27656935507154967 [1.77069634 1.86321123]\n",
            "0.2765691873488192 [1.77073684 1.86320477]\n",
            "0.27656902151509233 [1.77077712 1.86319835]\n",
            "0.27656885754909355 [1.77081716 1.86319196]\n",
            "0.2765686954297877 [1.77085698 1.86318561]\n",
            "0.27656853513637575 [1.77089657 1.8631793 ]\n",
            "0.27656837664829326 [1.77093594 1.86317302]\n",
            "0.276568219945208 [1.77097509 1.86316677]\n",
            "0.2765680650070152 [1.77101401 1.86316057]\n",
            "0.27656791181383894 [1.77105272 1.86315439]\n",
            "0.27656776034602426 [1.77109121 1.86314826]\n",
            "0.27656761058413915 [1.77112948 1.86314215]\n",
            "0.2765674625089716 [1.77116753 1.86313608]\n",
            "0.27656731610152346 [1.77120537 1.86313005]\n",
            "0.27656717134301145 [1.771243   1.86312405]\n",
            "0.2765670282148657 [1.77128041 1.86311808]\n",
            "0.27656688669872276 [1.77131761 1.86311215]\n",
            "0.2765667467764271 [1.7713546  1.86310625]\n",
            "0.27656660843002856 [1.77139138 1.86310039]\n",
            "0.2765664716417781 [1.77142796 1.86309455]\n",
            "0.27656633639412653 [1.77146433 1.86308875]\n",
            "0.27656620266972326 [1.77150049 1.86308299]\n",
            "0.27656607045141124 [1.77153645 1.86307725]\n",
            "0.276565939722229 [1.77157221 1.86307155]\n",
            "0.2765658104654046 [1.77160776 1.86306588]\n",
            "0.27656568266435605 [1.77164311 1.86306024]\n",
            "0.2765655563026857 [1.77167827 1.86305464]\n",
            "0.2765654313641836 [1.77171322 1.86304906]\n",
            "0.2765653078328217 [1.77174798 1.86304352]\n",
            "0.27656518569275046 [1.77178254 1.86303801]\n",
            "0.276565064928301 [1.77181691 1.86303253]\n",
            "0.2765649455239798 [1.77185108 1.86302708]\n",
            "0.2765648274644692 [1.77188506 1.86302166]\n",
            "0.27656471073462174 [1.77191884 1.86301627]\n",
            "0.2765645953194625 [1.77195244 1.86301091]\n",
            "0.27656448120418486 [1.77198585 1.86300558]\n",
            "0.27656436837414766 [1.77201907 1.86300029]\n",
            "0.2765642568148766 [1.7720521  1.86299502]\n",
            "0.27656414651206007 [1.77208494 1.86298978]\n",
            "0.2765640374515454 [1.7721176  1.86298457]\n",
            "0.2765639296193422 [1.77215007 1.86297939]\n",
            "0.27656382300161625 [1.77218236 1.86297424]\n",
            "0.2765637175846886 [1.77221447 1.86296912]\n",
            "0.2765636133550361 [1.7722464  1.86296403]\n",
            "0.2765635102992864 [1.77227814 1.86295897]\n",
            "0.2765634084042177 [1.77230971 1.86295394]\n",
            "0.27656330765675863 [1.7723411  1.86294893]\n",
            "0.2765632080439835 [1.77237231 1.86294395]\n",
            "0.2765631095531126 [1.77240335 1.862939  ]\n",
            "0.2765630121715109 [1.77243421 1.86293408]\n",
            "0.27656291588668436 [1.77246489 1.86292919]\n",
            "0.2765628206862808 [1.77249541 1.86292432]\n",
            "0.27656272655808667 [1.77252575 1.86291948]\n",
            "0.2765626334900264 [1.77255592 1.86291467]\n",
            "0.2765625414701596 [1.77258591 1.86290989]\n",
            "0.27656245048668066 [1.77261574 1.86290513]\n",
            "0.27656236052791755 [1.7726454 1.8629004]\n",
            "0.2765622715823288 [1.7726749 1.8628957]\n",
            "0.2765621836385042 [1.77270422 1.86289102]\n",
            "0.27656209668516035 [1.77273339 1.86288637]\n",
            "0.27656201071114145 [1.77276238 1.86288175]\n",
            "0.2765619257054198 [1.77279121 1.86287715]\n",
            "0.2765618416570876 [1.77281988 1.86287258]\n",
            "0.27656175855536275 [1.77284839 1.86286803]\n",
            "0.2765616763895849 [1.77287674 1.86286351]\n",
            "0.2765615951492111 [1.77290493 1.86285901]\n",
            "0.27656151482382 [1.77293295 1.86285454]\n",
            "0.27656143540310646 [1.77296082 1.8628501 ]\n",
            "0.27656135687688055 [1.77298854 1.86284568]\n",
            "0.27656127923506874 [1.77301609 1.86284129]\n",
            "0.27656120246771015 [1.77304349 1.86283692]\n",
            "0.27656112656495613 [1.77307074 1.86283257]\n",
            "0.27656105151706833 [1.77309783 1.86282825]\n",
            "0.27656097731441925 [1.77312477 1.86282396]\n",
            "0.2765609039474896 [1.77315155 1.86281968]\n",
            "0.2765608314068658 [1.77317819 1.86281544]\n",
            "0.2765607596832428 [1.77320467 1.86281121]\n",
            "0.27656068876741813 [1.77323101 1.86280701]\n",
            "0.27656061865029347 [1.77325719 1.86280284]\n",
            "0.2765605493228743 [1.77328323 1.86279868]\n",
            "0.27656048077626577 [1.77330912 1.86279456]\n",
            "0.2765604130016739 [1.77333487 1.86279045]\n",
            "0.2765603459904042 [1.77336047 1.86278637]\n",
            "0.2765602797338605 [1.77338592 1.86278231]\n",
            "0.27656021422354066 [1.77341123 1.86277827]\n",
            "0.27656014945104074 [1.7734364  1.86277426]\n",
            "0.2765600854080513 [1.77346143 1.86277027]\n",
            "0.2765600220863561 [1.77348631 1.8627663 ]\n",
            "0.2765599594778317 [1.77351106 1.86276235]\n",
            "0.27655989757444555 [1.77353566 1.86275843]\n",
            "0.2765598363682559 [1.77356013 1.86275453]\n",
            "0.2765597758514106 [1.77358445 1.86275065]\n",
            "0.2765597160161452 [1.77360865 1.86274679]\n",
            "0.27655965685478484 [1.7736327  1.86274295]\n",
            "0.2765595983597378 [1.77365662 1.86273914]\n",
            "0.2765595405235005 [1.7736804  1.86273535]\n",
            "0.2765594833386526 [1.77370405 1.86273157]\n",
            "0.2765594267978584 [1.77372756 1.86272782]\n",
            "0.27655937089386384 [1.77375094 1.8627241 ]\n",
            "0.27655931561949654 [1.77377419 1.86272039]\n",
            "0.27655926096766537 [1.77379731 1.8627167 ]\n",
            "0.2765592069313591 [1.7738203  1.86271304]\n",
            "0.2765591535036454 [1.77384316 1.86270939]\n",
            "0.27655910067766926 [1.77386589 1.86270576]\n",
            "0.27655904844665435 [1.77388849 1.86270216]\n",
            "0.27655899680389895 [1.77391096 1.86269858]\n",
            "0.2765589457427784 [1.77393331 1.86269501]\n",
            "0.27655889525674193 [1.77395553 1.86269147]\n",
            "0.2765588453393122 [1.77397762 1.86268795]\n",
            "0.2765587959840854 [1.77399959 1.86268444]\n",
            "0.2765587471847297 [1.77402144 1.86268096]\n",
            "0.2765586989349841 [1.77404316 1.86267749]\n",
            "0.27655865122865847 [1.77406476 1.86267405]\n",
            "0.2765586040596335 [1.77408624 1.86267062]\n",
            "0.27655855742185687 [1.7741076  1.86266722]\n",
            "0.27655851130934517 [1.77412883 1.86266383]\n",
            "0.27655846571618303 [1.77414995 1.86266046]\n",
            "0.27655842063652136 [1.77417094 1.86265712]\n",
            "0.2765583760645761 [1.77419182 1.86265379]\n",
            "0.27655833199462954 [1.77421258 1.86265048]\n",
            "0.27655828842102725 [1.77423323 1.86264718]\n",
            "0.2765582453381795 [1.77425375 1.86264391]\n",
            "0.276558202740559 [1.77427416 1.86264066]\n",
            "0.276558160622701 [1.77429446 1.86263742]\n",
            "0.2765581189792027 [1.77431464 1.8626342 ]\n",
            "0.2765580778047199 [1.7743347 1.862631 ]\n",
            "0.2765580370939711 [1.77435466 1.86262782]\n",
            "0.2765579968417344 [1.7743745  1.86262465]\n",
            "0.2765579570428438 [1.77439423 1.86262151]\n",
            "0.27655791769219507 [1.77441384 1.86261838]\n",
            "0.2765578787847386 [1.77443335 1.86261527]\n",
            "0.27655784031548397 [1.77445275 1.86261218]\n",
            "0.2765578022794953 [1.77447203 1.8626091 ]\n",
            "0.2765577646718929 [1.77449121 1.86260604]\n",
            "0.27655772748785157 [1.77451028 1.862603  ]\n",
            "0.2765576907226016 [1.77452924 1.86259998]\n",
            "0.2765576543714266 [1.7745481  1.86259697]\n",
            "0.2765576184296616 [1.77456685 1.86259398]\n",
            "0.27655758289269605 [1.77458549 1.86259101]\n",
            "0.2765575477559724 [1.77460403 1.86258805]\n",
            "0.2765575130149801 [1.77462246 1.86258511]\n",
            "0.27655747866526464 [1.77464079 1.86258219]\n",
            "0.27655744470241844 [1.77465901 1.86257928]\n",
            "0.2765574111220835 [1.77467713 1.86257639]\n",
            "0.27655737791995294 [1.77469515 1.86257352]\n",
            "0.27655734509176594 [1.77471307 1.86257066]\n",
            "0.2765573126333115 [1.77473089 1.86256782]\n",
            "0.27655728054042567 [1.7747486  1.86256499]\n",
            "0.2765572488089912 [1.77476622 1.86256219]\n",
            "0.2765572174349368 [1.77478373 1.86255939]\n",
            "0.27655718641423777 [1.77480115 1.86255661]\n",
            "0.2765571557429137 [1.77481847 1.86255385]\n",
            "0.27655712541703126 [1.77483569 1.86255111]\n",
            "0.2765570954326981 [1.77485282 1.86254837]\n",
            "0.2765570657860678 [1.77486984 1.86254566]\n",
            "0.27655703647333796 [1.77488678 1.86254296]\n",
            "0.2765570074907469 [1.77490361 1.86254027]\n",
            "0.27655697883457664 [1.77492035 1.8625376 ]\n",
            "0.2765569505011519 [1.774937   1.86253495]\n",
            "0.2765569224868362 [1.77495355 1.86253231]\n",
            "0.27655689478803586 [1.77497001 1.86252969]\n",
            "0.2765568674011982 [1.77498637 1.86252708]\n",
            "0.2765568403228088 [1.77500265 1.86252448]\n",
            "0.276556813549394 [1.77501883 1.8625219 ]\n",
            "0.2765567870775186 [1.77503492 1.86251933]\n",
            "0.2765567609037866 [1.77505092 1.86251678]\n",
            "0.2765567350248416 [1.77506683 1.86251425]\n",
            "0.2765567094373616 [1.77508264 1.86251172]\n",
            "0.2765566841380646 [1.77509837 1.86250921]\n",
            "0.2765566591237051 [1.77511401 1.86250672]\n",
            "0.27655663439107325 [1.77512957 1.86250424]\n",
            "0.27655660993699704 [1.77514503 1.86250177]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bzeVjc2Vp7y"
      },
      "source": [
        "## Review\n",
        "* We store our data in arrays where **each row is an input example** $x^{(i)}$ and **each column is a feature**. Training example $x^{(i)}$ corresponds to training label $y^{(i)}$.\n",
        "* **Gradient descent** is an **optimization process** that **minimizes loss** $J(W)$ where $W$ is a set of parameters (or weights). The loss measures the difference between  predictions $\\hat{Y}$ using the current values of $W$ and the target labels $Y$, and gradient descent updates $W$ by taking a **step in the direction of the loss gradient**.\n",
        "* Each pass over the training data by the gradient descent algorithm is called an **epoch**. The algorithm has no specific stopping point, but we often choose to stop when the parameter values have **converged**, that is, the change in values in the next step are less than some small $\\epsilon$."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}