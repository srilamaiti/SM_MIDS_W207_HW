{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0mavHZlbpFpR",
        "m2j3vtPothqH",
        "jStvd4wGp8Ju",
        "i2353h6hNEnz",
        "GSEWn5BEN1Sz"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srilamaiti/SM_MIDS_W207_HW/blob/main/tumor_detection_xception_mobilenet_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***1. Setup***"
      ],
      "metadata": {
        "id": "NUylKvIS6Lsf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***A. Installing New Libraries***"
      ],
      "metadata": {
        "id": "5dlGr3VX6DyA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAIyFDyH5uLR",
        "outputId": "9405a5ec-302a-4184-b13a-b012bd034fe9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: imgaug in /usr/local/lib/python3.8/dist-packages (0.4.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.8/dist-packages (from imgaug) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from imgaug) (7.1.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.8/dist-packages (from imgaug) (2.9.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from imgaug) (1.7.3)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.8/dist-packages (from imgaug) (0.18.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from imgaug) (3.2.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.8/dist-packages (from imgaug) (1.8.5.post1)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.8/dist-packages (from imgaug) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from imgaug) (1.15.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->imgaug) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->imgaug) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image>=0.14.2->imgaug) (2.6.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->imgaug) (1.4.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install imgaug"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install livelossplot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "91Xb-Plzo2E5",
        "outputId": "dbb62e3e-89d5-4def-b823-b73de2ea0ca2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting livelossplot\n",
            "  Downloading livelossplot-0.5.5-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from livelossplot) (3.2.2)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.8/dist-packages (from livelossplot) (2.3.3)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.8/dist-packages (from bokeh->livelossplot) (2.11.3)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.8/dist-packages (from bokeh->livelossplot) (7.1.2)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.8/dist-packages (from bokeh->livelossplot) (1.21.6)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.8/dist-packages (from bokeh->livelossplot) (6.0.4)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.8/dist-packages (from bokeh->livelossplot) (6.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.8/dist-packages (from bokeh->livelossplot) (21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from bokeh->livelossplot) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.8/dist-packages (from bokeh->livelossplot) (4.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=2.9->bokeh->livelossplot) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=16.8->bokeh->livelossplot) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->bokeh->livelossplot) (1.15.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->livelossplot) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->livelossplot) (1.4.4)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.5.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***B. Importing Libraries***"
      ],
      "metadata": {
        "id": "UQBd_dFh6S1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***a. Importing General Purpose Libraries***"
      ],
      "metadata": {
        "id": "Iv-y8vdS62gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import joblib\n",
        "import glob\n",
        "import random\n",
        "from itertools import product\n",
        "import gc\n",
        "import subprocess\n",
        "import shutil\n",
        "import copy\n",
        "import statistics as st\n",
        "from scipy import stats\n",
        "from pathlib import Path\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ],
      "metadata": {
        "id": "ujHcENda69HI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***b. Importing Image Processing and Visualization Libraries***"
      ],
      "metadata": {
        "id": "xM_Fw_tt7EpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imutils import rotate as rotate\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import Polygon\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "from mlxtend.plotting import plot_confusion_matrix\n",
        "from skimage.color import gray2rgb\n",
        "import skimage.io as skio\n",
        "from imgaug import augmenters as img_aug\n",
        "import imgaug as iaug\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 10)\n",
        "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']"
      ],
      "metadata": {
        "id": "0NV7G1UoIi4I"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***c. Importing Sklearn Functionalities***"
      ],
      "metadata": {
        "id": "qSyqwXIMIp7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.decomposition import IncrementalPCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "import sklearn"
      ],
      "metadata": {
        "id": "fUh7ts6QI8Lx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***d. Importing Tensorflow Libraries***"
      ],
      "metadata": {
        "id": "2C55GxBKnW3V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from livelossplot import PlotLossesKeras\n",
        "from keras.utils.layer_utils import count_params\n",
        "\n",
        "from tensorflow.keras.layers import RandomFlip\n",
        "from tensorflow.keras.layers import RandomZoom\n",
        "from tensorflow.keras.layers import RandomRotation\n",
        "from tensorflow.keras.layers import Conv1D\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Concatenate\n",
        "from tensorflow.keras.layers import MaxPooling1D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import AveragePooling2D\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling1D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import Lambda\n",
        "from tensorflow.keras.layers import Multiply\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Bidirectional\n",
        "from tensorflow.keras.layers import PReLU\n",
        "from keras.layers.core import Activation\n",
        "from keras.layers.convolutional import SeparableConv1D\n",
        "from keras.layers.convolutional import SeparableConv2D\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.densenet import *\n",
        "from keras.applications.resnet import ResNet152\n",
        "from keras.applications.nasnet import NASNetMobile\n",
        "from keras.applications.nasnet import NASNetLarge\n",
        "from keras.applications.efficientnet import EfficientNetB7\n",
        "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
        "from keras.applications.nasnet import preprocess_input\n",
        "from keras.applications import MobileNetV2\n",
        "from keras.applications.xception import Xception\n",
        "from keras.applications.inception_v3 import *\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import array_to_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import save_img\n",
        "\n",
        "from tensorflow.python.ops.numpy_ops import np_config\n",
        "\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "\n",
        "from tensorflow.keras.losses import mae\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.losses import binary_crossentropy\n",
        "\n",
        "from keras.models import Model\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.models import load_model\n"
      ],
      "metadata": {
        "id": "eUXe1TURnkN5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***C. Mounting Google Drive***"
      ],
      "metadata": {
        "id": "JLAsViNZnt1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required to read the data from Kaggle\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content/gdrive/MyDrive/Kaggle\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCewn8D_n4oL",
        "outputId": "9b31f4f4-e3a8-4be2-d097-b3deccf7fa0f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***D. Downloading Data from Kaggle***"
      ],
      "metadata": {
        "id": "0mavHZlbpFpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash /content/gdrive/MyDrive/Kaggle/download_kaggle_data.ksh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxaSmgjsoxfh",
        "outputId": "df44a249-f75a-49a7-ed14-da2675f04357"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading histopathologic-cancer-detection.zip to /content\n",
            "100% 6.30G/6.31G [01:21<00:00, 72.9MB/s]\n",
            "100% 6.31G/6.31G [01:21<00:00, 82.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!kaggle competitions download -c histopathologic-cancer-detection\n",
        "#!unzip -o -qq \\*.zip  && rm *.zip"
      ],
      "metadata": {
        "id": "fDXH4brO_sGj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***E. Defining Vartiables***"
      ],
      "metadata": {
        "id": "aZ9PZBKVtI__"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "#sample_size = 80000\n",
        "sample_size = 1000\n",
        "#batch_size = 256\n",
        "batch_size = 128\n",
        "#epochs = 10\n",
        "epochs = 3\n",
        "\n",
        "image_size = 96\n",
        "#number_of_splits = 8\n",
        "number_of_splits = 3\n",
        "run_mode = ['interim_test', 'final_test']\n",
        "\n",
        "# Transfer learning model list\n",
        "transfer_learning_model_list = ['VGG16', \n",
        "                                'VGG19', \n",
        "                                'DenseNet201', \n",
        "                                'InceptionV3', \n",
        "                                'ResNet50', \n",
        "                                'EfficientNetB7', \n",
        "                                'MobileNet', \n",
        "                                'Xception'\n",
        "                               ]\n",
        "learning_rate_list = [.01, .001, .0001, .00001]\n",
        "optimizer_list = ['sgd', 'adam']\n",
        "dropout_list = [.2, .4, .6]\n",
        "kernel_size_list = [(3,3), (4,4), (5,5)]\n",
        "dense_layer_node_list = [512, 256, 128]\n",
        "fully_conneted_layer_list = [1, 2, 3]\n",
        "epoch_list = [5, 10, 15, 20]\n",
        "\n",
        "train_path = os.getcwd() + \"/train/\"\n",
        "test_path = os.getcwd() + \"/test/\"\n",
        "\n",
        "original_input_file_list = train_path + '*.tif'\n",
        "original_output_file_list = test_path + '*.tif'\n",
        "\n",
        "current_working_dir = os.getcwd()\n",
        "\n",
        "train_label_file = 'train_labels.csv'\n",
        "test_label_file = 'sample_submission.csv'\n",
        "\n",
        "image_file_extension = '.tif'\n",
        "\n",
        "train_files_path = os.path.join(current_working_dir, train_path)\n",
        "test_files_path = os.path.join(current_working_dir, test_path)\n",
        "\n",
        "image_processing_train_positive_path = '/content/image_processing/train/positive'\n",
        "image_processing_train_negative_path = '/content/image_processing/train/negative'\n",
        "\n",
        "image_processing_validation_positive_path = '/content/image_processing/validation/positive'\n",
        "image_processing_validation_negative_path = '/content/image_processing/validation/negative'\n",
        "\n",
        "image_processing_test_positive_path = '/content/image_processing/test/positive'\n",
        "image_processing_test_negative_path = '/content/image_processing/test/negative'\n",
        "\n",
        "image_processing_train_path = \"/content/image_processing/train/\"\n",
        "image_processing_validation_path = \"/content/image_processing/validation/\"\n",
        "image_processing_test_path = \"/content/image_processing/test/\"\n",
        "\n",
        "random.seed(1)\n",
        "random_state = 1234\n",
        "\n",
        "dropout_rate = .5\n",
        "\n",
        "consolidated_df_model_kpi = pd.DataFrame()\n",
        "\n",
        "df_model_kfold_epoch_pred_pct = pd.DataFrame()\n",
        "df_model_kfold_epoch_pred_bin = pd.DataFrame()\n",
        "\n",
        "saved_model_names_list = []\n",
        "\n",
        "grayscale_image_augmentation_list = ['adjust_random_brightness',\n",
        "                                     'adjust_random_contrast',\n",
        "                                     'random_flip_left_right',\n",
        "                                     'random_flip_up_down',\n",
        "                                     'rotate_image_by_angle',\n",
        "                                     'rotate_image_by_90_or_180_or_270_deg',\n",
        "                                     'random_zoom',\n",
        "                                     'resize_with_crop_or_pad'\n",
        "                                    ]\n",
        "'''"
      ],
      "metadata": {
        "id": "e4rSklTftQY-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "53b6b7b7-33a5-475a-ccce-ca4ed5370e3c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n#sample_size = 80000\\nsample_size = 1000\\n#batch_size = 256\\nbatch_size = 128\\n#epochs = 10\\nepochs = 3\\n\\nimage_size = 96\\n#number_of_splits = 8\\nnumber_of_splits = 3\\nrun_mode = [\\'interim_test\\', \\'final_test\\']\\n\\n# Transfer learning model list\\ntransfer_learning_model_list = [\\'VGG16\\', \\n                                \\'VGG19\\', \\n                                \\'DenseNet201\\', \\n                                \\'InceptionV3\\', \\n                                \\'ResNet50\\', \\n                                \\'EfficientNetB7\\', \\n                                \\'MobileNet\\', \\n                                \\'Xception\\'\\n                               ]\\nlearning_rate_list = [.01, .001, .0001, .00001]\\noptimizer_list = [\\'sgd\\', \\'adam\\']\\ndropout_list = [.2, .4, .6]\\nkernel_size_list = [(3,3), (4,4), (5,5)]\\ndense_layer_node_list = [512, 256, 128]\\nfully_conneted_layer_list = [1, 2, 3]\\nepoch_list = [5, 10, 15, 20]\\n\\ntrain_path = os.getcwd() + \"/train/\"\\ntest_path = os.getcwd() + \"/test/\"\\n\\noriginal_input_file_list = train_path + \\'*.tif\\'\\noriginal_output_file_list = test_path + \\'*.tif\\'\\n\\ncurrent_working_dir = os.getcwd()\\n\\ntrain_label_file = \\'train_labels.csv\\'\\ntest_label_file = \\'sample_submission.csv\\'\\n\\nimage_file_extension = \\'.tif\\'\\n\\ntrain_files_path = os.path.join(current_working_dir, train_path)\\ntest_files_path = os.path.join(current_working_dir, test_path)\\n\\nimage_processing_train_positive_path = \\'/content/image_processing/train/positive\\'\\nimage_processing_train_negative_path = \\'/content/image_processing/train/negative\\'\\n\\nimage_processing_validation_positive_path = \\'/content/image_processing/validation/positive\\'\\nimage_processing_validation_negative_path = \\'/content/image_processing/validation/negative\\'\\n\\nimage_processing_test_positive_path = \\'/content/image_processing/test/positive\\'\\nimage_processing_test_negative_path = \\'/content/image_processing/test/negative\\'\\n\\nimage_processing_train_path = \"/content/image_processing/train/\"\\nimage_processing_validation_path = \"/content/image_processing/validation/\"\\nimage_processing_test_path = \"/content/image_processing/test/\"\\n\\nrandom.seed(1)\\nrandom_state = 1234\\n\\ndropout_rate = .5\\n\\nconsolidated_df_model_kpi = pd.DataFrame()\\n\\ndf_model_kfold_epoch_pred_pct = pd.DataFrame()\\ndf_model_kfold_epoch_pred_bin = pd.DataFrame()\\n\\nsaved_model_names_list = []\\n\\ngrayscale_image_augmentation_list = [\\'adjust_random_brightness\\',\\n                                     \\'adjust_random_contrast\\',\\n                                     \\'random_flip_left_right\\',\\n                                     \\'random_flip_up_down\\',\\n                                     \\'rotate_image_by_angle\\',\\n                                     \\'rotate_image_by_90_or_180_or_270_deg\\',\\n                                     \\'random_zoom\\',\\n                                     \\'resize_with_crop_or_pad\\'\\n                                    ]\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class config_defition:\n",
        "    \"\"\"\n",
        "    This class is a static class holding all the configuration parameters.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Constructor method to define all the variables.\n",
        "        \"\"\"\n",
        "        self.sample_size = 80000\n",
        "        #self.sample_size = 1000\n",
        "        self.batch_size = 256\n",
        "        self.epochs = 10\n",
        "        #self.epochs = 3\n",
        "        self.image_size = 96\n",
        "        self.number_of_splits = 5\n",
        "        #self.number_of_splits = 3\n",
        "        self.dropout_rate = .5\n",
        "        self.run_mode = ['interim_test', 'final_test']\n",
        "\n",
        "        # Transfer learning model list\n",
        "        self.transfer_learning_model_list = ['VGG19', \n",
        "                                             'DenseNet201', \n",
        "                                             'InceptionV3', \n",
        "                                             'ResNet152', \n",
        "                                             'EfficientNetB7', \n",
        "                                             'MobileNet', \n",
        "                                             'Xception',\n",
        "                                             'InceptionResNetV2'\n",
        "                                            ]\n",
        "        self.learning_rate_list = [.01, .001, .0001, .00001]\n",
        "        self.optimizer_list = ['sgd', 'adam']\n",
        "        self.dropout_list = [.2, .4, .6]\n",
        "        \n",
        "        self.train_path = os.getcwd() + \"/train/\"\n",
        "        self.test_path = os.getcwd() + \"/test/\"\n",
        "        self.original_input_file_list = self.train_path + '*.tif'\n",
        "        self.original_output_file_list = self.test_path + '*.tif'\n",
        "        self.current_working_dir = os.getcwd()\n",
        "        self.train_label_file = 'train_labels.csv'\n",
        "        self.test_label_file = 'sample_submission.csv'\n",
        "        self.image_file_extension = '.tif'\n",
        "        self.root_directory = '/content/image_processing'\n",
        "        self.train_files_path = os.path.join(self.current_working_dir, \n",
        "                                             self.train_path\n",
        "                                            )\n",
        "        self.test_files_path = os.path.join(self.current_working_dir, \n",
        "                                            self.test_path\n",
        "                                           )\n",
        "        \n",
        "        self.image_processing_train_positive_path = '/content/image_processing/train/positive'\n",
        "        self.image_processing_train_negative_path = '/content/image_processing/train/negative'\n",
        "\n",
        "        self.image_processing_validation_positive_path = '/content/image_processing/validation/positive'\n",
        "        self.image_processing_validation_negative_path = '/content/image_processing/validation/negative'\n",
        "\n",
        "        self.image_processing_test_positive_path = '/content/image_processing/test/positive'\n",
        "        self.image_processing_test_negative_path = '/content/image_processing/test/negative'\n",
        "\n",
        "        self.image_processing_train_path = \"/content/image_processing/train/\"\n",
        "        self.image_processing_validation_path = \"/content/image_processing/validation/\"\n",
        "        self.image_processing_test_path = \"/content/image_processing/test/\"\n",
        "        \n",
        "        random.seed(1)\n",
        "        self.random_state = 1234\n",
        "\n",
        "        self.consolidated_history_df = pd.DataFrame()\n",
        "        self.consolidated_test_kpi_df = pd.DataFrame()\n",
        "        self.consolidated_pred_df = pd.DataFrame()\n",
        "        self.test_kpi_df = pd.DataFrame()\n",
        "        self.temp_pred_df = pd.DataFrame()\n",
        "        \n",
        "        self.consolidated_df_model_kpi = pd.DataFrame()\n",
        "        self.saved_model_names_list = []\n",
        "        self.grayscale_image_augmentation_list = ['adjust_random_brightness',\n",
        "                                                  'adjust_random_contrast',\n",
        "                                                  'random_flip_left_right',\n",
        "                                                  'random_flip_up_down',\n",
        "                                                  'rotate_image_by_angle',\n",
        "                                                  'rotate_image_by_90_or_180_or_270_deg',\n",
        "                                                  'random_zoom',\n",
        "                                                  'resize_with_crop_or_pad'\n",
        "                                                 ]\n",
        "        \n",
        "cfg_proc = config_defition()"
      ],
      "metadata": {
        "id": "Pwn6f7t4Tlfu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***F. Misclenious Processing Class***"
      ],
      "metadata": {
        "id": "m2j3vtPothqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class misc_processing:\n",
        "    \"\"\"\n",
        "    This class contains methods for misclenious processings.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        None\n",
        "  \n",
        "    def create_dir_structure(self, root_directory):\n",
        "        \"\"\"\n",
        "        This method creates a directory tree in the form below:-\n",
        "        image_processing--| train         |---positive\n",
        "                          |               |---negative\n",
        "                          |  \n",
        "                          | validation    |---positive\n",
        "                          |               |---negative\n",
        "                          |\n",
        "                          | test          |---positive\n",
        "                          |               |---negative\n",
        "\n",
        "        \"\"\"\n",
        "        dirpath = Path(root_directory)\n",
        "        if dirpath.exists() and dirpath.is_dir():\n",
        "            shutil.rmtree(f'{root_directory}')\n",
        "        os.makedirs(f'{root_directory}', \n",
        "                    exist_ok = True\n",
        "                   )\n",
        "        for sub_folder in ['train', 'validation', 'test']:\n",
        "            for grp in ['positive', 'negative']:\n",
        "                os.makedirs(f'{root_directory}/{sub_folder}/{grp}', \n",
        "                            exist_ok = True\n",
        "                           )\n",
        "\n",
        "    def remove_files_from_dir(self, path):\n",
        "        \"\"\"\n",
        "        This method deletes all files under a given path.\n",
        "        \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(f\"Starting to remove files under {path}...\")\n",
        "        shutil.rmtree(path)\n",
        "        os.mkdir(path)\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print()\n",
        "    \n",
        "    def generate_fully_qualified_file_name_list(self, file_list):\n",
        "        \"\"\"\n",
        "        This method generates a list of fully qualified file names.\n",
        "        \"\"\"\n",
        "        qualified_file_name_list = [os.path.join(cfg_proc.current_working_dir, cfg_proc.train_path) + \n",
        "                                    img + \n",
        "                                    cfg_proc.image_file_extension \n",
        "                                    for img in file_list\n",
        "                                   ]\n",
        "        return qualified_file_name_list\n",
        "\n",
        "    def print_image_original(self, image_file_list, label_list):\n",
        "        \"\"\"\n",
        "        This method prints original images.\n",
        "        \"\"\"\n",
        "        nrows, ncols = 1,4 #print first 4 images\n",
        "        f, axs = plt.subplots(nrows, ncols, figsize=(14,12))\n",
        "        for i, image in enumerate(image_file_list):\n",
        "            axs[i].imshow(array_to_img(image))\n",
        "            pf = Polygon(((32, 32), (64, 32), (64, 64), (32, 64)),\n",
        "                         fc=(0.0, 0.0, 0.0, 0.0), \n",
        "                         ec=(0.0, 0.9, 0.0 ,0.9), lw=4, linestyle='--')\n",
        "            pf.set_edgecolor('r')\n",
        "            axs[i].add_patch(pf)\n",
        "            axs[i].set(title=label_list[i])\n",
        "\n",
        "    def print_image_in_diff_orientation(self, image_file):\n",
        "        \"\"\"\n",
        "        This method prints images.\n",
        "        \"\"\"\n",
        "        tf.random.set_seed(1234)\n",
        "        fig = plt.figure(figsize=(14, 12))\n",
        "        #fig = plt.figure()\n",
        "        image = skio.imread(image_file, plugin = \"tifffile\")\n",
        "        \n",
        "        # plot original\n",
        "        ax = fig.add_subplot(1, 5, 1)\n",
        "        ax.imshow(array_to_img(image))\n",
        "        pf = Polygon(((32, 32), (64, 32), (64, 64), (32, 64)),\n",
        "                fc = (0.0, 0.0, 0.0, 0.0), \n",
        "                ec = (0.0, 0.9, 0.0 ,0.9), lw=4, linestyle='--')\n",
        "        pf.set_edgecolor('r')\n",
        "        ax.add_patch(pf)\n",
        "        ax.set_title('Original', size=15);\n",
        "        \n",
        "        # resize\n",
        "        ax = fig.add_subplot(1, 5, 2)\n",
        "        img_resize = tf.image.resize(image, size=(224, 224))\n",
        "        ax.imshow(array_to_img(img_resize))\n",
        "        pf = Polygon(((80, 80), (144, 80), (144, 144), (80, 144)),\n",
        "                fc = (0.0, 0.0, 0.0, 0.0), \n",
        "                ec = (0.0, 0.9, 0.0 ,0.9), lw=4, linestyle='--')\n",
        "        pf.set_edgecolor('r')\n",
        "        ax.add_patch(pf)\n",
        "        ax.set_title('Step 1: Resize', size=15);\n",
        "        \n",
        "        # adjust brightness\n",
        "        ax = fig.add_subplot(1, 5, 3)\n",
        "        img_bright = tf.image.adjust_brightness(img_resize, 0.3)\n",
        "        ax.imshow(array_to_img(img_bright))\n",
        "        pf = Polygon(((80, 80), (144, 80), (144, 144), (80, 144)),\n",
        "                fc = (0.0, 0.0, 0.0, 0.0), \n",
        "                ec = (0.0, 0.9, 0.0 ,0.9), lw=4, linestyle='--')\n",
        "        pf.set_edgecolor('r')\n",
        "        ax.add_patch(pf)\n",
        "        ax.set_title('Step 2: Brightness', size=15);\n",
        "        \n",
        "        # adjust contrast\n",
        "        ax = fig.add_subplot(1, 5, 4)\n",
        "        img_contrast = tf.image.adjust_contrast(img_bright, contrast_factor=3)\n",
        "        ax.imshow(array_to_img(img_contrast))\n",
        "        pf = Polygon(((80, 80), (144, 80), (144, 144), (80, 144)),\n",
        "                fc = (0.0, 0.0, 0.0, 0.0), \n",
        "                ec = (0.0, 0.9, 0.0 ,0.9), lw=4, linestyle='--')\n",
        "        pf.set_edgecolor('r')\n",
        "        ax.add_patch(pf)\n",
        "        ax.set_title('Step 3: Contrast', size=15);\n",
        "        \n",
        "        # flip left right\n",
        "        ax = fig.add_subplot(1, 5, 5)\n",
        "        img_flip = tf.image.flip_left_right(img_contrast)\n",
        "        ax.imshow(array_to_img(img_flip))\n",
        "        pf = Polygon(((80, 80), (144, 80), (144, 144), (80, 144)),\n",
        "                fc = (0.0, 0.0, 0.0, 0.0), \n",
        "                ec = (0.0, 0.9, 0.0 ,0.9), lw=4, linestyle='--')\n",
        "        pf.set_edgecolor('r')\n",
        "        ax.add_patch(pf)\n",
        "        ax.set_title('Step 4: Flip left right');\n",
        "\n",
        "    def get_id_and_label_list(self, file_path, file_extension):\n",
        "        \"\"\"\n",
        "        This function gets the imgae id and corresponding label.\n",
        "        \"\"\"\n",
        "        file_list = []\n",
        "        for file_name in glob.glob(file_path + '*' + file_extension):\n",
        "            file_list.append(file_name)\n",
        "        return file_list\n",
        "\n",
        "    def compute_mean_and_std(self, image_file_list, r_mid_pos = 48, c_mid_pos = 48):\n",
        "        \"\"\"\n",
        "        This method computes mean and std at the center of the image.\n",
        "        \"\"\"\n",
        "        center_pixel_value_list = []\n",
        "        for image_file in image_file_list:\n",
        "            image = skio.imread(image_file, plugin = \"tifffile\")\n",
        "            center_pixel_value_list.append(image[r_mid_pos, c_mid_pos])\n",
        "        np_array_center_pixel_value = np.array(center_pixel_value_list)\n",
        "        return np.mean(np_array_center_pixel_value), np.std(np_array_center_pixel_value)\n",
        "\n",
        "    def copy_file_from_one_to_other(self, file_names, dest_path):\n",
        "        \"This method moves chunks of files in one to other.\"\n",
        "        os.system('cp -r ' + file_names + ' ' + dest_path)\n",
        "\n",
        "    def process_copy_files(self, file_name_list, dest_path):\n",
        "        \"\"\"\"\n",
        "        This method processes moving files from one dir to the other. \n",
        "        This is the master process to run actual moving in chunks.\n",
        "        \"\"\"\n",
        "        '''\n",
        "        process_chunk_size = 100\n",
        "        for idx in range(0, len(file_name_list), process_chunk_size):\n",
        "            if idx % 10000 == 0:\n",
        "                print(\"Processing index: \", idx)\n",
        "            self.copy_file_from_one_to_other(' '.join(file_name_list[idx : idx + process_chunk_size]), dest_path)\n",
        "        '''\n",
        "        for file in file_name_list:\n",
        "            shutil.copy(file, dest_path)\n",
        "    \n",
        "    def check_file_count_in_a_directory(self, dir_path):\n",
        "        \"\"\"\n",
        "        This method checks the file count in a directory\n",
        "        \"\"\"\n",
        "        cmd_string = 'ls ' + dir_path + \" | wc -l\"\n",
        "        file_count = int(subprocess.check_output(cmd_string, shell=True, text=True).strip())\n",
        "        return file_count\n",
        "\n",
        "    def get_mini_batch_data(self, image_list, mini_batch_size):\n",
        "        \"\"\"\n",
        "        This method performs as a generator to spit out data in small batches.\n",
        "        \"\"\"\n",
        "        return (image_list[idx : idx + mini_batch_size] for idx in range(0, len(image_list), mini_batch_size))\n",
        "\n",
        "    def get_aug_step_list(self):\n",
        "        \"\"\"\n",
        "        This method executes image augmentation pipeline.\n",
        "        \"\"\"\n",
        "        sometimes = lambda aug: img_aug.Sometimes(0.5, aug)\n",
        "        img_aug_seq = img_aug.Sequential(\n",
        "        [\n",
        "            # apply the following augmenters to most images\n",
        "            img_aug.Fliplr(0.5), # horizontally flip 50% of all images\n",
        "            img_aug.Flipud(0.2), # vertically flip 20% of all images\n",
        "            sometimes(img_aug.Affine(\n",
        "                scale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)}, # scale images to 80-120% of their size, individually per axis\n",
        "                translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)}, # translate by -20 to +20 percent (per axis)\n",
        "                rotate=(-10, 10), # rotate by -45 to +45 degrees\n",
        "                shear=(-5, 5), # shear by -16 to +16 degrees\n",
        "                order=[0, 1], # use nearest neighbour or bilinear interpolation (fast)\n",
        "                cval=(0, 255), # if mode is constant, use a cval between 0 and 255\n",
        "                mode=iaug.ALL # use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
        "            )),\n",
        "            # execute 0 to 5 of the following (less important) augmenters per image\n",
        "            # don't execute all of them, as that would often be way too strong\n",
        "            img_aug.SomeOf((0, 5),\n",
        "                [\n",
        "                    sometimes(img_aug.Superpixels(p_replace=(0, 1.0), n_segments=(20, 200))), # convert images into their superpixel representation\n",
        "                    img_aug.OneOf([\n",
        "                        img_aug.GaussianBlur((0, 1.0)), # blur images with a sigma between 0 and 3.0\n",
        "                        img_aug.AverageBlur(k=(3, 5)), # blur image using local means with kernel sizes between 2 and 7\n",
        "                        img_aug.MedianBlur(k=(3, 5)), # blur image using local medians with kernel sizes between 2 and 7\n",
        "                    ]),\n",
        "                    img_aug.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)), # sharpen images\n",
        "                    img_aug.Emboss(alpha=(0, 1.0), strength=(0, 2.0)), # emboss images\n",
        "                    # search either for all edges or for directed edges,\n",
        "                    # blend the result with the original image using a blobby mask\n",
        "                    img_aug.SimplexNoiseAlpha(img_aug.OneOf([\n",
        "                        img_aug.EdgeDetect(alpha=(0.5, 1.0)),\n",
        "                        img_aug.DirectedEdgeDetect(alpha=(0.5, 1.0), direction=(0.0, 1.0)),\n",
        "                    ])),\n",
        "                    img_aug.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.01*255), per_channel=0.5), # add gaussian noise to images\n",
        "                    img_aug.OneOf([\n",
        "                        img_aug.Dropout((0.01, 0.05), per_channel=0.5), # randomly remove up to 10% of the pixels\n",
        "                        img_aug.CoarseDropout((0.01, 0.03), size_percent=(0.01, 0.02), per_channel=0.2),\n",
        "                    ]),\n",
        "                    img_aug.Invert(0.01, per_channel=True), # invert color channels\n",
        "                    img_aug.Add((-2, 2), per_channel=0.5), # change brightness of images (by -10 to 10 of original value)\n",
        "                    img_aug.AddToHueAndSaturation((-1, 1)), # change hue and saturation\n",
        "                    # either change the brightness of the whole image (sometimes\n",
        "                    # per channel) or change the brightness of subareas\n",
        "                    img_aug.OneOf([\n",
        "                        img_aug.Multiply((0.9, 1.1), per_channel=0.5),\n",
        "                        img_aug.FrequencyNoiseAlpha(\n",
        "                            exponent=(-1, 0),\n",
        "                            first=img_aug.Multiply((0.9, 1.1), per_channel=True),\n",
        "                            second=img_aug.ContrastNormalization((0.9, 1.1))\n",
        "                        )\n",
        "                    ]),\n",
        "                    sometimes(img_aug.ElasticTransformation(alpha=(0.5, 3.5), sigma=0.25)), # move pixels locally around (with random strengths)\n",
        "                    sometimes(img_aug.PiecewiseAffine(scale=(0.01, 0.05))), # sometimes move parts of the image around\n",
        "                    sometimes(img_aug.PerspectiveTransform(scale=(0.01, 0.1)))\n",
        "                ],\n",
        "                random_order=True\n",
        "            )\n",
        "        ],\n",
        "        random_order=True\n",
        "        )\n",
        "        return img_aug_seq\n",
        "\n",
        "    def get_id_label_map(self, df, filter_list):\n",
        "        \"\"\"\n",
        "        This method generates the id and label dictionary.\n",
        "        \"\"\"\n",
        "        return {k : v for k, v in zip(df[df.id.isin(filter_list)].id.values, \n",
        "                                      df[df.id.isin(filter_list)].label.values\n",
        "                                     )\n",
        "               }\n",
        "\n",
        "    def image_data_generator(self, list_files, label_list, batch_size, augment=False):\n",
        "        \"\"\"\n",
        "        This method is a generrator function to produce mini batch of data.\n",
        "        \"\"\"\n",
        "        image_augmentation_steps = self.get_aug_step_list()\n",
        "        while True:\n",
        "            shuffle(list_files)\n",
        "            for mini_batch in self.get_mini_batch_data(list_files, batch_size):\n",
        "                X = [cv2.imread(x) for x in mini_batch]\n",
        "                y = label_list\n",
        "                if augment:\n",
        "                    aug_X = image_augmentation_steps.augment_images(X)\n",
        "                    aug_y = y\n",
        "                    X = X + aug_X\n",
        "                X = [preprocess_input(x) for x in X]\n",
        "                \n",
        "        yield np.array(X), np.array(y)\n",
        "\n",
        "misc_proc = misc_processing()\n",
        "misc_proc.create_dir_structure(cfg_proc.root_directory)"
      ],
      "metadata": {
        "id": "KMaqq9V4toXo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***G. Visualization Processing Class***"
      ],
      "metadata": {
        "id": "jStvd4wGp8Ju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class data_viz_processing:\n",
        "    \"\"\"\n",
        "    This class contains methods to display various plots.\n",
        "    \"\"\"\n",
        "    def count_plot(self, data, label_col, title_val):\n",
        "        \"\"\"\n",
        "        This method plots count plot of the input data set.\n",
        "        \"\"\"\n",
        "        sns.countplot(data = data, x = label_col)\n",
        "        plt.title(title_val)\n",
        "        plt.show(block = False)\n",
        "\n",
        "    def pie_chart_plot(self, data, label_col, title_val):\n",
        "        \"\"\"\n",
        "        This method plots pie chart based on the given data.\n",
        "        \"\"\"\n",
        "        fig = px.pie(data, \n",
        "                     values = data[label_col].value_counts().values, \n",
        "                     names = data[label_col].unique())\n",
        "        fig.update_layout(\n",
        "                      title={\n",
        "                             'text'    : title_val,\n",
        "                             'y'       : .99,\n",
        "                             'x'       :  0.5,\n",
        "                             'xanchor' : 'center',\n",
        "                             'yanchor' : 'top'\n",
        "                            }\n",
        "                          )\n",
        "        fig.show()\n",
        "        plt.show(block = False)\n",
        "\n",
        "data_viz = data_viz_processing()"
      ],
      "metadata": {
        "id": "myB5dDG2qBTr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***H. Image Processing Class***"
      ],
      "metadata": {
        "id": "i2353h6hNEnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class image_processing:\n",
        "    \"\"\"\n",
        "    This class contains methods for image processing.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        None\n",
        "    \n",
        "    def read_image_file_in_np_array(self, image_list):\n",
        "        \"\"\"\n",
        "        This method reads each image file in a Numpy array and returns it.\n",
        "        \"\"\"\n",
        "        return np.asarray([skio.imread(image_file, plugin = \"tifffile\") for image_file in image_list])\n",
        "    \n",
        "    def convert_np_array_to_tensor(self, np_image_array):\n",
        "        \"\"\"\n",
        "        This method converts the numpy array representation of each image in tensor.\n",
        "        \"\"\"\n",
        "        return tf.convert_to_tensor(np_image_array, dtype = tf.float32)\n",
        "\n",
        "    def convert_int_tf_to_float(self, tf_image_list):\n",
        "        \"\"\"\n",
        "        This method converts integer TF value to float.\n",
        "        \"\"\"\n",
        "        return np.asanyarray([tf.cast(img, tf.float32) for img in tf_image_list])\n",
        "    \n",
        "    def convert_from_rgb_to_grayscale(self, tf_image_list, large_list_ind = False):\n",
        "        \"\"\"\n",
        "        This method converts color image to grayscale.\n",
        "        \"\"\"\n",
        "        if large_list_ind == False:\n",
        "            return tf.image.rgb_to_grayscale(tf_image_list) / 255.0\n",
        "        else:\n",
        "            None\n",
        "    \n",
        "    def combine_train_val(self, x_train, X_val, y_train, y_val):\n",
        "        \"\"\"\n",
        "        This method combines train and validation data, shuffles them and \n",
        "        returns back suffled data for k-fold cross validation.\n",
        "        \"\"\"\n",
        "        X_train_kfold = tf.concat([X_train, X_val] , axis = 0)\n",
        "        y_train_kfold = tf.concat([y_train, y_val] , axis = 0)\n",
        "\n",
        "        print(\"Shuffling the kfold train data...\")\n",
        "        tf.random.set_seed(1234) # for reproducibility\n",
        "    \n",
        "        test_shuffle_indices = tf.random.shuffle(tf.range(tf.shape(X_train_kfold)[0], dtype = tf.int32))\n",
        "        X_train_kfold = tf.gather(X_train_kfold, test_shuffle_indices)\n",
        "        y_train_kfold = tf.gather(y_train_kfold, test_shuffle_indices).numpy()\n",
        "        \n",
        "        print(f\"X_train_kfold shape: {X_train_kfold.shape}\")\n",
        "        print(f\"y_train_kfold shape: {y_train_kfold.shape}\")\n",
        "\n",
        "    def adjust_brightness(self, tf_image_list, delta):\n",
        "        \"\"\"\n",
        "        This method adjusts the image brightness.\n",
        "        \"\"\"\n",
        "        return tf.image.adjust_brightness(tf_image_list, delta = delta)\n",
        "\n",
        "    def adjust_random_brightness(self, tf_image_list, max_delta = .3, seed = (1,2)):\n",
        "        \"\"\"\n",
        "        This method adjusts random image brightness.\n",
        "        \"\"\"\n",
        "        return tf.image.stateless_random_brightness(tf_image_list, max_delta = max_delta, seed = seed)\n",
        "\n",
        "    def adjust_contrast(self, tf_image_list, contrast_factor):\n",
        "        \"\"\"\n",
        "        This method adjusts contrast of the image.\n",
        "        \"\"\"\n",
        "        return tf.image.adjust_contrast(tf_image_list, contrast_factor = contrast_factor)\n",
        "\n",
        "    def adjust_random_contrast(self, contrast_factor, lower = .2, upper = .5, seed = (1,2)):\n",
        "        \"\"\"\n",
        "        This method randomly contrasts images during training.\n",
        "        \"\"\"\n",
        "        return tf.image.stateless_random_contrast(contrast_factor, lower, upper, seed)\n",
        "\n",
        "    def flip_left_right(self, tf_image_list):\n",
        "        \"\"\"\n",
        "        This method applies flips the image from left to right.\n",
        "        \"\"\"\n",
        "        return tf.image.flip_left_right(tf_image_list)\n",
        "\n",
        "    def random_flip_left_right(self, tf_image_list, seed = (1,2)):\n",
        "        \"\"\"\n",
        "        This method randomly flips images left-right during training.\n",
        "        \"\"\"\n",
        "        return tf.image.stateless_random_flip_left_right(tf_image_list, seed)\n",
        "\n",
        "    def flip_up_down(self, tf_image_list):\n",
        "        \"\"\"\n",
        "        This method flips the image up-down.\n",
        "        \"\"\"\n",
        "        return tf.image.flip_up_down(tf_image_list)\n",
        "    \n",
        "    def random_flip_up_down(self, tf_image_list, seed = (1,2)):\n",
        "        \"\"\"\n",
        "        This method flips the image up-down.\n",
        "        \"\"\"\n",
        "        return tf.image.stateless_random_flip_up_down(tf_image_list, seed)\n",
        "\n",
        "    def rotate_image_by_90_or_180_or_270_deg(self, tf_image_list, k = 1):\n",
        "        \"\"\"\n",
        "        This method rotates images by 90/180/270 degrees.\n",
        "        k = 1 : 90 degree rotation\n",
        "        k = 2 : 180 degree rotation\n",
        "        k = 3 : 270 degree rotation\n",
        "        \"\"\"\n",
        "        return tf.image.rot90(tf_image_list, k)\n",
        "\n",
        "    def rotate_image_by_angle(self, tf_image_list, angle = tf.constant(np.pi/8)):\n",
        "        \"\"\"\n",
        "        This method rotates images by a given angle.\n",
        "        \"\"\"\n",
        "        rotate_layer = tf.keras.layers.RandomRotation(0.2)\n",
        "        rotated_image = rotate_layer(tf_image_list) \n",
        "        return rotated_image    \n",
        "    \n",
        "    def random_zoom(self, tf_image_list):\n",
        "        \"\"\"\n",
        "        This method zooms the image.\n",
        "        \"\"\"\n",
        "        zoom_layer = tf.keras.layers.RandomZoom(.5, .2)\n",
        "        zoomed_image = zoom_layer(tf_image_list) \n",
        "        return zoomed_image\n",
        "\n",
        "    def random_crop(self, tf_image_list, crop_height = 16, crop_width = 16):\n",
        "        \"\"\"\n",
        "        This method randomly crops the image.\n",
        "        \"\"\"\n",
        "        crop_layer = tf.keras.layers.RandomCrop(crop_height, crop_width)\n",
        "        cropped_image = crop_layer(tf_image_list) \n",
        "        return cropped_image\n",
        "\n",
        "    def resize_with_crop_or_pad(self, tf_image_list, crop_height = 32, crop_width = 32):\n",
        "        \"\"\"\n",
        "        This method crops and resizes the central part of the image.\n",
        "        \"\"\"\n",
        "        cropped_image = tf.image.resize_with_crop_or_pad(tf_image_list, crop_height, crop_width)\n",
        "        resized_image = tf.image.resize(cropped_image, [96, 96])\n",
        "        return resized_image\n",
        "\n",
        "    def image_augmentation_pipeline(self, train_image_list, test_image_list, validation_image_list):\n",
        "        \"\"\"\n",
        "        This method executes image augmentation tasks.\n",
        "        \"\"\"\n",
        "        for img_aug_func in grayscale_image_augmentation_list:\n",
        "            \n",
        "            print(\"Image augmentation function : \", img_aug_func)\n",
        "            \n",
        "            if img_aug_func == 'adjust_random_brightness':\n",
        "      \n",
        "                print(\"Handling random brightness adjustment for train_image_list\")\n",
        "                train_image_aug_list = self.adjust_random_brightness(tf_image_list = train_image_list, \n",
        "                                                                     max_delta = np.round(random.uniform(.1, .5),1)\n",
        "                                                                    )\n",
        "                gc.collect()\n",
        "                tf.keras.backend.clear_session()\n",
        "                print(f\"Shape of train_image_aug_list : {train_image_aug_list.shape}\")\n",
        "\n",
        "            elif img_aug_func == 'adjust_random_contrast':\n",
        "      \n",
        "                print(\"Handling contrast adjustment for X_train_positive_aug_tf\")\n",
        "                train_image_aug_list = self.adjust_random_contrast(tf_image_list = train_image_aug_list, \n",
        "                                                                   lower = np.round(random.uniform(.1, .3),1), \n",
        "                                                                   upper = np.round(random.uniform(.4, .6),1)\n",
        "                                                                  )\n",
        "                gc.collect()\n",
        "                tf.keras.backend.clear_session()\n",
        "                print(f\"Shape of train_image_aug_list : {train_image_aug_list.shape}\")\n",
        "\n",
        "            elif img_aug_func == 'random_flip_left_right':\n",
        "                \n",
        "                print(\"Handling random flip left and right for train_image_list\")\n",
        "                train_image_aug_list = self.random_flip_left_right(tf_image_list = train_image_aug_list)\n",
        "                gc.collect()\n",
        "                print(f\"Shape of X_train_positive_aug_tf : {train_image_aug_list.shape}\")\n",
        "\n",
        "            elif img_aug_func == 'random_flip_up_down':\n",
        "\n",
        "                print(\"Handling random flip up and down for train_image_list\")\n",
        "                train_image_aug_list = self.random_flip_up_down(tf_image_list = train_image_aug_list)\n",
        "                gc.collect()\n",
        "                tf.keras.backend.clear_session()\n",
        "                print(f\"Shape of X_train_positive_aug_tf : {train_image_aug_list.shape}\")\n",
        "\n",
        "            elif img_aug_func == 'rotate_image_by_angle':\n",
        "                \n",
        "                print(\"Handling image rotation by an angle for train_image_list\")\n",
        "                train_image_aug_list = self.rotate_image_by_angle(tf_image_list = train_image_aug_list)\n",
        "                gc.collect()\n",
        "                tf.keras.backend.clear_session()\n",
        "                print(f\"Shape of X_train_positive_aug_tf : {train_image_aug_list.shape}\")\n",
        "            \n",
        "            elif img_aug_func == 'rotate_image_by_90_or_180_or_270_deg':\n",
        "\n",
        "                print(\"Handling image rotation by 90 deg angle for train_image_list\")\n",
        "                train_image_aug_list = self.rotate_image_by_90_or_180_or_270_deg(tf_image_list = train_image_aug_list, \n",
        "                                                                                 k = random.randrange(1, 3)\n",
        "                                                                                )\n",
        "                gc.collect()\n",
        "                print(f\"Shape of X_train_positive_aug_tf : {train_image_aug_list.shape}\")\n",
        "\n",
        "            elif img_aug_func == 'random_zoom':\n",
        "\n",
        "                print(\"Handling random zoom for X_train_positive_aug_tf\")\n",
        "                train_image_aug_list = self.random_zoom(tf_image_list = train_image_aug_list)\n",
        "                gc.collect()\n",
        "                tf.keras.backend.clear_session()\n",
        "                print(f\"Shape of X_train_positive_aug_tf : {train_image_aug_list.shape}\")\n",
        "        \n",
        "            elif img_aug_func == 'resize_with_crop_or_pad':\n",
        "\n",
        "                print(\"Handling resize with crop or pad for train_image_list\")\n",
        "                train_image_aug_list = self.resize_with_crop_or_pad(tf_image_list = train_image_aug_list)\n",
        "                gc.collect()\n",
        "                print(f\"Shape of X_train_positive_aug_tf : {train_image_aug_list.shape}\")\n",
        "        \n",
        "        return train_image_aug_list\n",
        "\n",
        "img_proc = image_processing()"
      ],
      "metadata": {
        "id": "o4VmIP_cNQCS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***I. Misclenious Model Building, Metric Reporting and Plotting Class***"
      ],
      "metadata": {
        "id": "GSEWn5BEN1Sz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class misc_model_functionality_processing:\n",
        "    \"\"\"\n",
        "    This class contains misclenious methods, required for model KPI or model plotting.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        tf.random.set_seed(cfg_proc.random_state)\n",
        "        np.random.seed(cfg_proc.random_state)\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "    def model_summary_and_display_structure(self, model):\n",
        "        \"\"\"\n",
        "        This method shows model summary and displays the model structure.\n",
        "        \"\"\"\n",
        "        model.summary()\n",
        "        tf.keras.utils.plot_model(model)\n",
        "        plt.show(block = False)\n",
        "\n",
        "    def model_save(self, model, model_name):\n",
        "        \"\"\"\n",
        "        This method saves the model in a h5 file.\n",
        "        \"\"\"\n",
        "        tf.keras.backend.clear_session()\n",
        "        model.save(model_name + '.h5')\n",
        "    \n",
        "    def model_evaluation(self, model, X_test, y_test):\n",
        "        \"\"\"\n",
        "        This method evaluates the test data for a given model.\n",
        "        \"\"\"\n",
        "        self.test_results = model.evaluate(X_test, y_test)\n",
        "        print('\\nTest Loss : {:.2f}%'.format(self.test_results[0] * 100))\n",
        "        print('\\nTest Accuracy :  {:.2f}%'.format(self.test_results[1] * 100))\n",
        "\n",
        "    def model_prediction(self, model, X_test):\n",
        "        \"\"\"\n",
        "        This method predicts for a given model.\n",
        "        \"\"\"\n",
        "        # transform logits to probabilities\n",
        "        self.pred_logits = model.predict(X_test)\n",
        "        self.probas = tf.sigmoid(self.pred_logits)\n",
        "        self.probas = self.probas.numpy().flatten() * 100\n",
        "\n",
        "    def plot_model_accuracy_and_loss(self, history, model_name):\n",
        "        \"\"\"\n",
        "        This method plots model training and validation accuracies.\n",
        "        \"\"\"\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "        hist = history.history\n",
        "        x_arr = np.arange(len(hist['loss'])) + 1\n",
        "        \n",
        "        fig = plt.figure(figsize=(12, 4))\n",
        "        ax = fig.add_subplot(1, 2, 1)\n",
        "        ax.plot(x_arr, hist['loss'], '-o', label = 'Train loss')\n",
        "        ax.plot(x_arr, hist['val_loss'], '--<', label = 'Validation loss')\n",
        "        ax.legend(fontsize=15)\n",
        "        ax.set_xlabel('Epoch', size = 15)\n",
        "        ax.set_ylabel('Loss', size = 15)\n",
        "\n",
        "        ax = fig.add_subplot(1, 2, 2)\n",
        "        ax.plot(x_arr, hist['accuracy'], '-o', label = 'Train acc.')\n",
        "        ax.plot(x_arr, hist['val_accuracy'], '--<', label = 'Validation acc.')\n",
        "        ax.legend(fontsize = 15)\n",
        "        ax.set_xlabel('Epoch', size = 15)\n",
        "        ax.set_ylabel('Accuracy', size = 15)\n",
        "        ax.set_ylim(0,1)\n",
        "        plt.title(f\"Training and validation loss and accuracies for model : {model_name}\")\n",
        "        plt.show(block = False)\n",
        "\n",
        "    def build_pretained_model(self, model_name):\n",
        "        \"\"\"\n",
        "        This function utilizes transfer learning of a given model.\n",
        "        \"\"\"\n",
        "        tf.random.set_seed(random_state)\n",
        "        np.random.seed(random_state)\n",
        "        tf.keras.backend.clear_session()\n",
        "        input_shape = (image_size, image_size, 3)\n",
        "        if model_name == 'VGG16':\n",
        "            pretrained_model = VGG16(weights = 'imagenet', include_top = False, input_shape = input_shape)\n",
        "        elif model_name == 'VGG19':\n",
        "            pretrained_model = VGG19(weights = 'imagenet', include_top = False, input_shape = input_shape)\n",
        "        elif model_name == 'DenseNet201':\n",
        "            pretrained_model = tf.keras.applications.densenet.DenseNet201(weights = 'imagenet', include_top = False, input_shape = input_shape)\n",
        "        elif model_name == 'ResNet152':\n",
        "            pretrained_model = tf.keras.applications.resnet50.ResNet152(weights = 'imagenet', include_top = False, input_shape = input_shape)\n",
        "        elif model_name == 'EfficientNetB7':\n",
        "            pretrained_model = tf.keras.applications.efficientnet.EfficientNetB7(weights = 'imagenet', include_top = False, input_shape = input_shape)\n",
        "        elif model_name == 'MobileNet':\n",
        "            pretrained_model = tf.keras.applications.MobileNet(weights = 'imagenet', include_top = False, input_shape = input_shape)\n",
        "        elif model_name == 'Xception':\n",
        "            pretrained_model = tf.keras.applications.Xception(weights = 'imagenet', include_top = False, input_shape = input_shape)\n",
        "        elif model_name == 'InceptionV3':\n",
        "            pretrained_model = tf.keras.applications.InceptionV3(weights = 'imagenet', include_top = False, input_shape = input_shape)\n",
        "        elif model_name == 'InceptionV3':\n",
        "            pretrained_model = tf.keras.applications.InceptionResNetV2(weights = 'imagenet', include_top = False, input_shape = input_shape)\n",
        "        self.model_summary_and_display_structure(pretrained_model)\n",
        "        return pretrained_model\n",
        "\n",
        "    def model_plot_test_vs_predicted(self, X_test, y_test, y_pred):\n",
        "        \"\"\"\n",
        "        This method plots actual vs prected results against each images.\n",
        "        \"\"\"\n",
        "        # plot test data and associated predicred\n",
        "        fig = plt.figure(figsize=(20, 20))\n",
        "        \n",
        "        for j, example in enumerate(X_test[:20]):\n",
        "            ax = fig.add_subplot(8, 4, j+1)\n",
        "            ax.set_xticks([])\n",
        "            ax.set_yticks([])\n",
        "            ax.imshow(array_to_img(example))\n",
        "            if y_test[j]==0:\n",
        "                true_label = 'No Cancer'\n",
        "            else:\n",
        "                true_label = 'Cancer'\n",
        "    \n",
        "            ax.text(\n",
        "                0.5, -0.15, \n",
        "                'True Label: {:s}\\nPr(Cancer)={:.0f}%'.format(y_test, self.probas[j]), \n",
        "                size = 16, \n",
        "                color = 'grey',\n",
        "                horizontalalignment = 'center',\n",
        "                verticalalignment = 'center', \n",
        "                transform = ax.transAxes)\n",
        "    \n",
        "        plt.tight_layout()\n",
        "        plt.show(block = False)\n",
        "    \n",
        "    def plot_model_result_confusion_matrix(self, threshold_val, y_pred_probas, y_test):\n",
        "        \"\"\"\n",
        "        This method plots confusion matrix.\n",
        "        \"\"\"\n",
        "        predictions_val = [1 if x > threshold_val else 0 for x in y_pred_probas]\n",
        "        #print(\"a\")\n",
        "        model_confusion_matrix = confusion_matrix(y_test, predictions_val)\n",
        "        print('True Negatives: ', model_confusion_matrix[0][0])\n",
        "        print('False Positives: ', model_confusion_matrix[0][1])\n",
        "        print('False Negatives: ', model_confusion_matrix[1][0])\n",
        "        print('True Positives: ', model_confusion_matrix[1][1])\n",
        "        print('Total : ', np.sum(model_confusion_matrix[1]))\n",
        "        #print(\"b\")\n",
        "        #plot_confusion_matrix(y_pred_probas, ['Cancer', 'No Cancer'])\n",
        "        print('ROC AUC Score = ', roc_auc_score(y_test, predictions_val))\n",
        "        #print(\"c\")\n",
        "        fig, ax = plot_confusion_matrix(conf_mat = model_confusion_matrix,\n",
        "                                       show_absolute = True,\n",
        "                                       show_normed = True,\n",
        "                                       colorbar = True,\n",
        "                                       cmap = 'Dark2')\n",
        "        plt.title(f'Confusion matrix with threshold {threshold_val}')\n",
        "        plt.ylabel('Actual label')\n",
        "        plt.xlabel('Predicted label')\n",
        "        plt.show(block = False)\n",
        "\n",
        "    def plot_roc_auc_curve(self, y_pred_probas, y_test):\n",
        "        \"\"\"\n",
        "        This method plots ROC AUC Curve.\n",
        "        \"\"\"\n",
        "        #predictions_val = [1 if x > threshold_val else 0 for x in y_pred_probas]\n",
        "        fpr, tpr, thresholds = roc_curve(y_test, \n",
        "                                         y_pred_probas\n",
        "                                        )\n",
        "        auc_val = auc(fpr, tpr)\n",
        "        \n",
        "        plt.figure(1)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.plot(fpr, tpr, label='area = {:.2f}'.format(auc_val))\n",
        "        plt.xlabel('False positive rate')\n",
        "        plt.ylabel('True positive rate')\n",
        "        plt.title('ROC Curve')\n",
        "        plt.legend(loc = 'best')\n",
        "        plt.show(block = False)\n",
        "\n",
        "    def generate_report(self, threshold_val, y_pred_probas, y_test):\n",
        "        \"\"\"\n",
        "        This method generates model performance report.\n",
        "        \"\"\"\n",
        "        predictions_val = [1 if x > threshold_val else 0 for x in y_pred_probas]\n",
        "        model_report = classification_report(y_test, \n",
        "                                             predictions_val, \n",
        "                                             target_names = ['No Cancer', \n",
        "                                                             'Cancer'\n",
        "                                                            ]\n",
        "                                                )\n",
        "        print(f\"Classification report with threshold value {threshold_val}\")\n",
        "        print(model_report)\n",
        "\n",
        "    def ensemble_across_model_kfolds(self, df):\n",
        "        \"\"\"\n",
        "        This method performs ensemble method across model and all kfolds\n",
        "        using majority voting.\n",
        "        \"\"\"\n",
        "        df_kfold_ensemble_stats = df.groupby(['model', 'output_pos']).agg({'pred_bin' : [stats.mode], \n",
        "                                                                           'pred_pct' : [np.mean, np.min, np.max]}).reset_index()\n",
        "        df_kfold_ensemble_stats.columns = ['model', 'output_pos', 'majority_pred_bin', 'avg_pred_pct', 'min_pred_pct', 'max_pred_pct']\n",
        "        df_kfold_ensemble_stats[['majority_class_value', 'majority_count']] = pd.DataFrame(df_kfold_ensemble_stats['majority_pred_bin'].tolist(), \n",
        "                                                                                           index = df_kfold_ensemble_stats.index)\n",
        "        df_kfold_ensemble_stats['majority_class_value'] = df_kfold_ensemble_stats['majority_class_value'].apply(lambda x : x[0])\n",
        "        df_kfold_ensemble_stats['majority_count'] = df_kfold_ensemble_stats['majority_count'].apply(lambda x : x[0])\n",
        "        df_kfold_ensemble_stats['fmt_majority_class_value'] = np.where(df_kfold_ensemble_stats['majority_count'] == 2, \n",
        "                                                                       1, \n",
        "                                                                       df_kfold_ensemble_stats['majority_class_value'])\n",
        "        \n",
        "        # assigning back the actual value corresponding to output_pos\n",
        "        df_kfold_ensemble_stats['actual'] = df_kfold_ensemble_stats['output_pos'].apply(lambda x : data_proc.y_test[x-1])\n",
        "        # majority_class_prob will be calculated based on the majority_class.\n",
        "        df_kfold_ensemble_stats['majority_class_prob'] = None\n",
        "        for index_val, rec in df_kfold_ensemble_stats.iterrows():\n",
        "            model, output_pos, majority_pred_bin = rec[0], rec[1], rec[2]\n",
        "            avg_pred_pct, min_pred_pct, max_pred_pct = rec[3], rec[4], rec[5]\n",
        "            majority_class_value = rec[6]\n",
        "            majority_count = rec[7]\n",
        "            fmt_majority_class_value = rec[8]\n",
        "            actual= rec[9]\n",
        "            majority_class_prob = np.mean(df[(df.model == model) &\n",
        "                                             (df.output_pos == output_pos) &\n",
        "                                             (df.pred_bin == fmt_majority_class_value)\n",
        "                                            ]['pred_pct'])\n",
        "            #print(model, output_pos, fmt_majority_class_value, majority_class_prob)\n",
        "            df_kfold_ensemble_stats.iloc[index_val, -1] = majority_class_prob\n",
        "\n",
        "        df_kfold_ensemble_stats.head()\n",
        "        return df_kfold_ensemble_stats\n",
        "\n",
        "    def ensemble_across_models(self, df):\n",
        "        \"\"\"\n",
        "        This method performs ensemble methods across models using majority voting.\n",
        "        \"\"\"\n",
        "        df_ensemble_stats = df.groupby(['output_pos']).agg({'pred_bin' : [stats.mode], 'pred_pct' : [np.mean, np.min, np.max]}).reset_index()\n",
        "        df_ensemble_stats.columns = ['output_pos', 'majority_pred_bin', 'avg_pred_pct', 'min_pred_pct', 'max_pred_pct']\n",
        "        df_ensemble_stats[['majority_class_value', 'majority_count']] = pd.DataFrame(df_ensemble_stats['majority_pred_bin'].tolist(), \n",
        "                                                                                     index=df_ensemble_stats.index)\n",
        "        df_ensemble_stats['majority_class_value'] = df_ensemble_stats['majority_class_value'].apply(lambda x : x[0])\n",
        "        df_ensemble_stats['majority_count'] = df_ensemble_stats['majority_count'].apply(lambda x : x[0])\n",
        "        df_ensemble_stats['fmt_majority_class_value'] = np.where(df_ensemble_stats['majority_count'] == 2, \n",
        "                                                                 1, \n",
        "                                                                 df_ensemble_stats['majority_class_value'])\n",
        "        # assigning back the actual value corresponding to output_pos\n",
        "        df_ensemble_stats['actual'] = df_ensemble_stats['output_pos'].apply(lambda x : data_proc.y_test[x-1])\n",
        "        \n",
        "        # majority_class_prob will be calculated based on the majority_class.\n",
        "        df_ensemble_stats['majority_class_prob'] = None\n",
        "        \n",
        "        for index, rec in df_ensemble_stats.iterrows():\n",
        "            output_pos, majority_pred_bin = rec[0], rec[1]\n",
        "            avg_pred_pct, min_pred_pct, max_pred_pct = rec[2], rec[3], rec[4]\n",
        "            majority_class_value = rec[5]\n",
        "            majority_count = rec[6]\n",
        "            fmt_majority_class_value = rec[7]\n",
        "            actual= rec[8]\n",
        "\n",
        "            majority_class_prob = np.mean(df_actual_vs_pred_bin_pred_pct[(df.output_pos == output_pos) &\n",
        "                                                                         (df.pred_bin == fmt_majority_class_value)\n",
        "                                                                        ]['pred_pct'])\n",
        "            #print(model, output_pos, fmt_majority_class_value, majority_class_prob)\n",
        "            df_ensemble_stats.iloc[index, -1] = majority_class_prob\n",
        "\n",
        "        \n",
        "        df_ensemble_stats.head()\n",
        "        return df_ensemble_stats\n",
        "\n",
        "    def display_model_stats_across_all_spilts(self, model_name, consolidated_df_model_kpi, df_actual_vs_pred_bin_pred_pct):\n",
        "        \"\"\"\n",
        "        This function displays model specific summary stats across all splits.\n",
        "        \"\"\"\n",
        "        # Displays test accuracy and loss across all spilts.\n",
        "        for fold_idx in range(0, number_of_splits):\n",
        "            print('------------------------------------------------------------------------')\n",
        "            accuracy = df_actual_vs_pred_bin_pred_pct[(df_actual_vs_pred_bin_pred_pct.model == model_name) & \n",
        "                                                      (df_actual_vs_pred_bin_pred_pct.kfold == fold_idx + 1)\n",
        "                                                    ]['test_accuracy'].drop_duplicates() * 100\n",
        "            loss = df_actual_vs_pred_bin_pred_pct[(df_actual_vs_pred_bin_pred_pct.model == model_name) & \n",
        "                                                  (df_actual_vs_pred_bin_pred_pct.kfold == fold_idx + 1)\n",
        "                                                ]['test_loss'].drop_duplicates()\n",
        "            print(f'> Model {model_name} : Fold: {fold_idx + 1} - Loss: {loss[0]} - Accuracy: {accuracy[0]}%')\n",
        "            print('------------------------------------------------------------------------')\n",
        "\n",
        "        # Average and std for train accuracy and loss\n",
        "        average_train_accuracy_per_fold = consolidated_df_model_kpi[(consolidated_df_model_kpi.model == model_name) & \n",
        "                                                                    (consolidated_df_model_kpi.epoch == epochs)\n",
        "                                                                   ].groupby(['model']).agg({'accuracy': np.mean}).values[0][0]\n",
        "        std_for_average_train_accuracy_per_fold = consolidated_df_model_kpi[(consolidated_df_model_kpi.model == model_name) & \n",
        "                                                                    (consolidated_df_model_kpi.epoch == epochs)\n",
        "                                                                   ].groupby(['model']).agg({'accuracy': np.std}).values[0][0]\n",
        "    \n",
        "        average_train_loss_per_fold = consolidated_df_model_kpi[(consolidated_df_model_kpi.model == model_name) & \n",
        "                                                                    (consolidated_df_model_kpi.epoch == epochs)\n",
        "                                                                   ].groupby(['model']).agg({'loss': np.mean}).values[0][0]\n",
        "        std_for_average_train_loss_per_fold = consolidated_df_model_kpi[(consolidated_df_model_kpi.model == model_name) & \n",
        "                                                                    (consolidated_df_model_kpi.epoch == epochs)\n",
        "                                                                   ].groupby(['model']).agg({'loss': np.std}).values[0][0]\n",
        "\n",
        "        # Average and std for validation accuracy and loss\n",
        "        average_validation_accuracy_per_fold = consolidated_df_model_kpi[(consolidated_df_model_kpi.model == model_name) & \n",
        "                                                                    (consolidated_df_model_kpi.epoch == epochs)\n",
        "                                                                   ].groupby(['model']).agg({'val_accuracy': np.mean}).values[0][0]\n",
        "        std_for_average_validation_accuracy_per_fold = consolidated_df_model_kpi[(consolidated_df_model_kpi.model == model_name) & \n",
        "                                                                    (consolidated_df_model_kpi.epoch == epochs)\n",
        "                                                                   ].groupby(['model']).agg({'val_accuracy': np.std}).values[0][0]\n",
        "    \n",
        "        average_validation_loss_per_fold = consolidated_df_model_kpi[(consolidated_df_model_kpi.model == model_name) & \n",
        "                                                                    (consolidated_df_model_kpi.epoch == epochs)\n",
        "                                                                   ].groupby(['model']).agg({'val_loss': np.mean}).values[0][0]\n",
        "        std_for_average_validation_loss_per_fold = consolidated_df_model_kpi[(consolidated_df_model_kpi.model == model_name) & \n",
        "                                                                    (consolidated_df_model_kpi.epoch == epochs)\n",
        "                                                                   ].groupby(['model']).agg({'val_loss': np.std}).values[0][0]\n",
        "\n",
        "        # Average and std for test accuracy and loss\n",
        "        average_test_accuracy_per_fold = np.mean(df_actual_vs_pred_bin_pred_pct[df_actual_vs_pred_bin_pred_pct.model == model_name].test_accuracy)\n",
        "        std_for_average_test_accuracy_per_fold = np.std(df_actual_vs_pred_bin_pred_pct[df_actual_vs_pred_bin_pred_pct.model == model_name].test_accuracy)\n",
        "    \n",
        "        average_test_loss_per_fold = np.mean(df_actual_vs_pred_bin_pred_pct[df_actual_vs_pred_bin_pred_pct.model == model_name].test_loss)\n",
        "        std_for_average_test_loss_per_fold = np.std(df_actual_vs_pred_bin_pred_pct[df_actual_vs_pred_bin_pred_pct.model == model_name].test_loss)\n",
        "    \n",
        "        print(f'Average train, validation and test accuracy and loss for all folds for the model : {model_name}')\n",
        "        print(f'> Train accuracy: {average_train_accuracy_per_fold} (+- {std_for_average_test_accuracy_per_fold})')\n",
        "        print(f'> Train loss: {average_train_loss_per_fold} (+- {std_for_average_train_accuracy_per_fold})')\n",
        "\n",
        "        print(f'> Validation accuracy: {average_validation_accuracy_per_fold} (+- {std_for_average_validation_accuracy_per_fold})')\n",
        "        print(f'> Validation loss: {average_validation_loss_per_fold} (+- {std_for_average_validation_loss_per_fold})')\n",
        "    \n",
        "        print(f'> Test accuracy: {average_test_accuracy_per_fold} (+- {std_for_average_test_accuracy_per_fold})')\n",
        "        print(f'> Test loss: {average_test_loss_per_fold} (+- {std_for_average_test_loss_per_fold})')\n",
        "\n",
        "model_proc = misc_model_functionality_processing()"
      ],
      "metadata": {
        "id": "HC8mI7nSOBnZ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***2. Data Processing***"
      ],
      "metadata": {
        "id": "GMct-xh6t74w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class data_processing:\n",
        "    \"\"\"\n",
        "    This class mimics the data processing pipeline.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, run_mode):\n",
        "        self.train_file_list = []\n",
        "        self.test_file_list = []\n",
        "        self.run_mode = run_mode\n",
        "        np.random.seed(cfg_proc.random_state)\n",
        "                \n",
        "    def get_file_names_list(self):\n",
        "        \"\"\"\n",
        "    \t  This method builds the list of train and test files.\n",
        "        It also reads the original color images and save them with _gs extension \n",
        "        in the same path as the original image. The grayscale images will be \n",
        "        used for modeling whereas the color images are used for data \n",
        "        visualization purposes.\n",
        "    \t  \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"Starting to build fully qualified train and test file name lists...\")\n",
        "        # Original input images are color which we will later convert to grascale.\n",
        "        self.train_file_color_list = misc_proc.get_id_and_label_list(cfg_proc.train_files_path, \n",
        "                                                                     cfg_proc.image_file_extension\n",
        "                                                                    )\n",
        "        self.test_file_color_list = misc_proc.get_id_and_label_list(cfg_proc.test_files_path, \n",
        "                                                                    cfg_proc.image_file_extension\n",
        "                                                                   )\n",
        "        self.train_file_list = []\n",
        "\n",
        "        # Loading the input images as grayscale images and saving them back with \n",
        "        # \"_gs\" extension to distinguish.\n",
        "        # We are also adding 90/180/270 deg rotation images in the train set.\n",
        "        # For modeling purpose, we will use the grayscale images and \n",
        "        # for visualization purposes we will use the original color images.\n",
        "        for image_file in self.train_file_color_list:\n",
        "\n",
        "            img_gs = load_img(image_file, color_mode = \"grayscale\")\n",
        "            img_array_gs = img_to_array(img_gs)\n",
        "            save_img(image_file.split(\".\")[0] + \n",
        "                     '_gs' + \n",
        "                     cfg_proc.image_file_extension, \n",
        "                     img_array_gs\n",
        "                    )\n",
        "            self.train_file_list.append(image_file.split(\".\")[0] + '_gs' + \n",
        "                                        cfg_proc.image_file_extension\n",
        "                                       )\n",
        "            \n",
        "        if self.run_mode == 'interim_test':\n",
        "            print(f\"Length of train_file_list : {len(self.train_file_list)}\")\n",
        "            print(f\"Length of test_file_list : {len(self.test_file_list)}\")\n",
        "        print(\"Completed building train and test file name lists...\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print()\n",
        "\n",
        "    def get_label_info(self):\n",
        "        \"\"\"\n",
        "    \t  This method reads the train and test label information from \n",
        "        train_labels.csv and sample_submission.csv.\n",
        "        These files have the below structure:-\n",
        "        id and label.\n",
        "        Corresponding to the train or test id, there will be an image file \n",
        "        prssent in the respective train oo test folder.\n",
        "    \t  \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"Starting to get label info...\")\n",
        "        self.train_label = pd.read_csv(cfg_proc.train_label_file)\n",
        "        self.test_label = pd.read_csv(cfg_proc.test_label_file)\n",
        "        if self.run_mode == 'interim_test':\n",
        "            print(f\"Number of train labels : {len(self.train_label)}\")\n",
        "            print(f\"Number of test labels : {len(self.test_label)}\")\n",
        "\n",
        "        self.qualified_train_file_names_list = misc_proc.generate_fully_qualified_file_name_list(self.train_label.id.values.tolist())\n",
        "        self.qualified_test_file_names_list = misc_proc.generate_fully_qualified_file_name_list(self.test_label.id.values.tolist())\n",
        "        \n",
        "        self.train_label_positive = self.train_label[self.train_label['label'] == 1]\n",
        "        self.train_label_negative = self.train_label[self.train_label['label'] == 0]\n",
        "        \n",
        "        print(\"Completed getting label info...\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print()\n",
        "\n",
        "    def create_labels(self, train_val_test_ind, data):\n",
        "        \"\"\"\n",
        "        This method creates label of given length.\n",
        "        \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(f\"Starting to create labels for {train_val_test_ind}...\")\n",
        "        if train_val_test_ind.lower() == 'train':\n",
        "            self.y_train = np.asarray(data['label'].values.tolist())\n",
        "        elif train_val_test_ind.lower() == 'test':\n",
        "            self.y_test = np.asarray(data['label'].values.tolist())\n",
        "        elif train_val_test_ind.lower() == 'validation':\n",
        "            self.y_validation = np.asarray(data['label'].values.tolist())    \n",
        "        print(f\"Completed building labels for {train_val_test_ind}...\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print()\n",
        "\n",
        "    def create_rotated_images(self, target_dir, rotation_angle_ind = 1):\n",
        "        \"\"\"\n",
        "        This method will crate the rotated images and save it in the target path.\n",
        "        Rotation angle indicator 1 : 90 Deg  clockwise\n",
        "        Rotation angle indicator 2 : 180 Deg clockwise\n",
        "        Rotation angle indicator 3 : 270 Deg clockwise\n",
        "        \n",
        "        \"\"\"\n",
        "        image_file_list = [file for file in os.listdir(target_dir) if cfg_proc.image_file_extension in file and file[0] != '.']\n",
        "        for image_file in image_file_list:\n",
        "\n",
        "            img_gs = load_img(image_file, color_mode = \"grayscale\")\n",
        "            img_array_gs = img_to_array(img_gs)\n",
        "            if rotation_angle_ind == 1 :\n",
        "                rot_angle = 90\n",
        "                img_rotated_gs = cv2.rotate(img_array_gs, \n",
        "                                            cv2.ROTATE_90_CLOCKWISE\n",
        "                                           )\n",
        "            elif rotation_angle_ind == 2 :\n",
        "                rot_angle = 180\n",
        "                img_rotated_gs = cv2.rotate(img_array_gs, \n",
        "                                            cv2.ROTATE_180\n",
        "                                           )\n",
        "            elif rotation_angle_ind == 3 :\n",
        "                rot_angle = 270\n",
        "                img_rotated_gs = cv2.rotate(img_array_gs, \n",
        "                                            cv2.ROTATE_90_COUNTERCLOCKWISE\n",
        "                                           )\n",
        "            img_array_rotated_gs = img_to_array(img_rotated_gs)\n",
        "            \n",
        "            save_img(image_file.split(\".\")[0] +'_' + str(rot_angle) + \n",
        "                     '_rot_gs' + \n",
        "                     cfg_proc.image_file_extension, \n",
        "                     img_rotated_gs\n",
        "                    )\n",
        "            \n",
        "    def split_data_based_on_indices(self, train_indices, validation_indices, add_extra_image_flag = 0):\n",
        "        \"\"\"\n",
        "        This method splits data based on indices.\n",
        "        \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"Starting to split data based on indices...\")\n",
        "        # New train and validation set and corresponding labels based on the kfold split process generated indices.\n",
        "        self.df_train = self.df_train_original.iloc[train_indices]\n",
        "        self.df_validation = self.df_train_original.iloc[validation_indices]\n",
        "        self.create_labels(train_val_test_ind = 'train', data = self.df_train)\n",
        "        self.create_labels(train_val_test_ind = 'validation', data = self.df_validation)\n",
        "\n",
        "        if self.run_mode == 'interim_test':\n",
        "            print(f\"Length of train data : {len(self.df_train)}, length of validation data : {len(self.df_validation)}, length of test data : {len(self.df_test)}\")\n",
        "            print(f\"Length of train positive data : {len(self.df_train[self.df_train.label == 1])}, length of validation positive data : {len(self.df_validation[self.df_validation.label == 1])}, length of test positive data : {len(self.df_test[self.df_test.label == 1])}\")\n",
        "            print(f\"Length of train negative data : {len(self.df_train[self.df_train.label == 0])}, length of validation negative data : {len(self.df_validation[self.df_validation.label == 0])}, length of test negative data : {len(self.df_test[self.df_test.label == 0])}\")\n",
        "        \n",
        "        \"\"\"\n",
        "        Both df_train and df_validation have three columns id, id_gs and label.\n",
        "        id is the original color file name without extension and id_gs is the\n",
        "        grayscale file name, derived off id column along with a \"_gs\" suffix.\n",
        "        For modeling purpose, we will use the _gs file and for data visulaization\n",
        "        purposes, we will use the original color images.\n",
        "        Thus, to move the files in a different directory, we will move the \n",
        "        grayscale images.\n",
        "        \"\"\"\n",
        "\n",
        "        misc_proc.remove_files_from_dir(cfg_proc.image_processing_train_positive_path)\n",
        "        misc_proc.remove_files_from_dir(cfg_proc.image_processing_train_negative_path)\n",
        "        self.train_positive_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_train[self.df_train.label == 1].id_gs.values.tolist())\n",
        "        self.train_negative_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_train[self.df_train.label == 0].id_gs.values.tolist())\n",
        "        print(f\"Copying train_positive_file_list under {cfg_proc.image_processing_train_positive_path}\")\n",
        "        misc_proc.process_copy_files(self.train_positive_file_list, cfg_proc.image_processing_train_positive_path)\n",
        "        print(f\"Copying train_negative_file_list under {cfg_proc.image_processing_train_negative_path}\")\n",
        "        misc_proc.process_copy_files(self.train_negative_file_list, cfg_proc.image_processing_train_negative_path)\n",
        "        print(f\"File count under {cfg_proc.image_processing_train_positive_path} after moving new files is: {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_train_positive_path)}\")\n",
        "        print(f\"File count under {cfg_proc.image_processing_train_negative_path} after moving new files is : {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_train_negative_path)}\")\n",
        "\n",
        "        # Adding extra rotated images for train images only : Start of the changes #\n",
        "        # Based on the indicator column add_extra_image_flag, we can add extra images or not.\n",
        "        # Default value for add_extra_image_flag = 0, which means no extra image will be added for training set.\n",
        "        # If the value is 1, then it will add 90, 180 and 270 deg rotated images on top of original training images.\n",
        "        if add_extra_image_flag == 1:\n",
        "\n",
        "            print(f\"Adding 90 deg rotated images under {image_processing_train_positive_path}...\")\n",
        "            self.create_rotated_images(self.image_processing_train_positive_path, 1)\n",
        "            print(f\"File count after adding 90 deg rotated images under {cfg_proc.image_processing_train_positive_path} after moving new files is: {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_train_positive_path)}\")\n",
        "            print(f\"Adding 90 deg rotated images under {image_processing_train_negative_path}...\")\n",
        "            self.create_rotated_images(self.image_processing_train_negative_path, 1)\n",
        "            print(f\"File count after adding 90 deg rotated images under {cfg_proc.image_processing_train_negative_path} after moving new files is : {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_train_negative_path)}\")\n",
        "\n",
        "            print(f\"Adding 180 deg rotated images under {image_processing_train_positive_path}...\")\n",
        "            self.create_rotated_images(self.image_processing_train_positive_path, 2)\n",
        "            print(f\"File count after adding 180 deg rotated images under {cfg_proc.image_processing_train_positive_path} after moving new files is: {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_train_positive_path)}\")\n",
        "            print(f\"Adding 180 deg rotated images under {image_processing_train_negative_path}...\")\n",
        "            self.create_rotated_images(self.image_processing_train_negative_path, 2)\n",
        "            print(f\"File count after adding 180 deg rotated images under {cfg_proc.image_processing_train_negative_path} after moving new files is : {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_train_negative_path)}\")\n",
        "\n",
        "            print(f\"Adding 270 deg rotated images under {image_processing_train_positive_path}...\")\n",
        "            self.create_rotated_images(self.image_processing_train_positive_path, 3)\n",
        "            print(f\"File count after adding 90 deg rotated images under {cfg_proc.image_processing_train_positive_path} after moving new files is: {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_train_positive_path)}\")\n",
        "            print(f\"Adding 270 deg rotated images under {image_processing_train_negative_path}...\")\n",
        "            self.create_rotated_images(self.image_processing_train_negative_path, 3)\n",
        "            print(f\"File count after adding 90 deg rotated images under {cfg_proc.image_processing_train_negative_path} after moving new files is : {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_train_negative_path)}\")\n",
        "        \n",
        "        # Adding extra rotated images for train images only : End of the changes #\n",
        "        \n",
        "        misc_proc.remove_files_from_dir(cfg_proc.image_processing_validation_positive_path)\n",
        "        misc_proc.remove_files_from_dir(cfg_proc.image_processing_validation_negative_path)\n",
        "        self.validation_positive_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_validation[self.df_validation.label == 1].id_gs.values.tolist())\n",
        "        self.validation_negative_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_validation[self.df_validation.label == 0].id_gs.values.tolist())\n",
        "        print(f\"Copying validation_positive_file_list under {cfg_proc.image_processing_validation_positive_path}\")\n",
        "        misc_proc.process_copy_files(self.validation_positive_file_list, cfg_proc.image_processing_validation_positive_path)\n",
        "        print(f\"Copying validation_negative_file_list under {cfg_proc.image_processing_validation_negative_path}\")\n",
        "        misc_proc.process_copy_files(self.validation_negative_file_list, cfg_proc.image_processing_validation_negative_path)\n",
        "        print(f\"File count under {cfg_proc.image_processing_validation_positive_path} after moving new files is: {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_validation_positive_path)}\")\n",
        "        print(f\"File count under {cfg_proc.image_processing_validation_negative_path} after moving new files is : {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_validation_negative_path)}\")\n",
        "\n",
        "        '''\n",
        "        misc_proc.remove_files_from_dir(image_processing_test_positive_path)\n",
        "        misc_proc.remove_files_from_dir(image_processing_test_negative_path)\n",
        "        self.test_positive_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_test[self.df_test.label == 1].id_gs.values.tolist())\n",
        "        self.test_negative_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_test[self.df_test.label == 0].id_gs.values.tolist())\n",
        "        print(f\"Copying test_positive_file_list under {image_processing_test_positive_path}\")\n",
        "        misc_proc.process_copy_files(self.test_positive_file_list, image_processing_test_positive_path)\n",
        "        print(f\"Copying test_negative_file_list under {image_processing_test_negative_path}\")\n",
        "        misc_proc.process_copy_files(self.test_negative_file_list, image_processing_test_negative_path)\n",
        "        print(f\"File count under {image_processing_test_positive_path} after moving new files is: {misc_proc.check_file_count_in_a_directory(image_processing_test_positive_path)}\")\n",
        "        print(f\"File count under {image_processing_test_negative_path} after moving new files is : {misc_proc.check_file_count_in_a_directory(image_processing_test_negative_path)}\")\n",
        "        '''\n",
        "        print(\"Completed spliting the data sets based on indices...\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print()\n",
        "\n",
        "    def initial_split_data(self):\n",
        "        \"\"\"\n",
        "    \t  This method uses train data to split into train, validation and test sets.\n",
        "    \t  The reason we are repurposing the train set is because we do not have labels for test data.\n",
        "    \t  We also see data imbalance issue and thus we are undersampling the most populated class (negative images).\n",
        "    \t  \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"Starting to split data...\")\n",
        "\n",
        "        \"\"\"\n",
        "        Extracting top sample_size records from train_label_positive and \n",
        "        train_label_negative seperately and then combine them together so \n",
        "        that distribution is uniform.\n",
        "        \"\"\"\n",
        "        self.train_label_sample_positive = self.train_label_positive.head(cfg_proc.sample_size)\n",
        "        self.train_label_sample_negative = self.train_label_negative.head(cfg_proc.sample_size)\n",
        "        self.train_label_processed = pd.concat([self.train_label_sample_negative, \n",
        "          \t                                    self.train_label_sample_positive\n",
        "        \t                                     ], \n",
        "        \t                                     axis = 0).reset_index(drop = True)\n",
        "\n",
        "        \"\"\"\n",
        "        Getting the remaining records (length of uiverse - sample size) serves\n",
        "        as test data set. We have also made sure distribution is uniform here.\n",
        "        \"\"\"\n",
        "        '''\n",
        "        remaining_length = 50 #len(self.train_label_positive) - len(self.train_label_sample_positive)\n",
        "        self.test_positive_df = self.train_label_positive[sample_size : sample_size + remaining_length]\n",
        "        self.test_negative_df = self.train_label_negative[sample_size : sample_size + remaining_length]\n",
        "        self.df_test = pd.concat([self.test_positive_df, self.test_negative_df], axis = 0).reset_index(drop = True)\n",
        "        self.df_test = shuffle(self.df_test, random_state = random_state)\n",
        "        self.create_labels(train_val_test_ind = 'test', data = self.df_test)\n",
        "        '''\n",
        "\n",
        "        # shuffle\n",
        "        self.train_label_processed = shuffle(self.train_label_processed, random_state = cfg_proc.random_state)\n",
        "        label = self.train_label_processed['label']\n",
        "        self.df_train, self.df_test = train_test_split(self.train_label_processed, \n",
        "          \t                                           test_size = 0.1, \n",
        "        \t                                             random_state =  cfg_proc.random_state, \n",
        "        \t                                             stratify = label\n",
        "        \t                                            )\n",
        "        self.create_labels(train_val_test_ind = 'train', data = self.df_train)\n",
        "        self.create_labels(train_val_test_ind = 'test', data = self.df_test)\n",
        "        self.df_train['id_gs'] = self.df_train['id'].apply(lambda x : x + '_gs')\n",
        "        self.df_test['id_gs']  = self.df_test['id'].apply(lambda x : x + '_gs')\n",
        "        self.df_train_original = copy.deepcopy(self.df_train)\n",
        "\n",
        "        \"\"\"\n",
        "        At this point, df_test has three columns id, id_gs and label.\n",
        "        id denotes original color image name without extension and id_gs is the\n",
        "        grayscale image name derived off id column data, suffixed with '_gs'\n",
        "        extension.\n",
        "        We will use grayscale images for all our training, so while moving the\n",
        "        images to appropriate directory, we need to move the grayscale images.\n",
        "        \"\"\"\n",
        "        misc_proc.create_dir_structure(root_directory = '/content/image_processing')\n",
        "        misc_proc.remove_files_from_dir(cfg_proc.image_processing_test_positive_path)\n",
        "        misc_proc.remove_files_from_dir(cfg_proc.image_processing_test_negative_path)\n",
        "        self.test_positive_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_test[self.df_test.label == 1].id_gs.values.tolist())\n",
        "        self.test_negative_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_test[self.df_test.label == 0].id_gs.values.tolist())\n",
        "        print(f\"Copying test_positive_file_list under {cfg_proc.image_processing_test_positive_path}\")\n",
        "        misc_proc.process_copy_files(self.test_positive_file_list, cfg_proc.image_processing_test_positive_path)\n",
        "        print(f\"Copying test_negative_file_list under {cfg_proc.image_processing_test_negative_path}\")\n",
        "        misc_proc.process_copy_files(self.test_negative_file_list, cfg_proc.image_processing_test_negative_path)\n",
        "        print(f\"File count under {cfg_proc.image_processing_test_positive_path} is {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_test_positive_path)}\")\n",
        "        print(f\"File count under {cfg_proc.image_processing_test_negative_path} is {misc_proc.check_file_count_in_a_directory(cfg_proc.image_processing_test_negative_path)}\")\n",
        "\n",
        "        self.sample_positive_label = self.train_label_sample_positive['label'].values.tolist()\n",
        "        self.sample_negative_label = self.train_label_sample_negative['label'].values.tolist()\n",
        "\n",
        "        self.df_train_positive = self.df_train[self.df_train.label == 1]\n",
        "        self.df_train_negative = self.df_train[self.df_train.label == 0]\n",
        "\n",
        "        self.df_test_positive = self.df_test[self.df_test.label == 1]\n",
        "        self.df_test_negative = self.df_test[self.df_test.label == 0]\n",
        "\n",
        "        # Train color files\n",
        "        self.train_positive_color_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_train_positive.id.values.tolist())\n",
        "        self.train_negative_color_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_train_negative.id.values.tolist())\n",
        "\n",
        "        # Test color files\n",
        "        self.test_positive_color_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_test_positive.id.tolist())\n",
        "        self.test_negative_color_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_test_negative.id.tolist())\n",
        "\n",
        "        # Train grayscale files\n",
        "        self.train_positive_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_train_positive.id_gs.values.tolist())\n",
        "        self.train_negative_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_train_negative.id_gs.values.tolist())\n",
        "\n",
        "        # Test grayscale files\n",
        "        self.test_positive_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_test_positive.id_gs.tolist())\n",
        "        self.test_negative_file_list = misc_proc.generate_fully_qualified_file_name_list(self.df_test_negative.id_gs.tolist())\n",
        "\n",
        "        if self.run_mode == 'interim_test':\n",
        "            \n",
        "            print(f\"Length of df_train : {len(self.df_train)}\")\n",
        "            print(f\"Length of df_test : {len(self.df_test)}\")\n",
        "            print(f\"Length of y_train : {len(self.y_train)}\")\n",
        "            print(f\"Length of y_test : {len(self.y_test)}\")\n",
        "\n",
        "            print(\"Positive and negative images distribution in df_train\")\n",
        "            print(self.df_train['label'].value_counts())\n",
        "\n",
        "            print(\"Positive and negative images distribution in df_test\")\n",
        "            print(self.df_test['label'].value_counts())\n",
        "\n",
        "            print(f\"Length of df_train_positive : {len(self.df_train_positive)}\")\n",
        "            print(f\"Length of df_train_positive : {len(self.df_train_positive)}\")\n",
        "\n",
        "            print(f\"Length of df_test_positive : {len(self.df_test_positive)}\")\n",
        "            print(f\"Length of df_test_negative : {len(self.df_test_negative)}\")\n",
        "\n",
        "            print(f\"Length of train_positive_file_list : {len(self.train_positive_file_list)}\")\n",
        "            print(f\"Length of train_negative_file_list : {len(self.train_negative_file_list)}\")\n",
        "\n",
        "            print(f\"Length of test_positive_file_list : {len(self.test_positive_file_list)}\")\n",
        "            print(f\"Length of test_negative_file_list : {len(self.test_negative_file_list)}\")\n",
        "\n",
        "        print(\"Completed spliting the data sets...\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print()\n",
        "\n",
        "    def get_data_distribution(self, train_or_test_ind):\n",
        "        \"\"\"\n",
        "     \t  This method shows the distribution of positive and negative images in the data set. \n",
        "     \t  \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(f\"Starting to get data distributions for {train_or_test_ind}...\")\n",
        "        if train_or_test_ind.lower() == 'train':\n",
        "            print(\"Data distribution in the train data set\")\n",
        "            print(self.train_label['label'].value_counts())\n",
        "            data_viz.count_plot(data = self.train_label, \n",
        "                                 label_col = 'label',\n",
        "                                 title_val = \"Distribution of Labels in Train Data\"\n",
        "                                )\n",
        "            data_viz.pie_chart_plot(data = self.train_label, \n",
        "                                     label_col = 'label',\n",
        "                                     title_val = \"Train Label Percentage Pie Chart\"\n",
        "                                    )\n",
        "        elif train_or_test_ind.lower() == 'test':\n",
        "            print(\"Data distribution in the test data set\")\n",
        "            print(self.test_label['label'].value_counts())  \n",
        "            data_viz.count_plot(data = self.test_label, \n",
        "                                 label_col = 'label',\n",
        "                                 title_val = \"Distribution of Labels in Test Data\"\n",
        "                                )\n",
        "            data_viz.pie_chart_plot(data = self.test_label, \n",
        "                                     label_col = 'label',\n",
        "                                     title_val = \"Test Label Percentage Pie Chart\"\n",
        "                                    )\n",
        "        print(f\"Completed getting data distributions for {train_or_test_ind}...\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print()\n",
        "\n",
        "    def check_duplicate_ids(self, train_or_test_ind):\n",
        "        \"\"\"\n",
        "    \t  This method checks if there is any duplicate ids in the data set.\n",
        "    \t  \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(f\"Starting to check duplicates for {train_or_test_ind}...\")\n",
        "        if train_or_test_ind.lower() == 'train':\n",
        "            df_train_id_count = pd.DataFrame(self.train_label.groupby(['id'])['id'].count())\n",
        "            df_train_id_count.columns = ['id_count']\n",
        "            df_train_id_count.reset_index(inplace = True)\n",
        "            print(\"Number of train duplicate entries : \", len(df_train_id_count[df_train_id_count.id_count > 1]))\n",
        "        elif train_or_test_ind.lower() == 'test':\n",
        "            df_test_id_count = pd.DataFrame(self.test_label.groupby(['id'])['id'].count())\n",
        "            df_test_id_count.columns = ['id_count']\n",
        "            df_test_id_count.reset_index(inplace = True)\n",
        "            print(\"Number of test duplicate entries : \", len(df_test_id_count[df_test_id_count.id_count > 1]))\n",
        "        print(f\"Completed checking duplicates for {train_or_test_ind}...\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print()\n",
        "\n",
        "    def data_visualization(self, train_or_test_ind, positive_or_negative_ind, image_list, number_of_images = 5):\n",
        "        \"\"\"\n",
        "        This method visualizes the data.\n",
        "        \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(f\"Starting data visualization for {train_or_test_ind} and {positive_or_negative_ind}...\")\n",
        "        if train_or_test_ind.lower() == 'train':\n",
        "            print(f\"Displaying training {positive_or_negative_ind.lower()} images\")\n",
        "        if train_or_test_ind.lower() == 'test':\n",
        "            print(f\"Displaying test {positive_or_negative_ind.lower()} images\")\n",
        "\n",
        "        for image in image_list[:number_of_images]:\n",
        "            misc_proc.print_image_in_diff_orientation(image)\n",
        "            plt.show(block = False)\n",
        "\n",
        "        print(f\"Completed getting data visualizations for {train_or_test_ind}...\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print()\n",
        "\n",
        "    def get_image_summary_stats(self):\n",
        "        \"\"\"\n",
        "        This method gets positive and negative images summary stats at the picture level and each color (R, G, B) channel level.\n",
        "        \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"Starting to get positive and negative images summary stats...\")\n",
        "\n",
        "        # Whole image wise stats\n",
        "        print(\"Mean and standard deviation at center for positive train images: \", misc_proc.compute_mean_and_std(self.train_positive_color_file_list))\n",
        "        print(\"Mean and standard deviation at center for negative train images: \", misc_proc.compute_mean_and_std(self.train_negative_color_file_list))\n",
        "\n",
        "        number_of_bins = 64 \n",
        "        figw, axw = plt.subplots(1,2, sharey = True, sharex = True, figsize = (8,2), dpi = 150)\n",
        "        axw[0].hist(img_proc.read_image_file_in_np_array(self.train_positive_color_file_list)[:,:,:,(0,1,2)].flatten(),\n",
        "                                                        bins = number_of_bins, \n",
        "                                                        density = True);\n",
        "        axw[1].hist(img_proc.read_image_file_in_np_array(self.train_negative_color_file_list)[:,:,:,(0,1,2)].flatten(),\n",
        "                                                        bins = number_of_bins, \n",
        "                                                        density = True);\n",
        "\n",
        "        axw[0].set_title(\"Train positive images\");\n",
        "        axw[1].set_title(\"Train negative images\");\n",
        "\n",
        "        axw[0].set_xlabel(\"Mean brightness\")\n",
        "        axw[1].set_xlabel(\"Mean brightness\")\n",
        "        axw[0].set_ylabel(\"Relative frequency\")\n",
        "        axw[1].set_ylabel(\"Relative frequency\")\n",
        "        plt.show(block = False);\n",
        "\n",
        "        # Channel wise stats\n",
        "        print(\"Average across red, green and blue channels for train positive images\")\n",
        "        print(np.mean(img_proc.read_image_file_in_np_array(self.train_positive_color_file_list), axis = (0,1,2)))\n",
        "        print(\"Standard Deviation across red, green and blue channels for Train positive images\")\n",
        "        print(np.std(img_proc.read_image_file_in_np_array(self.train_positive_color_file_list), axis = (0,1,2)))\n",
        "\n",
        "        print(\"Average across red, green and blue channels for train X_train_img_file_negative images\")\n",
        "        print(np.mean(img_proc.read_image_file_in_np_array(self.train_negative_color_file_list), axis = (0,1,2)))\n",
        "        print(\"Standard Deviation across red, green and blue channels for train X_train_img_file_negative images\")\n",
        "        print(np.std(img_proc.read_image_file_in_np_array(self.train_negative_color_file_list), axis = (0,1,2)))\n",
        "\n",
        "        # Red Channel\n",
        "        figr, axr = plt.subplots(1,2, sharey = True, sharex = True, figsize = (8,2), dpi = 150)\n",
        "        axr[0].hist(img_proc.read_image_file_in_np_array(self.train_positive_color_file_list)[:,:,:,(0)].flatten(),\n",
        "                                                        bins = number_of_bins, \n",
        "                                                        density = True);\n",
        "        axr[1].hist(img_proc.read_image_file_in_np_array(self.train_negative_color_file_list)[:,:,:,(0)].flatten(),\n",
        "                                                        bins = number_of_bins, \n",
        "                                                        density = True);\n",
        "\n",
        "        axr[0].set_title(\"Train positive images\");\n",
        "        axr[1].set_title(\"Train negative images\");\n",
        "\n",
        "        axr[0].set_xlabel(\"Mean red brightness\")\n",
        "        axr[1].set_xlabel(\"Mean red brightness\")\n",
        "        axr[0].set_ylabel(\"Relative frequency\")\n",
        "        axr[1].set_ylabel(\"Relative frequency\")\n",
        "        plt.show(block = False);\n",
        "\n",
        "        # Green Channel\n",
        "        figg, axg = plt.subplots(1,2, sharey = True, sharex = True, figsize = (8,2), dpi = 150)\n",
        "        axg[0].hist(img_proc.read_image_file_in_np_array(self.train_positive_color_file_list)[:,:,:,(1)].flatten(),\n",
        "                                                        bins = number_of_bins, \n",
        "                                                        density = True);\n",
        "        axg[1].hist(img_proc.read_image_file_in_np_array(self.train_negative_color_file_list)[:,:,:,(1)].flatten(),\n",
        "                                                        bins = number_of_bins, \n",
        "                                                        density = True);\n",
        "\n",
        "        axg[0].set_title(\"Train positive images\");\n",
        "        axg[1].set_title(\"Train negative images\");\n",
        "\n",
        "        axg[0].set_xlabel(\"Mean green brightness\")\n",
        "        axg[1].set_xlabel(\"Mean green brightness\")\n",
        "        axg[0].set_ylabel(\"Relative frequency\")\n",
        "        axg[1].set_ylabel(\"Relative frequency\")\n",
        "        plt.show(block = False);\n",
        "\n",
        "        # Blue Channel\n",
        "        figb, axb = plt.subplots(1,2, \n",
        "                                 sharey = True, \n",
        "                                 sharex = True, \n",
        "                                 figsize = (8,2), \n",
        "                                 dpi = 150\n",
        "                                )\n",
        "        axb[0].hist(img_proc.read_image_file_in_np_array(self.train_positive_color_file_list)[:,:,:,(2)].flatten(),\n",
        "                                                         bins = number_of_bins, \n",
        "                                                         density = True);\n",
        "        axb[1].hist(img_proc.read_image_file_in_np_array(self.train_negative_color_file_list)[:,:,:,(2)].flatten(),\n",
        "                                                         bins = number_of_bins, \n",
        "                                                         density = True);\n",
        "\n",
        "        axb[0].set_title(\"Train positive images\");\n",
        "        axb[1].set_title(\"Train negative images\");\n",
        "\n",
        "        axb[0].set_xlabel(\"Mean blue brightness\")\n",
        "        axb[1].set_xlabel(\"Mean blue brightness\")\n",
        "        axb[0].set_ylabel(\"Relative frequency\")\n",
        "        axb[1].set_ylabel(\"Relative frequency\")\n",
        "        plt.show(block = False);\n",
        "\n",
        "        print(f\"Completed get positive and negative images summary stats...\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print()\n",
        "\n",
        "    def data_processing_pipeline(self):\n",
        "        \"\"\"\n",
        "        This method performs required data processing steps.\n",
        "        \"\"\"\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"Starting data processing pipeline...\")\n",
        "        self.get_file_names_list()\n",
        "        self.get_label_info()\n",
        "        self.initial_split_data()\n",
        "        \n",
        "        if self.run_mode == 'interim_test':\n",
        "            self.check_duplicate_ids('train')\n",
        "            self.check_duplicate_ids('test')\n",
        "            self.get_data_distribution('train')\n",
        "            self.get_data_distribution('test')\n",
        "            self.get_image_summary_stats()\n",
        "            \n",
        "        #self.move_files()\n",
        "        print(\"Completed data processing pipeline...\")\n",
        "        print()\n",
        "        print(\"*****************************************************\")\n",
        "        print(\"*****************************************************\")\n",
        "\n",
        "'''\n",
        "data_proc = data_processing(run_mode = 'final_test')\n",
        "'''\n",
        "# Used for testing\n",
        "#data_proc = data_processing(run_mode = 'interim_test') \n",
        "data_proc = data_processing(run_mode = 'final_test')\n",
        "data_proc.data_processing_pipeline()\n",
        "\n",
        "# Data visualizations\n",
        "data_proc.data_visualization(train_or_test_ind = 'train', positive_or_negative_ind = 'positive', image_list = data_proc.train_positive_color_file_list)\n",
        "data_proc.data_visualization(train_or_test_ind = 'train', positive_or_negative_ind = 'negative', image_list = data_proc.train_positive_color_file_list)"
      ],
      "metadata": {
        "id": "9ASpSKwV7EnJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "outputId": "834ce1fa-079a-4b19-8766-d3d00c5dd96e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-fc4d4f19b4aa>\"\u001b[0;36m, line \u001b[0;32m139\u001b[0m\n\u001b[0;31m    '_' + str(rot_angle) + _rot_gs' +\u001b[0m\n\u001b[0m                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of train data : {len(data_proc.df_train)}\")\n",
        "print(f\"Length of test data : {len(data_proc.df_test)}\")"
      ],
      "metadata": {
        "id": "o67jG5rOF6xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -ltr /content/image_processing/test/positive/*.tif|wc -l\n",
        "!ls -ltr /content/image_processing/test/negative/*.tif|wc -l"
      ],
      "metadata": {
        "id": "Ug0Fd0AkNVZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***3. Model Building***"
      ],
      "metadata": {
        "id": "ISUTBnGe72tT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def soft_voting_cllasifier(df):\n",
        "    voting_df = df.groupby(['y_test_index', 'actual_val']).agg({'sigmoid_val' : np.mean}).reset_index()\n",
        "    voting_df.columns = ['y_test_index', \n",
        "                         'actual_val',\n",
        "                         'assigned_probability'\n",
        "                    ]\n",
        "    voting_df['assigned_binary_value'] = np.where(voting_df['assigned_probability'] > .5,\n",
        "                                                  1,\n",
        "                                                  0\n",
        "                                                 )\n",
        "    voting_df['actual_val'] = voting_df['actual_val'].apply(lambda x : int(x))\n",
        "    return voting_df"
      ],
      "metadata": {
        "id": "ou6znlH7qnkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hard_voting_cllasifier(df):\n",
        "    agg_df = df.groupby(['y_test_index']).agg({'th50_bin_pred_val': [np.size, np.min, np.max, stats.mode]}).reset_index()\n",
        "    agg_df.columns = ['y_test_index', \n",
        "                      'th50_bin_pred_val_count', \n",
        "                      'th50_bin_pred_val_min', \n",
        "                      'th50_bin_pred_val_max',\n",
        "                      'pred_bin'\n",
        "                     ]\n",
        "    agg_df[['majority_class_value', 'majority_count']] = pd.DataFrame(agg_df['pred_bin'].tolist(), index = agg_df.index)\n",
        "    agg_df['majority_class_value'] = agg_df['majority_class_value'].apply(lambda x : x[0])\n",
        "    agg_df['majority_count'] = agg_df['majority_count'].apply(lambda x : x[0])\n",
        "    agg_df['assigned_binary_value'] = np.where((agg_df['th50_bin_pred_val_min'] == agg_df['th50_bin_pred_val_max']) | (agg_df['majority_count'] < cfg_proc.number_of_splits), agg_df['majority_class_value'], -1)\n",
        "    agg_df['assigned_binary_value'] = np.where(agg_df['majority_count'] * 2 == agg_df['th50_bin_pred_val_count'], 1, agg_df['assigned_binary_value'])\n",
        "\n",
        "    clf_list = []\n",
        "    for rec in agg_df[['y_test_index', 'assigned_binary_value']].values.tolist():\n",
        "        avg_sigmoid_val = cfg_proc.consolidated_pred_df[(cfg_proc.consolidated_pred_df.y_test_index == rec[0]) & (cfg_proc.consolidated_pred_df.th50_bin_pred_val == rec[1])]['sigmoid_val'].mean()\n",
        "        clf_list.append((rec[0], avg_sigmoid_val, rec[1]))\n",
        "    df_clf = pd.DataFrame(clf_list)\n",
        "    df_clf.columns   = ['y_test_index', \n",
        "                        'assigned_probability', \n",
        "                        'assigned_binary_value'\n",
        "                       ]\n",
        "    df_clf['y_test_index'] = df_clf['y_test_index'].apply(lambda x : int(x))\n",
        "    df_clf = pd.merge(df_clf, \n",
        "                      df[['y_test_index', 'actual_val']].drop_duplicates(), \n",
        "                      how = 'inner', \n",
        "                      on = 'y_test_index'\n",
        "                     )\n",
        "    df_clf['actual_val'] = df_clf['actual_val'].astype(int)\n",
        "    return df_clf"
      ],
      "metadata": {
        "id": "Rwv2V8jVpNk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc(name, labels, predictions, **kwargs):\n",
        "  fp, tp, _ = sklearn.metrics.roc_curve(labels, predictions)\n",
        "\n",
        "  plt.plot(100*fp, 100*tp, label=name, linewidth=2, **kwargs)\n",
        "  plt.xlabel('False positives [%]')\n",
        "  plt.ylabel('True positives [%]')\n",
        "  plt.xlim([-0.5,20])\n",
        "  plt.ylim([80,100.5])\n",
        "  plt.grid(True)\n",
        "  ax = plt.gca()\n",
        "  ax.set_aspect('equal')\n",
        "  plt.show(block = False)\n",
        "\n",
        "def plot_metrics(history):\n",
        "  metrics = ['loss', 'accuracy', 'precision', 'recall']\n",
        "  for n, metric in enumerate(metrics):\n",
        "    name = metric.replace(\"_\",\" \").capitalize()\n",
        "    plt.subplot(2,2,n+1)\n",
        "    plt.plot(history.epoch, \n",
        "             history.history[metric], \n",
        "             color = colors[0], \n",
        "             label = 'Train')\n",
        "    plt.plot(history.epoch, \n",
        "             history.history['val_'+ metric],\n",
        "             color=colors[1], \n",
        "             linestyle=\"--\", \n",
        "             label = 'Validation')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel(name)\n",
        "    if metric == 'loss':\n",
        "      plt.ylim([0, plt.ylim()[1]])\n",
        "    elif metric == 'auc':\n",
        "      plt.ylim([0.8,1])\n",
        "    else:\n",
        "      plt.ylim([0,1])\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show(block = False)\n",
        "\n",
        "def plot_cm(labels, predictions, p=0.5):\n",
        "  cm = confusion_matrix(labels, predictions > p)\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cm, annot=True, fmt=\"d\")\n",
        "  plt.title('Confusion matrix @{:.2f}'.format(p))\n",
        "  plt.ylabel('Actual label')\n",
        "  plt.xlabel('Predicted label')\n",
        "\n",
        "  print('True Negatives: ', cm[0][0])\n",
        "  print('False Positives: ', cm[0][1])\n",
        "  print('False Negatives: ', cm[1][0])\n",
        "  print('True Positives: ', cm[1][1])\n",
        "  print('Total : ', np.sum(cm[1]))\n",
        "  plt.show(block = False)"
      ],
      "metadata": {
        "id": "GFyVnBuWfcpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data generators\n",
        "\n",
        "'''\n",
        "rotation rules\n",
        "(x, y) -90->  (-y, x)\n",
        "(x, y) -180-> (-x, -y)\n",
        "(x, y) -270-> (y, -x)\n",
        "(x, y) 90-> (y, -x)\n",
        "(x, y) 180-> (-x, -y)\n",
        "(x, y) 270-> (-y, x)\n",
        "'''\n",
        "\n",
        "def custom_augmentation(np_tensor):\n",
        " \n",
        "    def random_contrast(np_tensor):\n",
        "        return tf.image.random_contrast(np_tensor, 0.1, 5)\n",
        " \n",
        "    def random_hue(np_tensor):\n",
        "        return tf.image.random_hue(np_tensor, 0.5)\n",
        " \n",
        "    def random_saturation(np_tensor):\n",
        "        return tf.image.random_saturation(np_tensor, 0.2, 3)\n",
        " \n",
        "    def gaussian_noise(np_tensor):\n",
        "        mean = 0\n",
        "        # variance: randomly between 1 to 25\n",
        "        var = np.random.randint(1, 26)\n",
        "        # sigma is square root of the variance value\n",
        "        noise = np.random.normal(mean,var**0.5,np_tensor.shape)\n",
        "        return np.clip(np_tensor + noise, 0, 255).astype('int')\n",
        "\n",
        "    augmnted_tensor = random_contrast(np_tensor)\n",
        "    #augmnted_tensor = random_hue(augmnted_tensor)\n",
        "    #augmnted_tensor = random_saturation(augmnted_tensor)\n",
        "    #augmented_tensor = gaussian_noise(augmnted_tensor)\n",
        "  \n",
        "    return np.array(augmnted_tensor)\n",
        "\n",
        "# Change the choice value here.\n",
        "choice = 1\n",
        "\n",
        "# Train data generator\n",
        "if choice == 1 : # Choice 1 : No extra images, no contrast\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                       horizontal_flip = True,\n",
        "                                       vertical_flip = True,\n",
        "                                       rotation_range = 180,\n",
        "                                       zoom_range = 0.4, \n",
        "                                       width_shift_range = 0.3,\n",
        "                                       height_shift_range = 0.3,\n",
        "                                       shear_range = 0.3\n",
        "                                       #,preprocessing_function = custom_augmentation \n",
        "                                      )\n",
        "\n",
        "elif choice == 2 : # Choice 2 : No extra images, contrast\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                       horizontal_flip = True,\n",
        "                                       vertical_flip = True,\n",
        "                                       rotation_range = 180,\n",
        "                                       zoom_range = 0.4, \n",
        "                                       width_shift_range = 0.3,\n",
        "                                       height_shift_range = 0.3,\n",
        "                                       shear_range = 0.3,\n",
        "                                       preprocessing_function = custom_augmentation \n",
        "                                      )\n",
        "\n",
        "elif choice == 3 : # Choice 3 : Extra images, no contrast\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                       horizontal_flip = True,\n",
        "                                       vertical_flip = True,\n",
        "                                       #rotation_range = 180,\n",
        "                                       zoom_range = 0.4, \n",
        "                                       width_shift_range = 0.3,\n",
        "                                       height_shift_range = 0.3,\n",
        "                                       shear_range = 0.3\n",
        "                                       #,preprocessing_function = custom_augmentation \n",
        "                                      )\n",
        "elif choice == 4 : # Choice 4 : Extra images, contrast\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                       horizontal_flip = True,\n",
        "                                       vertical_flip = True,\n",
        "                                       #rotation_range = 180,\n",
        "                                       zoom_range = 0.4, \n",
        "                                       width_shift_range = 0.3,\n",
        "                                       height_shift_range = 0.3,\n",
        "                                       shear_range = 0.3,\n",
        "                                       preprocessing_function = custom_augmentation \n",
        "                                      )\n",
        "    \n",
        "# Validation data generator\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale = 1.0/255\n",
        "    )\n",
        "\n",
        "# Test data generator\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    rescale = 1.0/255\n",
        "    )\n",
        "\n",
        "for kfold, (train_indices, validation_indices) in enumerate(StratifiedKFold(n_splits     =  cfg_proc.number_of_splits, \n",
        "                                                                            shuffle      = True, \n",
        "                                                                            random_state = cfg_proc.random_state\n",
        "                                                                           ).split(data_proc.df_train_original['label'].values.tolist(), \n",
        "                                                                                   data_proc.df_train_original['label'].values.tolist()\n",
        "                                                                                  )):\n",
        "    '''\n",
        "    if kfold > 0:\n",
        "        break\n",
        "    '''\n",
        "    print(f\"k-fold : {kfold + 1}, length of train data : {len(train_indices)}, length of validation data : {len(validation_indices)}\")\n",
        "    \n",
        "    # Baseed on the choices selected, we are running appropraite split_data_based_on_indices method.\n",
        "    print(f\"Baseed on the choices selected, we are running appropraite split_data_based_on_indices method, our choice is {choice}\")\n",
        "    if choice == 3 or choice == 4:\n",
        "        data_proc.split_data_based_on_indices(train_indices = train_indices, validation_indices = validation_indices, add_extra_image_flag = 1)\n",
        "    elif choice == 1 or choice == 2:\n",
        "        data_proc.split_data_based_on_indices(train_indices = train_indices, validation_indices = validation_indices, add_extra_image_flag = 0)\n",
        "    \n",
        "    train_dataset_from_data_generator = train_datagen.flow_from_directory(cfg_proc.image_processing_train_path,\n",
        "                                                                          target_size = (cfg_proc.image_size, cfg_proc.image_size),\n",
        "                                                                          class_mode = 'binary',\n",
        "                                                                          batch_size = cfg_proc.batch_size,\n",
        "                                                                          color_mode = 'rgb',\n",
        "                                                                          shuffle    = True,\n",
        "                                                                          seed       = cfg_proc.random_state\n",
        "                                                                         )\n",
        "    validation_dataset_from_data_generator = val_datagen.flow_from_directory(cfg_proc.image_processing_validation_path,\n",
        "                                                                             target_size = (cfg_proc.image_size, cfg_proc.image_size),\n",
        "                                                                             class_mode = 'binary',\n",
        "                                                                             batch_size = cfg_proc.batch_size,\n",
        "                                                                             color_mode = 'rgb',\n",
        "                                                                             shuffle = True,\n",
        "                                                                             seed    = cfg_proc.random_state\n",
        "                                                                           )\n",
        "    test_dataset_from_data_generator = test_datagen.flow_from_directory(cfg_proc.image_processing_test_path,\n",
        "                                                                        target_size = (cfg_proc.image_size, cfg_proc.image_size),\n",
        "                                                                        batch_size = cfg_proc.batch_size,\n",
        "                                                                        class_mode = 'binary',\n",
        "                                                                        color_mode = 'rgb',\n",
        "                                                                        shuffle    = False,\n",
        "                                                                        seed       = cfg_proc.random_state\n",
        "                                                                       )\n",
        "        \n",
        "    tf.random.set_seed(cfg_proc.random_state)\n",
        "    np.random.seed(cfg_proc.random_state)\n",
        "\n",
        "    input_shape = (cfg_proc.image_size, cfg_proc.image_size, 3)\n",
        "    inputs = Input(input_shape)\n",
        "\n",
        "    xception = Xception(include_top = False, input_shape = input_shape)(inputs)\n",
        "    mobile_net = MobileNetV2(include_top = False, input_shape = input_shape)(inputs)\n",
        "    xception.trainable = True\n",
        "    mobile_net.trainable = True\n",
        "\n",
        "    outputs = Concatenate(axis=-1)([GlobalAveragePooling2D()(xception), GlobalAveragePooling2D()(mobile_net)])\n",
        "    outputs = Dropout(0.5)(outputs)\n",
        "    outputs = Dense(1, activation = 'sigmoid')(outputs)\n",
        "    model = Model(inputs, outputs)\n",
        "    trainable_count = count_params(model.trainable_weights)\n",
        "    non_trainable_count = count_params(model.non_trainable_weights)\n",
        "    print(f\"Traininable parameters : {trainable_count}, non trainable parameters : {non_trainable_count}\")\n",
        "\n",
        "    metrics_list = [\n",
        "      keras.metrics.TruePositives(name  = 'tp'),\n",
        "      keras.metrics.FalsePositives(name = 'fp'),\n",
        "      keras.metrics.TrueNegatives(name  = 'tn'),\n",
        "      keras.metrics.FalseNegatives(name = 'fn'), \n",
        "      keras.metrics.BinaryAccuracy(name = 'accuracy'),\n",
        "      keras.metrics.Precision(name      = 'precision'),\n",
        "      keras.metrics.Recall(name         = 'recall'),\n",
        "      keras.metrics.AUC(name            = 'auc'),\n",
        "      keras.metrics.AUC(name            = 'prc', \n",
        "                        curve = 'PR'\n",
        "                       ) # precision-recall curve\n",
        "    ]\n",
        "    model.compile(optimizer = Adam(learning_rate = 0.0001, \n",
        "                                   decay = 0.00001\n",
        "                                  ),\n",
        "                  loss = tf.keras.losses.BinaryCrossentropy(),\n",
        "                  metrics = metrics_list\n",
        "                  )\n",
        "    model.summary()\n",
        "    tf.keras.utils.plot_model(model, \n",
        "                              to_file='model_structure.png', \n",
        "                              show_shapes = True,\n",
        "                              show_layer_names = True)\n",
        "    plt.show(block = False)\n",
        "\n",
        "    history = model.fit(train_dataset_from_data_generator,\n",
        "                        epochs = cfg_proc.epochs,\n",
        "                        steps_per_epoch = len(train_dataset_from_data_generator),\n",
        "                        validation_data = validation_dataset_from_data_generator,\n",
        "                        validation_steps = len(validation_dataset_from_data_generator),\n",
        "                        verbose = 1\n",
        "                      )\n",
        "\n",
        "    # Model save\n",
        "    print(\"Saving model...\")\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "    model_name = f'tumor_detection_xception_mobilenet_combo_k{kfold + 1}_choice_1'\n",
        "    model.save(model_name + '.h5')\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "    # Plot metrices\n",
        "    print(\"Plotting the metrices...\")\n",
        "    plot_metrics(history)\n",
        "    \n",
        "    # Collecting train and validation metrices and append it back to consolidated one\n",
        "    temp_result_df = pd.DataFrame()\n",
        "    temp_result_df = pd.DataFrame(history.history)\n",
        "    temp_result_df['kfold'] = kfold + 1\n",
        "    cfg_proc.consolidated_history_df = pd.concat([temp_result_df, \n",
        "                                                  cfg_proc.consolidated_history_df\n",
        "                                                 ], \n",
        "                                                 axis = 0\n",
        "                                                )\n",
        "\n",
        "    # Model Predict, transform logits to probabilities\n",
        "    print(\"Model predict....\")\n",
        "    step_size_test = np.ceil(test_dataset_from_data_generator.n / test_dataset_from_data_generator.batch_size)\n",
        "    test_dataset_from_data_generator.reset()\n",
        "    probas_sigmoid = model.predict(test_dataset_from_data_generator, \n",
        "                                   batch_size = cfg_proc.batch_size, \n",
        "                                   verbose = 1\n",
        "                                  )\n",
        "    predictions_binary_th50 = [1 if x > .5 else 0 for x in probas_sigmoid]\n",
        "\n",
        "    # Model evaluate\n",
        "    print(\"Model evaluate and test metrices....\")\n",
        "    test_results = model.evaluate(test_dataset_from_data_generator, \n",
        "                                  batch_size = cfg_proc.batch_size,\n",
        "                                  verbose = 0\n",
        "                                 )   \n",
        "    test_kpi_dict = {}\n",
        "    for name, value in zip(model.metrics_names, test_results):\n",
        "        print(name, ': ', value)\n",
        "        test_kpi_dict[name] = value\n",
        "        print()\n",
        "\n",
        "    # Plot Confusion Matric\n",
        "    print(\"Confusion Matrix...\")\n",
        "    #plot_cm(data_proc.y_test, probas_sigmoid)\n",
        "    model_proc.plot_model_result_confusion_matrix(threshold_val = .5, \n",
        "                                                  y_pred_probas = probas_sigmoid, \n",
        "                                                  y_test        = test_dataset_from_data_generator.classes)\n",
        "    \n",
        "    print(\"Ploting AOC-AUC...\")\n",
        "    model_proc.plot_roc_auc_curve(y_pred_probas = probas_sigmoid, \n",
        "                                  y_test = test_dataset_from_data_generator.classes\n",
        "                                 )\n",
        "    \n",
        "    print(f\"Classification report with 50% threshold\")\n",
        "    model_proc.generate_report(threshold_val = 0.5, \n",
        "                               y_pred_probas = probas_sigmoid, \n",
        "                               y_test = test_dataset_from_data_generator.classes\n",
        "                              )\n",
        "    \n",
        "    temp_pred_df = pd.DataFrame(np.concatenate([probas_sigmoid, \n",
        "                                                np.asarray(predictions_binary_th50).reshape(-1,1),\n",
        "                                                test_dataset_from_data_generator.classes.reshape(-1,1)\n",
        "                                               ], \n",
        "                                               axis = 1\n",
        "                                              ))\n",
        "    temp_pred_df.columns = ['sigmoid_val', \n",
        "                            'th50_bin_pred_val', \n",
        "                            'actual_val'\n",
        "                           ]\n",
        "    temp_pred_df['kfold'] = kfold + 1\n",
        "    temp_pred_df.insert(0, 'y_test_index', range(1, 1 + len(temp_pred_df)))\n",
        "    cfg_proc.consolidated_pred_df = pd.concat([temp_pred_df, \n",
        "                                               cfg_proc.consolidated_pred_df\n",
        "                                              ], \n",
        "                                              axis = 0\n",
        "                                             )\n",
        "    print(\"********************************************\")\n",
        "    print(\"********************************************\")\n",
        "\n",
        "    test_kpi_df = pd.DataFrame([kfold + 1,\n",
        "                                test_kpi_dict['loss'], \n",
        "                                test_kpi_dict['accuracy'],\n",
        "                                test_kpi_dict['tp'],\n",
        "                                test_kpi_dict['fp'],\n",
        "                                test_kpi_dict['tn'],\n",
        "                                test_kpi_dict['fn'],\n",
        "                                test_kpi_dict['precision'],\n",
        "                                test_kpi_dict['recall'],\n",
        "                                test_kpi_dict['auc'],\n",
        "                                test_kpi_dict['prc']\n",
        "                               ]).T\n",
        "    test_kpi_df.columns = ['kfold', \n",
        "                           'test_loss', \n",
        "                           'test_accuracy',\n",
        "                           'test_true_positive',\n",
        "                           'test_false_positive',\n",
        "                           'test_true_negative',\n",
        "                           'test_false_negative',\n",
        "                           'test_precission',\n",
        "                           'test_recall',\n",
        "                           'test_auc',\n",
        "                           'test_prc'\n",
        "                          ]\n",
        "    cfg_proc.consolidated_test_kpi_df = pd.concat([test_kpi_df, \n",
        "                                                   cfg_proc.consolidated_test_kpi_df\n",
        "                                                  ], \n",
        "                                                  axis = 0\n",
        "                                                 )"
      ],
      "metadata": {
        "id": "Xv4rrJ4ivweJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_proc.consolidated_test_kpi_df.head()"
      ],
      "metadata": {
        "id": "dVXeb92At6il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_proc.consolidated_test_kpi_df.describe()"
      ],
      "metadata": {
        "id": "1GqgPvx4TvwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_proc.consolidated_history_df.head()"
      ],
      "metadata": {
        "id": "YMg9xiLjsRAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_proc.consolidated_history_df.describe()"
      ],
      "metadata": {
        "id": "xFe7R6UmTsoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_proc.consolidated_pred_df.head()"
      ],
      "metadata": {
        "id": "-eeEXZVHsTBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_proc.consolidated_pred_df.describe()"
      ],
      "metadata": {
        "id": "n6_HOu2uTyP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_proc.consolidated_pred_df['th50_bin_pred_val'] = np.where(cfg_proc.consolidated_pred_df['sigmoid_val'] < .5, 0, 1)"
      ],
      "metadata": {
        "id": "oslznJ7p_s0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg_proc.consolidated_history_df.to_csv('/content/gdrive/MyDrive/Kaggle/consolidated_history_choice_1.csv', index = False)\n",
        "cfg_proc.consolidated_test_kpi_df.to_csv('/content/gdrive/MyDrive/Kaggle/consolidated_test_kpi_choice_1.csv', index = False)\n",
        "cfg_proc.consolidated_pred_df.to_csv('/content/gdrive/MyDrive/Kaggle/consolidated_pred_choice_1.csv', index = False)"
      ],
      "metadata": {
        "id": "Bd44KWNEucIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calling ensemble soft voting...\")\n",
        "df_soft_voting_clf = soft_voting_cllasifier(cfg_proc.consolidated_pred_df)\n",
        "df_soft_voting_clf.head()"
      ],
      "metadata": {
        "id": "s7_JezIqFGeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_accuracy_soft_voting = accuracy_score(df_soft_voting_clf['actual_val'].values.tolist(), df_soft_voting_clf['assigned_binary_value'].values.tolist())\n",
        "final_precision_soft_voting = precision_score(df_soft_voting_clf['assigned_binary_value'].values.tolist(), df_soft_voting_clf['actual_val'].values.tolist())\n",
        "final_recall_soft_voting = recall_score(df_soft_voting_clf['assigned_binary_value'].values.tolist(), df_soft_voting_clf['actual_val'].values.tolist())\n",
        "final_f1_score_soft_voting = f1_score(df_soft_voting_clf['assigned_binary_value'].values.tolist(), df_soft_voting_clf['actual_val'].values.tolist())\n",
        "fpr, tpr, thresholds = roc_curve(df_soft_voting_clf['actual_val'].values.tolist(), df_soft_voting_clf['assigned_probability'].values.tolist())\n",
        "final_auc_val_soft_voting = auc(fpr, tpr)\n",
        "print(f\"Final Ensemble Accuracy : {final_accuracy_soft_voting}\")\n",
        "print(f\"Final Ensemble Precision : {final_precision_soft_voting}\")\n",
        "print(f\"Final Ensemble Recall : {final_recall_soft_voting}\")\n",
        "print(f\"Final Ensemble F1 Score : {final_f1_score_soft_voting}\")\n",
        "print(f\"Final Ensemble AUC Score : {final_auc_val_soft_voting}\")"
      ],
      "metadata": {
        "id": "uzn2J3rbypzS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_soft_voting_clf['final_accuracy_soft_voting'] = final_accuracy_soft_voting\n",
        "df_soft_voting_clf['final_precision_soft_voting'] = final_precision_soft_voting\n",
        "df_soft_voting_clf['final_recall_soft_voting'] = final_recall_soft_voting\n",
        "df_soft_voting_clf['final_auc_val_soft_voting'] = final_auc_val_soft_voting\n",
        "df_soft_voting_clf['final_f1_score_soft_voting'] = final_f1_score_soft_voting\n",
        "df_soft_voting_clf.to_csv('soft_voting_clf_result.csv', index = False)"
      ],
      "metadata": {
        "id": "lsyhTOTKQXB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Calling ensemble hard voting...\")\n",
        "df_hard_voting_clf = hard_voting_cllasifier(cfg_proc.consolidated_pred_df)\n",
        "df_hard_voting_clf.head()"
      ],
      "metadata": {
        "id": "YiTw242_0TaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_accuracy_hard_voting = accuracy_score(df_hard_voting_clf['assigned_binary_value'].values.tolist(), df_hard_voting_clf['assigned_binary_value'].values.tolist())\n",
        "final_precision_hard_voting = precision_score(df_hard_voting_clf['assigned_binary_value'].values.tolist(), df_hard_voting_clf['assigned_binary_value'].values.tolist())\n",
        "final_recall_hard_voting = recall_score(df_hard_voting_clf['assigned_binary_value'].values.tolist(), df_hard_voting_clf['assigned_binary_value'].values.tolist())\n",
        "final_f1_score_hard_voting = f1_score(df_hard_voting_clf['assigned_binary_value'].values.tolist(), df_hard_voting_clf['assigned_binary_value'].values.tolist())\n",
        "fpr, tpr, thresholds = roc_curve(df_hard_voting_clf['assigned_binary_value'].values.tolist(), df_hard_voting_clf['assigned_probability'].values.tolist())\n",
        "final_auc_val_hard_voting = auc(fpr, tpr)\n",
        "print(f\"Final Ensemble Accuracy : {final_accuracy_hard_voting}\")\n",
        "print(f\"Final Ensemble Precision : {final_precision_hard_voting}\")\n",
        "print(f\"Final Ensemble Recall : {final_recall_hard_voting}\")\n",
        "print(f\"Final Ensemble F1 Score : {final_f1_score_hard_voting}\")\n",
        "print(f\"Final Ensemble AUC Score : {final_auc_val_hard_voting}\")"
      ],
      "metadata": {
        "id": "wpsSGmJdyp2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_hard_voting_clf['final_accuracy_hard_voting'] = final_accuracy_hard_voting\n",
        "df_hard_voting_clf['final_precision_hard_voting'] = final_precision_hard_voting\n",
        "df_hard_voting_clf['final_recall_hard_voting'] = final_recall_hard_voting\n",
        "df_hard_voting_clf['final_auc_val_hard_voting'] = final_auc_val_hard_voting\n",
        "df_hard_voting_clf['final_f1_score_hard_voting'] = final_f1_score_hard_voting\n",
        "df_hard_voting_clf.to_csv('hard_voting_clf_result.csv', index = False)"
      ],
      "metadata": {
        "id": "bBJuhaZPQvFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_soft_voting_clf.columns = ['y_test_index', 'soft_voting_assigned_probability', 'soft_voting_assigned_binary_value',\n",
        "       'actual_val', 'final_accuracy_soft_voting',\n",
        "       'final_precision_soft_voting', 'final_recall_soft_voting',\n",
        "       'final_auc_val_soft_voting', 'final_f1_score_soft_voting']\n",
        "df_hard_voting_clf.columns = ['y_test_index', 'hard_voting_assigned_probability', 'hard_voting_assigned_binary_value',\n",
        "       'actual_val', 'final_accuracy_hard_voting',\n",
        "       'final_precision_hard_voting', 'final_recall_hard_voting',\n",
        "       'final_auc_val_hard_voting', 'final_f1_score_hard_voting']\n",
        "df_soft_voting_clf.drop(columns = ['actual_val'], inplace = True)"
      ],
      "metadata": {
        "id": "r6wbGPmsAWS4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_voting_combined = pd.concat([df_hard_voting_clf, df_soft_voting_clf], axis = 1)\n",
        "df_voting_combined.head()"
      ],
      "metadata": {
        "id": "1BvbmlpEyD0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_voting_combined = df_voting_combined[['y_test_index',\n",
        "                                         'actual_val',\n",
        "                                         'hard_voting_assigned_probability',\n",
        "                                         'soft_voting_assigned_probability',\n",
        "                                         'hard_voting_assigned_binary_value',\n",
        "                                         'soft_voting_assigned_binary_value',\n",
        "                                         'final_accuracy_hard_voting',\n",
        "                                         'final_accuracy_soft_voting',\n",
        "                                         'final_precision_hard_voting',\n",
        "                                         'final_precision_soft_voting',\n",
        "                                         'final_recall_hard_voting', \n",
        "                                         'final_recall_soft_voting',\n",
        "                                         'final_auc_val_hard_voting',\n",
        "                                         'final_auc_val_soft_voting',\n",
        "                                         'final_f1_score_hard_voting',\n",
        "                                         'final_f1_score_soft_voting'\n",
        "                                        ]]\n",
        "df_voting_combined = df_voting_combined.iloc[:,1:]"
      ],
      "metadata": {
        "id": "wVW12kcTAm9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_voting_combined.head()"
      ],
      "metadata": {
        "id": "X7LV-jY58cz2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_voting_combined.to_csv('/content/gdrive/MyDrive/Kaggle/combined_voting_results_choice_1.csv', index = False)"
      ],
      "metadata": {
        "id": "_3PTM1QkQ7Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Qw7-aeRQ7aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jrJLB1lgQ7gB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "test_img = load_img(data_proc.train_file_list[0], color_mode =\"grayscale\")\n",
        "test_img_arry = img_to_array(test_img)\n",
        "print(type(test_img))\n",
        "print(test_img.format)\n",
        "print(test_img.mode)\n",
        "print(test_img.size)\n",
        "print(test_img.getbands())\n",
        "print(test_img_arry.shape)\n",
        "\n",
        "test_img1 = load_img(data_proc.train_file_list[0])\n",
        "test_img1_arry = img_to_array(test_img1)\n",
        "print(test_img1_arry.shape)\n",
        "print(test_img1.getbands())\n",
        "'''"
      ],
      "metadata": {
        "id": "F-_a0JOfmzX3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}